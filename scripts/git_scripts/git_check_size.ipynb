{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/scripts/git_scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.options.display.max_colwidth = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\n",
    "    os.environ[\"PROJ_irox_oer\"]\n",
    "    )\n",
    "\n",
    "res = subprocess.check_output(\n",
    "    [\"git\", \"ls-files\", \"--others\", \"--exclude-standard\"]\n",
    "    )\n",
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for path_i in res.splitlines():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    file_path_i = path_i.decode('UTF-8')\n",
    "    file_path_root_i = \"/\".join(file_path_i.split(\"/\")[0:-1])\n",
    "    file_name_i = file_path_i.split(\"/\")[-1]\n",
    "    file_name_no_ext_i = file_name_i.split(\".\")[0]\n",
    "    file_ext_i = file_name_i.split(\".\")[-1]\n",
    "    size_i = os.stat(file_path_i).st_size / 1000  # Size in KB\n",
    "\n",
    "    # #####################################################\n",
    "    file_name_py = file_name_i.split(\".\")[0] + \".py\"\n",
    "    file_path_py = os.path.join(\n",
    "        \"/\".join(file_path_i.split(\"/\")[0:-1]),\n",
    "        file_name_py)\n",
    "    py_file_exists = False\n",
    "    if Path(file_path_py).is_file():\n",
    "        py_file_exists = True\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"file_path_root\"] = file_path_root_i\n",
    "    data_dict_i[\"file_path\"] = file_path_i\n",
    "    data_dict_i[\"file_name\"] = file_name_i\n",
    "    data_dict_i[\"file_name_no_ext\"] = file_name_no_ext_i\n",
    "    data_dict_i[\"file_ext\"] = file_ext_i\n",
    "    data_dict_i[\"size_kb\"] = size_i\n",
    "    data_dict_i[\"py_file_exists\"] = py_file_exists\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "    \n",
    "# #########################################################\n",
    "df = pd.DataFrame(data_dict_list)\n",
    "df[\"size_mb\"] = df.size_kb / 1000\n",
    "# df = df.sort_values(\"size_kb\", ascending=False)\n",
    "df = df.sort_values(\"file_path\", ascending=False)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commiting ipynb/py pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git add \\\n",
      "workflow/xrd_bulks/xrd_bulks.ipynb \\\n",
      "workflow/xrd_bulks/xrd_bulks.py \\\n",
      "workflow/xrd_bulks/plot_xrd_patterns.ipynb \\\n",
      "workflow/xrd_bulks/plot_xrd_patterns.py \\\n",
      "workflow/process_bulk_dft/write_atoms_json.ipynb \\\n",
      "workflow/process_bulk_dft/write_atoms_json.py \\\n",
      "workflow/process_bulk_dft/read_json_to_new_ase.ipynb \\\n",
      "workflow/process_bulk_dft/read_json_to_new_ase.py \\\n",
      "workflow/process_bulk_dft/manually_classify_bulks/classify_bulks.ipynb \\\n",
      "workflow/process_bulk_dft/manually_classify_bulks/classify_bulks.py \\\n",
      "workflow/process_bulk_dft/get_bulk_coor_env.ipynb \\\n",
      "workflow/process_bulk_dft/get_bulk_coor_env.py \\\n",
      "workflow/process_bulk_dft/create_final_df_dft.ipynb \\\n",
      "workflow/process_bulk_dft/create_final_df_dft.py \\\n",
      "workflow/oer_vs_features/oer_vs_features.ipynb \\\n",
      "workflow/oer_vs_features/oer_vs_features.py \\\n",
      "workflow/oer_vs_features/oer_lin_model.ipynb \\\n",
      "workflow/oer_vs_features/oer_lin_model.py \\\n",
      "workflow/oer_analysis/temp_plot_data/plot_oer_data.ipynb \\\n",
      "workflow/oer_analysis/temp_plot_data/plot_oer_data.py \\\n",
      "workflow/oer_analysis/oer_analysis.ipynb \\\n",
      "workflow/oer_analysis/oer_analysis.py \\\n",
      "workflow/feature_engineering/generate_features/oxid_state/oxid_state.ipynb \\\n",
      "workflow/feature_engineering/generate_features/oxid_state/oxid_state.py \\\n",
      "workflow/feature_engineering/generate_features/octahedra_volume/sandbox.ipynb \\\n",
      "workflow/feature_engineering/generate_features/octahedra_volume/sandbox.py \\\n",
      "workflow/feature_engineering/generate_features/octahedra_volume/octa_volume.ipynb \\\n",
      "workflow/feature_engineering/generate_features/octahedra_volume/octa_volume.py \\\n",
      "workflow/feature_engineering/collect_feature_data.ipynb \\\n",
      "workflow/feature_engineering/collect_feature_data.py \\\n",
      "workflow/enumerate_adsorption/test_dev_rdf_comp/subtract_rdf_dfs.ipynb \\\n",
      "workflow/enumerate_adsorption/test_dev_rdf_comp/subtract_rdf_dfs.py \\\n",
      "workflow/enumerate_adsorption/test_dev_rdf_comp/sandbox.ipynb \\\n",
      "workflow/enumerate_adsorption/test_dev_rdf_comp/sandbox.py \\\n",
      "workflow/enumerate_adsorption/test_dev_rdf_comp/opt_rdf_comp.ipynb \\\n",
      "workflow/enumerate_adsorption/test_dev_rdf_comp/opt_rdf_comp.py \\\n",
      "workflow/enumerate_adsorption/sandbox.ipynb \\\n",
      "workflow/enumerate_adsorption/sandbox.py \\\n",
      "workflow/enumerate_adsorption/get_all_active_sites.ipynb \\\n",
      "workflow/enumerate_adsorption/get_all_active_sites.py \\\n",
      "workflow/creating_slabs/write_slab_ids.ipynb \\\n",
      "workflow/creating_slabs/write_slab_ids.py \\\n",
      "workflow/creating_slabs/slab_similarity/slab_similarity.ipynb \\\n",
      "workflow/creating_slabs/slab_similarity/slab_similarity.py \\\n",
      "workflow/creating_slabs/creating_symm_slabs/sandbox.ipynb \\\n",
      "workflow/creating_slabs/creating_symm_slabs/sandbox.py \\\n",
      "workflow/creating_slabs/creating_symm_slabs/creat_symm_slabs.ipynb \\\n",
      "workflow/creating_slabs/creating_symm_slabs/creat_symm_slabs.py \\\n",
      "workflow/__misc__/analysis_for_jens/analysis.ipynb \\\n",
      "workflow/__misc__/analysis_for_jens/analysis.py \\\n",
      "scripts/repo_file_operations/convert_jup_to_pyth.ipynb \\\n",
      "scripts/repo_file_operations/convert_jup_to_pyth.py \\\n",
      "scripts/repo_file_operations/clean_jup.ipynb \\\n",
      "scripts/repo_file_operations/clean_jup.py \\\n",
      "scripts/git_scripts/git_check_size.ipynb \\\n",
      "scripts/git_scripts/git_check_size.py \\\n",
      "scripts/git_scripts/get_modified_py_files.ipynb \\\n",
      "scripts/git_scripts/get_modified_py_files.py \\\n",
      "sandbox/write_atoms_organized.ipynb \\\n",
      "sandbox/write_atoms_organized.py \\\n",
      "sandbox/read_all_data_objs.ipynb \\\n",
      "sandbox/read_all_data_objs.py \\\n",
      "sandbox/job_accouting.ipynb \\\n",
      "sandbox/job_accouting.py \\\n",
      "dft_workflow/run_slabs/setup_new_jobs_from_oh.ipynb \\\n",
      "dft_workflow/run_slabs/setup_new_jobs_from_oh.py \\\n",
      "dft_workflow/run_slabs/setup_new_jobs.ipynb \\\n",
      "dft_workflow/run_slabs/setup_new_jobs.py \\\n",
      "dft_workflow/run_slabs/run_oh_covered/setup_dft_oh.ipynb \\\n",
      "dft_workflow/run_slabs/run_oh_covered/setup_dft_oh.py \\\n",
      "dft_workflow/run_slabs/run_o_covered/setup_dft.ipynb \\\n",
      "dft_workflow/run_slabs/run_o_covered/setup_dft.py \\\n",
      "dft_workflow/run_slabs/run_bare_oh_covered/setup_dft_bare.ipynb \\\n",
      "dft_workflow/run_slabs/run_bare_oh_covered/setup_dft_bare.py \\\n",
      "dft_workflow/manually_analyze_slabs/manually_analyze_slabs.ipynb \\\n",
      "dft_workflow/manually_analyze_slabs/manually_analyze_slabs.py \\\n",
      "dft_workflow/job_processing/sandbox.ipynb \\\n",
      "dft_workflow/job_processing/sandbox.py \\\n",
      "dft_workflow/job_processing/parse_job_dirs.ipynb \\\n",
      "dft_workflow/job_processing/parse_job_dirs.py \\\n",
      "dft_workflow/job_processing/parse_job_data.ipynb \\\n",
      "dft_workflow/job_processing/parse_job_data.py \\\n",
      "dft_workflow/job_processing/collect_job_dirs_data.ipynb \\\n",
      "dft_workflow/job_processing/collect_job_dirs_data.py \\\n",
      "dft_workflow/job_processing/clean_dft_dirs.ipynb \\\n",
      "dft_workflow/job_processing/clean_dft_dirs.py \\\n",
      "dft_workflow/job_processing/analyze_jobs.ipynb \\\n",
      "dft_workflow/job_processing/analyze_jobs.py \\\n",
      "dft_workflow/job_analysis/prepare_oer_sets/prepare_oer_sets.ipynb \\\n",
      "dft_workflow/job_analysis/prepare_oer_sets/prepare_oer_sets.py \\\n",
      "dft_workflow/job_analysis/measure_comp_resources/measure_cpu_hours.ipynb \\\n",
      "dft_workflow/job_analysis/measure_comp_resources/measure_cpu_hours.py \\\n",
      "dft_workflow/job_analysis/get_init_slabs_bare_oh/get_init_slabs_bare_oh.ipynb \\\n",
      "dft_workflow/job_analysis/get_init_slabs_bare_oh/get_init_slabs_bare_oh.py \\\n",
      "dft_workflow/job_analysis/df_coord_for_post_dft/coord_env_for_post_dft.ipynb \\\n",
      "dft_workflow/job_analysis/df_coord_for_post_dft/coord_env_for_post_dft.py \\\n",
      "dft_workflow/job_analysis/create_oh_slabs/create_oh_slabs.ipynb \\\n",
      "dft_workflow/job_analysis/create_oh_slabs/create_oh_slabs.py \\\n",
      "dft_workflow/job_analysis/compare_magmoms/sandbox.ipynb \\\n",
      "dft_workflow/job_analysis/compare_magmoms/sandbox.py \\\n",
      "dft_workflow/job_analysis/compare_magmoms/decide_jobs_magmoms.ipynb \\\n",
      "dft_workflow/job_analysis/compare_magmoms/decide_jobs_magmoms.py \\\n",
      "dft_workflow/job_analysis/compare_magmoms/compare_magoms.ipynb \\\n",
      "dft_workflow/job_analysis/compare_magmoms/compare_magoms.py \\\n",
      "dft_workflow/job_analysis/collect_collate_dft_data/collect_collate_dft.ipynb \\\n",
      "dft_workflow/job_analysis/collect_collate_dft_data/collect_collate_dft.py \\\n",
      "dft_workflow/job_analysis/atoms_indices_order/correct_atom_indices_order.ipynb \\\n",
      "dft_workflow/job_analysis/atoms_indices_order/correct_atom_indices_order.py \\\n",
      "dft_workflow/job_analysis/analyze_oh_jobs/anal_oh_slabs.ipynb \\\n",
      "dft_workflow/job_analysis/analyze_oh_jobs/anal_oh_slabs.py \\\n",
      "dft_workflow/bin/sync_unsub_jobs_to_clus.ipynb \\\n",
      "dft_workflow/bin/sync_unsub_jobs_to_clus.py \\\n",
      "dft_workflow/bin/run_unsub_jobs.ipynb \\\n",
      "dft_workflow/bin/run_unsub_jobs.py \\\n",
      "dft_workflow/bin/delete_unsub_jobs.ipynb \\\n",
      "dft_workflow/bin/delete_unsub_jobs.py \\\n",
      "dft_workflow/bin/anal_job_out.ipynb \\\n",
      "dft_workflow/bin/anal_job_out.py \\\n",
      "__misc__/00_group_meeting/group_meeting.ipynb \\\n",
      "__misc__/00_group_meeting/group_meeting.py \\\n"
     ]
    }
   ],
   "source": [
    "df_i = df[df.py_file_exists == True]\n",
    "df_i = df_i[df_i.file_ext == \"ipynb\"]\n",
    "\n",
    "print(\"git add \\\\\")\n",
    "for ind_i, row_i in df_i.iterrows():\n",
    "    tmp = 42\n",
    "\n",
    "    # #########################################################\n",
    "    file_path_root_i = row_i.file_path_root\n",
    "    file_path_i = row_i.file_path\n",
    "    file_name_no_ext_i = row_i.file_name_no_ext\n",
    "    # #########################################################\n",
    "\n",
    "    df_py = df[\n",
    "        (df.file_path_root == file_path_root_i) & \\\n",
    "        (df.file_name_no_ext == file_name_no_ext_i) & \\\n",
    "        (df.file_ext == \"py\") & \\\n",
    "        [True for i in range(len(df))]\n",
    "        ]\n",
    "\n",
    "    mess_i = \"isjdifjsdijfisdj\"\n",
    "    assert df_py.shape[0] == 1, mess_i\n",
    "\n",
    "    row_py_i = df_py.iloc[0]\n",
    "\n",
    "    # #########################################################\n",
    "    file_path_py_i = row_py_i.file_path\n",
    "    # #########################################################\n",
    "\n",
    "    print(\n",
    "        file_path_i, \" \\\\\",\n",
    "        \"\\n\",\n",
    "        file_path_py_i, \" \\\\\",\n",
    "        sep=\"\")\n",
    "    # print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path_i = row_i.file_path"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of notebooks: \n",
      " 1.559 MB\n"
     ]
    }
   ],
   "source": [
    "# 1.774\n",
    "\n",
    "print(\n",
    "    \"Total size of notebooks:\",\n",
    "    \"\\n\",\n",
    "    np.round(df.size_mb.sum(), 3),\n",
    "    \"MB\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.sort_values(\"size_mb\", ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df.iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_i = df[df.file_ext == \"ipynb\"]\n",
    "\n",
    "# for ind_i, row_i in df_i.iterrows():\n",
    "\n",
    "#     # #####################################################\n",
    "#     file_path_i = row_i.file_path\n",
    "#     file_name_i = row_i.file_name\n",
    "#     # #####################################################\n",
    "\n",
    "#     file_name_py = file_name_i.split(\".\")[0] + \".py\"\n",
    "#     file_path_py = os.path.join(\n",
    "#         \"/\".join(file_path_i.split(\"/\")[0:-1]),\n",
    "#         file_name_py\n",
    "#         )\n",
    "\n",
    "#     py_file_exists = False\n",
    "#     if Path(file_path_py).is_file():\n",
    "#         py_file_exists = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# file_name_i = \n",
    "# path_i.split(\"/\")[-1]\n",
    "# path_i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_IrOx_Active_Learning_OER]",
   "language": "python",
   "name": "conda-env-PROJ_IrOx_Active_Learning_OER-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
