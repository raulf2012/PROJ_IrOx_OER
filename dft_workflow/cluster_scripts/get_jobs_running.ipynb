{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get jobs currently being processed on clusters\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# #########################################################\n",
    "from methods import get_df_jobs_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"COMPENV\"] != \"wsl\":\n",
    "    TEST = False\n",
    "\n",
    "if verbose:\n",
    "    print(\"TEST:\", TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths = get_df_jobs_paths()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse currently running/pending jobs from `jobs_mine` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compenv = os.environ[\"COMPENV\"]\n",
    "scripts_dir = os.environ[\"sc\"]\n",
    "\n",
    "if TEST:\n",
    "    compenv = \"sherlock\"\n",
    "\n",
    "\n",
    "if compenv == \"sherlock\":\n",
    "    bash_comm = \"python %s/08_slurm_jobs/jobs.py\" % scripts_dir\n",
    "elif compenv == \"slac\":\n",
    "    bash_comm = \"/usr/local/bin/bjobs -w\"\n",
    "elif compenv == \"nersc\":\n",
    "    bash_comm = \"/global/homes/f/flores12/usr/bin/queues\"\n",
    "else:\n",
    "    bash_comm = \"\"\n",
    "\n",
    "\n",
    "if verbose:\n",
    "    print(\"bash_comm:\", \"\\n\", bash_comm, sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEST:\n",
    "    result = subprocess.run(\n",
    "        bash_comm.split(\" \"),\n",
    "        stdout=subprocess.PIPE,\n",
    "        )\n",
    "\n",
    "    output_list = result.stdout.decode('utf-8').splitlines()\n",
    "    if verbose:\n",
    "        print(output_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    output_list = [\n",
    "        'No prev_jobs.txt found. Writing prev_jobs.txt now',\n",
    "        '+----------+-----------+----+----------+-------+---------------------------------------------------------------------------------------------------------------------------------------------------+',\n",
    "        '|  Job ID  | Partition | ST | Run Time | Qtime | Path                                                                                                                                              |',\n",
    "        '+----------+-----------+----+----------+-------+---------------------------------------------------------------------------------------------------------------------------------------------------+',\n",
    "        '| 17512866 | owners,ir | PD |   0.0    |  0.7  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/7ic1vt7pz4/010/active_site__37/01_attempt/_01    |',\n",
    "        '| 17512840 |    iric   | R  |   0.3    |  0.4  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/8l919k6s7p/1-100/active_site__50/01_attempt/_01  |',\n",
    "        '| 17513849 | owners,ir | PD |   0.0    |  0.2  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/8p937183bh/2-1-11/active_site__33/01_attempt/_01 |',\n",
    "        '| 17512889 | owners,ir | PD |   0.0    |  0.7  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/926dnunrxf/010/active_site__48/01_attempt/_01    |',\n",
    "        '| 17512874 | owners,ir | PD |   0.0    |  0.7  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/bgcpc2vabf/010/active_site__64/01_attempt/_01    |',\n",
    "        '| 17512851 | owners,ir | PD |   0.0    |  0.7  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/cq7smr6lvj/20-3/active_site__49/01_attempt/_01   |',\n",
    "        '| 17512897 | owners,ir | PD |   0.0    |  0.7  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nscdbpmdct/1-102/active_site__28/01_attempt/_01  |',\n",
    "        '| 17512835 | owners,ir | PD |   0.0    |  0.7  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/v2blxebixh/2-10/active_site__67/01_attempt/_01   |',\n",
    "        '| 17512860 | owners,ir | PD |   0.0    |  0.7  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/zimixdvdxd/2-1-11/active_site__56/01_attempt/_01 |',\n",
    "        '| 17512854 | owners,ir | PD |   0.0    |  0.7  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/zimixdvdxd/2-1-11/active_site__61/01_attempt/_01 |',\n",
    "        '| 17354096 |   owners  | R  |   12.7   |  3.3  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/mrbine8k72/010/oh/active_site__33/02_attempt/_04    |',\n",
    "        '| 17232073 |   owners  | R  |   12.7   |  2.4  | /scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/run_slabs/run_oh_covered/out_data/dft_jobs/mrbine8k72/010/oh/active_site__34/01_attempt/_03    |',\n",
    "        '+----------+-----------+----+----------+-------+---------------------------------------------------------------------------------------------------------------------------------------------------+',\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJ_irox_oer = os.environ[\"PROJ_irox_oer\"]\n",
    "\n",
    "if TEST:\n",
    "    PROJ_irox_oer = \"/scratch/users/flores12/PROJ_IrOx_OER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for line_i in output_list:\n",
    "    if PROJ_irox_oer in line_i:\n",
    "        for split_j in line_i.split(\" \"):\n",
    "            if PROJ_irox_oer in split_j:\n",
    "                path_parsed_j = split_j\n",
    "\n",
    "                find_ind = path_parsed_j.find(PROJ_irox_oer)\n",
    "                path_short_j = path_parsed_j[find_ind + len(PROJ_irox_oer) + 1:]\n",
    "\n",
    "                # #########################################\n",
    "                data_dict_i = dict()\n",
    "                # #########################################\n",
    "                data_dict_i[\"compenv\"] = compenv\n",
    "                data_dict_i[\"path\"] = path_parsed_j\n",
    "                data_dict_i[\"path_short\"] = path_short_j\n",
    "                # #########################################\n",
    "                data_dict_list.append(data_dict_i)\n",
    "                # #########################################\n",
    "\n",
    "# #########################################################\n",
    "df = pd.DataFrame(data_dict_list)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting job_id by mapping path to `df_jobs_paths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for index_i, row_i in df.iterrows():\n",
    "    # #####################################################\n",
    "    path_short_i = row_i.path_short\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    df_paths_i = df_paths[df_paths.path_rel_to_proj == path_short_i]\n",
    "\n",
    "    df_paths_2_i = df_paths[df_paths.path_rel_to_proj__no_compenv == path_short_i]\n",
    "\n",
    "    # assert df_paths_i.shape[0] == 1, \"Must only be one\"\n",
    "    assert df_paths_i.shape[0] <= 1, \"Must only be one, or 0 (not ideal)\"\n",
    "\n",
    "\n",
    "    if df_paths_i.shape[0] > 0:\n",
    "        df_paths_i = df_paths_i\n",
    "    elif df_paths_2_i.shape[0] > 0:\n",
    "        # print(\"SDIFJIDS\")\n",
    "        df_paths_i = df_paths_2_i\n",
    "\n",
    "\n",
    "    if df_paths_i.shape[0] > 0:\n",
    "        row_paths_i = df_paths_i.iloc[0]\n",
    "\n",
    "        job_id_i = row_paths_i.name\n",
    "\n",
    "        # #################################################\n",
    "        data_dict_i = dict()\n",
    "        # #################################################\n",
    "        data_dict_i[\"job_id\"] = job_id_i\n",
    "        # #################################################\n",
    "        data_dict_i.update(row_i.to_dict())\n",
    "        # #################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #################################################\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_on_clus = pd.DataFrame(data_dict_list)\n",
    "\n",
    "if len(data_dict_list) == 0:\n",
    "    df_jobs_on_clus = pd.DataFrame(columns=[\"job_id\", ])\n",
    "else:\n",
    "    df_jobs_on_clus = df_jobs_on_clus.set_index(\"job_id\", drop=False)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing dataframe to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_data_dir_rel_to_proj = os.path.join(\n",
    "    \"dft_workflow/cluster_scripts\",\n",
    "    \"out_data\",\n",
    "    )\n",
    "\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    out_data_dir_rel_to_proj)\n",
    "\n",
    "# print(\"directory:\", directory)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Pickling data ###########################################\n",
    "file_path = os.path.join(\n",
    "    directory, \"df_jobs_on_clus__%s.pickle\" % compenv)\n",
    "with open(file_path, \"wb\") as fle:\n",
    "    pickle.dump(df_jobs_on_clus, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syncing dataframe file to Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_rel_to_dropbox = os.path.join(\n",
    "    \"01_norskov/00_git_repos/PROJ_IrOx_OER\",\n",
    "    out_data_dir_rel_to_proj,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rclone_comm_i = \"rclone copy %s %s:%s\" % (file_path, os.environ[\"rclone_dropbox\"], dir_rel_to_dropbox)\n",
    "\n",
    "rclone_comm_list_i = [i for i in rclone_comm_i.split(\" \") if i != \"\"]\n",
    "\n",
    "result = subprocess.run(\n",
    "    rclone_comm_list_i,\n",
    "    stdout=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"get_jobs_running.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
