{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rerunning jobs with unfinished slabs\n",
    "---\n",
    "\n",
    "Main issue here is that I found slabs that seems to finish but were not force optimized/minimized for some strange reason"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs_anal,\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_features_targets,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_anal = get_df_jobs_anal()\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "df_features_targets = get_df_features_targets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering `df_jobs_anal` to oer_adsorbate rows"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind = df_jobs_anal.index.to_frame()\n",
    "\n",
    "df_ind = df_ind[df_ind.job_type == \"oer_adsorbate\"]\n",
    "\n",
    "df_jobs_anal_i = df_jobs_anal.loc[\n",
    "    df_ind.index\n",
    "    ]\n",
    "\n",
    "df_jobs_anal_i = df_jobs_anal_i[df_jobs_anal_i.job_completely_done == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for name_i, row_i in df_jobs_anal_i.iterrows():\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    name_dict_i = dict(zip(\n",
    "        df_jobs_anal_i.index.names,\n",
    "        name_i))\n",
    "    # #####################################################\n",
    "    job_id_i = row_i.job_id_max\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_paths_i = df_jobs_paths.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    path_i = row_paths_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_data_i = df_jobs_data.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    force_largest_i = row_data_i[\"force_largest\"]\n",
    "    force_sum_i = row_data_i[\"force_sum\"]\n",
    "    force_sum_per_atom_i = row_data_i[\"force_sum_per_atom\"]\n",
    "    num_scf_cycles_i = row_data_i.num_scf_cycles\n",
    "    # #####################################################\n",
    "\n",
    "    \n",
    "    if force_largest_i is not None:\n",
    "        if force_largest_i > 0.02:\n",
    "            print(name_i, \"|\", num_scf_cycles_i, \"|\", force_largest_i)\n",
    "            # print(path_i)\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i[\"job_id_max\"] = job_id_i\n",
    "        data_dict_i[\"force_largest\"] = force_largest_i\n",
    "        data_dict_i[\"force_sum\"] = force_sum_i\n",
    "        data_dict_i[\"force_sum_per_atom\"] = force_sum_per_atom_i\n",
    "        # #####################################################\n",
    "        data_dict_i.update(name_dict_i)\n",
    "        # #####################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #####################################################\n",
    "\n",
    "# #########################################################\n",
    "df = pd.DataFrame(data_dict_list)\n",
    "df = df.set_index(df_jobs_anal_i.index.names, drop=False)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if force_largest_i is not None:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_rerun = df[df.force_largest > 0.02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slac\ttefovuto_94\t16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df\n",
    "# df = df[\n",
    "#     (df[\"compenv\"] == \"slac\") &\n",
    "#     (df[\"slab_id\"] == \"tefovuto_94\") &\n",
    "#     (df[\"active_site\"] == 16.) &\n",
    "#     [True for i in range(len(df))]\n",
    "#     ]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /mnt/f/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking how many rows in `df_features_targets` include one of these not-really-finished jobs\n",
    "\n",
    "There are ~35 OER data points which use one of these not-really-finished jobs\n",
    "\n",
    "27 of these seem real, so 8 are jobs that are maybe still being processed or something"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_i, row_i in df_features_targets.iterrows():\n",
    "    tmp = 42\n",
    "\n",
    "    job_id_o_i = row_i[(\"data\", \"job_id_o\", \"\")]\n",
    "    job_id_oh_i = row_i[(\"data\", \"job_id_oh\", \"\")]\n",
    "    job_id_bare_i = row_i[(\"data\", \"job_id_bare\", \"\")]\n",
    "\n",
    "    o_not_good = job_id_o_i in df_to_rerun.job_id_max.tolist()\n",
    "    oh_not_good = job_id_oh_i in df_to_rerun.job_id_max.tolist()\n",
    "    bare_not_good = job_id_bare_i in df_to_rerun.job_id_max.tolist()\n",
    "\n",
    "    if o_not_good or oh_not_good or bare_not_good:\n",
    "        # print(name_i, o_not_good, oh_not_good, bare_not_good)\n",
    "        print(o_not_good, oh_not_good, bare_not_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "import os; import pickle\n",
    "\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/run_slabs/rerun_unfinished_slabs\",\n",
    "    \"out_data\")\n",
    "\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(os.path.join(directory, \"df_to_rerun__not_force_conv.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_to_rerun, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    y=df.force_largest.sort_values(ascending=False),\n",
    "    mode=\"markers\"\n",
    "    )\n",
    "data = [trace]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "# df.force_largest.sort_values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# incar_params_i = row_data_i.incar_params\n",
    "\n",
    "# incar_params_i[\"NSW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# ('oer_adsorbate', 'slac', 'vomelawi_63', 'bare', 60.0, 1)\n",
    "\n",
    "# # slac\twavihanu_77\tbare\t48.0\t1\n",
    "\n",
    "# df_ind = df_jobs_anal.index.to_frame()\n",
    "\n",
    "# df = df_ind\n",
    "# df = df[\n",
    "#     (df[\"job_type\"] == \"oer_adsorbate\") &\n",
    "#     (df[\"compenv\"] == \"slac\") &\n",
    "#     (df[\"slab_id\"] == \"wavihanu_77\") &\n",
    "#     (df[\"active_site\"] == 48.) &\n",
    "#     (df[\"ads\"] == \"bare\") &\n",
    "#     # (df[\"\"] == \"\") &\n",
    "#     [True for i in range(len(df))]\n",
    "#     ]\n",
    "# df_jobs_anal.loc[\n",
    "#     df.index\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_anal_i.iloc[0:1]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
