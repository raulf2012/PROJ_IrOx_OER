{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup new jobs to resubmit *O (and possibly *) from *OH to achieve better magmom matching\n",
    "---"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import copy\n",
    "import json\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.options.display.max_colwidth = 130\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_anal,\n",
    "    get_df_oer_groups,\n",
    "    get_df_jobs_oh_anal,\n",
    "    get_df_rerun_from_oh,\n",
    "    get_df_atoms_sorted_ind,\n",
    "    )\n",
    "from methods import get_df_coord\n",
    "from methods import get_df_jobs_paths\n",
    "from methods import get_df_jobs_data\n",
    "from methods import get_other_job_ids_in_set\n",
    "from methods import get_df_slab\n",
    "\n",
    "# #########################################################\n",
    "from dft_workflow_methods import get_job_spec_dft_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = get_df_jobs()\n",
    "\n",
    "df_oer_groups = get_df_oer_groups()\n",
    "\n",
    "df_jobs_oh_anal = get_df_jobs_oh_anal()\n",
    "\n",
    "df_rerun_from_oh = get_df_rerun_from_oh()\n",
    "df_rerun_from_oh_i = df_rerun_from_oh\n",
    "\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()\n",
    "\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "\n",
    "df_slab = get_df_slab()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rerun_from_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only setting up jobs for slab phase > 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slab_i = df_slab[df_slab.phase > 1]\n",
    "\n",
    "if verbose:\n",
    "    print(df_rerun_from_oh_i.shape[0])\n",
    "\n",
    "df_rerun_from_oh_i = df_rerun_from_oh_i.loc[\n",
    "    df_rerun_from_oh_i.slab_id.isin(df_slab_i.index)\n",
    "    ]\n",
    "if verbose:\n",
    "    print(df_rerun_from_oh_i.shape[0])\n",
    "\n",
    "df_rerun_from_oh_i = df_rerun_from_oh_i[df_rerun_from_oh_i.rerun_from_oh == True]\n",
    "if verbose:\n",
    "    print(df_rerun_from_oh_i.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rerun_from_oh_i = df_rerun_from_oh_i.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rerun_from_oh_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_dict = dict()\n",
    "indices_to_process = []\n",
    "# #########################################################\n",
    "for i_cnt, row_i in df_rerun_from_oh_i.iterrows():\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    slab_id_i = row_i.slab_id\n",
    "    active_site_i = row_i.active_site\n",
    "    job_id_most_stable_i = row_i.job_id_most_stable\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_jobs_i = df_jobs.loc[job_id_most_stable_i]\n",
    "    # #####################################################\n",
    "    att_num_i = row_jobs_i.att_num\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    # Getting the att_num values for *O  and bare jobs so new ones can be assigned \n",
    "    df_jobs_i = df_jobs[\n",
    "        (df_jobs.compenv == compenv_i) & \\\n",
    "        (df_jobs.slab_id == slab_id_i) & \\\n",
    "        [True for i in range(len(df_jobs))]\n",
    "        ]\n",
    "    df_jobs_o_i = df_jobs_i[\n",
    "        (df_jobs_i.ads == \"o\")\n",
    "        ]\n",
    "\n",
    "    df_jobs_bare_i = df_jobs_i[\n",
    "        (df_jobs_i.ads == \"bare\") & \\\n",
    "        (df_jobs_i.active_site == active_site_i) & \\\n",
    "        [True for i in range(len(df_jobs_i))]\n",
    "        ]\n",
    "\n",
    "    df_jobs_data_bare_i = df_jobs_data.loc[\n",
    "        df_jobs_bare_i.index\n",
    "        ]\n",
    "\n",
    "    df_restart_from_oh_i = df_jobs_data_bare_i[df_jobs_data_bare_i.rerun_from_oh == True]\n",
    "\n",
    "    job_ids_from_oh_restarted_jobs = []\n",
    "    for job_id_j in df_restart_from_oh_i.index.tolist():\n",
    "\n",
    "        tmp = get_other_job_ids_in_set(\n",
    "            job_id_j,\n",
    "            df_jobs=df_jobs,\n",
    "            )\n",
    "        job_ids_in_set = tmp.job_id.tolist()\n",
    "        job_ids_from_oh_restarted_jobs.extend(job_ids_in_set)\n",
    "\n",
    "    df_jobs_bare_i_2 = df_jobs_bare_i.drop(labels=job_ids_from_oh_restarted_jobs)\n",
    "\n",
    "    unique_att_nums_bare = list(df_jobs_bare_i_2.att_num.unique())\n",
    "    new_att_num_bare = np.max(unique_att_nums_bare) + 1\n",
    "\n",
    "    # print(\"new_att_num_bare:\", new_att_num_bare )\n",
    "\n",
    "\n",
    "\n",
    "    unique_bulk_ids = list(df_jobs_i.bulk_id.unique())\n",
    "    mess_i = \"iSSJfi\"\n",
    "    assert len(unique_bulk_ids) == 1, mess_i\n",
    "    bulk_id_i = unique_bulk_ids[0]\n",
    "\n",
    "    unique_facets = list(df_jobs_i.facet.unique())\n",
    "    mess_i = \"iSSJfi\"\n",
    "    assert len(unique_facets) == 1, mess_i\n",
    "    facet_i = unique_facets[0]\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Creating new directories\n",
    "    new_o_path = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        \"dft_workflow/run_slabs\",\n",
    "        \"run_o_covered/out_data/dft_jobs\",\n",
    "        compenv_i, bulk_id_i, facet_i,\n",
    "        \"active_site__\" + str(int(active_site_i)),\n",
    "        str(1).zfill(2) + \"_attempt\",\n",
    "        \"_01\",\n",
    "        )\n",
    "\n",
    "\n",
    "    new_bare_path = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        \"dft_workflow/run_slabs\",\n",
    "        \"run_bare_oh_covered/out_data/dft_jobs\",\n",
    "        compenv_i, bulk_id_i, facet_i,\n",
    "        \"bare\",\n",
    "        \"active_site__\" + str(int(active_site_i)),\n",
    "        str(new_att_num_bare).zfill(2) + \"_attempt\",\n",
    "        \"_01\",\n",
    "        )\n",
    "\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    my_file = Path(new_o_path)\n",
    "    o_path_does_not_exist = False\n",
    "    if not my_file.is_dir():\n",
    "        o_path_does_not_exist = True\n",
    "        \n",
    "    my_file = Path(new_bare_path)\n",
    "    bare_path_does_not_exist = False\n",
    "    if not my_file.is_dir():\n",
    "        bare_path_does_not_exist = True\n",
    "        \n",
    "    if o_path_does_not_exist or bare_path_does_not_exist:\n",
    "        indices_to_process.append(i_cnt)\n",
    "\n",
    "\n",
    "    data_dict_i[\"new_bare_path\"] = new_bare_path\n",
    "    data_dict_i[\"new_o_path\"] = new_o_path\n",
    "    data_dict_dict[i_cnt] = data_dict_i\n",
    "\n",
    "df_rerun_from_oh_i_2 = df_rerun_from_oh_i.loc[indices_to_process]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rerun_from_oh_i_2 = df_rerun_from_oh_i_2.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    df_rerun_from_oh_i_2.shape[0],\n",
    "    \" new jobs are being set up\",\n",
    "    sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rerun_from_oh_i_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories and initialize"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list_o = []\n",
    "data_dict_list_bare = []\n",
    "# #########################################################\n",
    "for i_cnt, row_i in df_rerun_from_oh_i_2.iterrows():\n",
    "    # print(\"i_cnt:\", i_cnt)\n",
    "\n",
    "    data_dict_o_i = dict()\n",
    "    data_dict_bare_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    slab_id_i = row_i.slab_id\n",
    "    active_site_i = row_i.active_site\n",
    "    job_id_most_stable_i = row_i.job_id_most_stable\n",
    "    # #####################################################\n",
    "\n",
    "    # #########################################################\n",
    "    row_jobs_i = df_jobs.loc[job_id_most_stable_i]\n",
    "    # #########################################################\n",
    "    att_num_i = row_jobs_i.att_num\n",
    "    # #########################################################\n",
    "\n",
    "    # #########################################################\n",
    "    # #########################################################\n",
    "    # #########################################################\n",
    "\n",
    "    # #########################################################\n",
    "    idx_i = pd.IndexSlice[compenv_i, slab_id_i, \"oh\", active_site_i, att_num_i]\n",
    "    row_atoms_i = df_atoms_sorted_ind.loc[idx_i, :]\n",
    "    # #########################################################\n",
    "    atoms_i = row_atoms_i.atoms_sorted_good\n",
    "    magmoms_sorted_good_i = row_atoms_i.magmoms_sorted_good\n",
    "    # #########################################################\n",
    "\n",
    "    if atoms_i.calc is None:\n",
    "        if magmoms_sorted_good_i is not None:\n",
    "            atoms_i.set_initial_magnetic_moments(magmoms_sorted_good_i)\n",
    "        else:\n",
    "            print(\"Not good there should be something here\")\n",
    "    else:\n",
    "        atoms_i.set_initial_magnetic_moments(\n",
    "            atoms_i.get_magnetic_moments()\n",
    "            )\n",
    "\n",
    "    # #########################################################\n",
    "    from local_methods import get_bare_o_from_oh\n",
    "    bare_o_out_dict =  get_bare_o_from_oh(\n",
    "        compenv=compenv_i,\n",
    "        slab_id=slab_id_i,\n",
    "        active_site=active_site_i,\n",
    "        att_num=att_num_i,\n",
    "        atoms=atoms_i,\n",
    "        )\n",
    "    atoms_bare = bare_o_out_dict[\"atoms_bare\"]\n",
    "    atoms_O = bare_o_out_dict[\"atoms_O\"]\n",
    "\n",
    "    write_atoms = True\n",
    "    if write_atoms:\n",
    "        atoms_i.write(\"__temp__/oh.traj\")\n",
    "        atoms_bare.write(\"__temp__/bare.traj\")\n",
    "        atoms_O.write(\"__temp__/o.traj\")\n",
    "\n",
    "    # #########################################################\n",
    "    # Getting the att_num values for *O  and bare jobs so new ones can be assigned \n",
    "    df_jobs_i = df_jobs[\n",
    "        (df_jobs.compenv == compenv_i) & \\\n",
    "        (df_jobs.slab_id == slab_id_i) & \\\n",
    "        # (df_jobs.active_site == active_site_i) & \\\n",
    "        [True for i in range(len(df_jobs))]\n",
    "        ]\n",
    "    df_jobs_o_i = df_jobs_i[\n",
    "        (df_jobs_i.ads == \"o\")\n",
    "        ]\n",
    "\n",
    "    # unique_att_nums_o = list(df_jobs_o_i.att_num.unique())\n",
    "    # new_att_num_o = np.max(unique_att_nums_o) + 1\n",
    "\n",
    "    df_jobs_data_o_i = df_jobs_data.loc[\n",
    "        df_jobs_o_i.index\n",
    "        ]\n",
    "\n",
    "    df_restart_from_oh_i = df_jobs_data_o_i[df_jobs_data_o_i.rerun_from_oh == True]\n",
    "\n",
    "    job_ids_from_oh_restarted_jobs = []\n",
    "    for job_id_j in df_restart_from_oh_i.index.tolist():\n",
    "\n",
    "        tmp = get_other_job_ids_in_set(\n",
    "            job_id_j,\n",
    "            df_jobs=df_jobs,\n",
    "            )\n",
    "        job_ids_in_set = tmp.job_id.tolist()\n",
    "        job_ids_from_oh_restarted_jobs.extend(job_ids_in_set)\n",
    "\n",
    "    df_jobs_o_i_2 = df_jobs_o_i.drop(labels=job_ids_from_oh_restarted_jobs)\n",
    "\n",
    "    unique_att_nums_o = list(df_jobs_o_i_2.att_num.unique())\n",
    "    new_att_num_o = np.max(unique_att_nums_o) + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i = data_dict_dict[i_cnt]\n",
    "    # #####################################################\n",
    "    new_o_path = data_dict_i[\"new_o_path\"]\n",
    "    new_bare_path = data_dict_i[\"new_bare_path\"]\n",
    "    # #####################################################\n",
    "\n",
    "    print(new_o_path)\n",
    "    print(new_bare_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(new_o_path):\n",
    "        os.makedirs(new_o_path)\n",
    "\n",
    "        # #################################################\n",
    "        # Copy dft script to job folder\n",
    "        copyfile(\n",
    "            os.path.join(os.environ[\"PROJ_irox_oer\"], \"dft_workflow/dft_scripts/slab_dft.py\"),\n",
    "            os.path.join(new_o_path, \"model.py\"),\n",
    "            )\n",
    "\n",
    "        # #################################################\n",
    "        # Copy atoms object to job folder\n",
    "        atoms_O.write(\n",
    "            os.path.join(new_o_path, \"init.traj\")\n",
    "            )\n",
    "        num_atoms_i = atoms_O.get_global_number_of_atoms()\n",
    "\n",
    "        # #################################################\n",
    "        data_dict_o_i[\"compenv\"] = compenv_i\n",
    "        data_dict_o_i[\"slab_id\"] = slab_id_i\n",
    "        data_dict_o_i[\"bulk_id\"] = bulk_id_i\n",
    "        data_dict_o_i[\"att_num\"] = new_att_num_o\n",
    "        data_dict_o_i[\"rev_num\"] = 1\n",
    "        data_dict_o_i[\"active_site\"] = \"NaN\"\n",
    "        data_dict_o_i[\"facet\"] = facet_i\n",
    "        data_dict_o_i[\"slab\"] = atoms_O\n",
    "        data_dict_o_i[\"num_atoms\"] = num_atoms_i\n",
    "        # data_dict_i[\"path_i\"] = path_i\n",
    "        data_dict_o_i[\"path_full\"] = new_o_path\n",
    "        # #############################################\n",
    "        data_dict_list_o.append(data_dict_o_i)\n",
    "        # #############################################\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(new_bare_path):\n",
    "        os.makedirs(new_bare_path)\n",
    "\n",
    "        # #############################################\n",
    "        # Copy dft script to job folder\n",
    "        copyfile(\n",
    "            os.path.join(os.environ[\"PROJ_irox_oer\"], \"dft_workflow/dft_scripts/slab_dft.py\"),\n",
    "            os.path.join(new_bare_path, \"model.py\"),\n",
    "            )\n",
    "\n",
    "        # #############################################\n",
    "        # Copy atoms object to job folder\n",
    "        atoms_bare.write(\n",
    "            os.path.join(new_bare_path, \"init.traj\")\n",
    "            )\n",
    "        num_atoms_i = atoms_bare.get_global_number_of_atoms()\n",
    "\n",
    "        # #############################################\n",
    "        data_dict_bare_i[\"compenv\"] = compenv_i\n",
    "        data_dict_bare_i[\"slab_id\"] = slab_id_i\n",
    "        data_dict_bare_i[\"bulk_id\"] = bulk_id_i\n",
    "        data_dict_bare_i[\"att_num\"] = new_att_num_o\n",
    "        data_dict_bare_i[\"rev_num\"] = 1\n",
    "        data_dict_bare_i[\"active_site\"] = \"NaN\"\n",
    "        data_dict_bare_i[\"facet\"] = facet_i\n",
    "        data_dict_bare_i[\"slab\"] = atoms_bare\n",
    "        data_dict_bare_i[\"num_atoms\"] = num_atoms_i\n",
    "        data_dict_bare_i[\"path_full\"] = new_bare_path\n",
    "        # #############################################\n",
    "        data_dict_list_bare.append(data_dict_bare_i)\n",
    "        # #############################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # Writing data_dict to mark that these were rerun from *OH\n",
    "    data_dict_out = dict(rerun_from_oh=True)\n",
    "\n",
    "    # #####################################################\n",
    "    slac_sub_queue_i = \"suncat3\"\n",
    "    dft_params_i = get_job_spec_dft_params(\n",
    "        compenv=compenv_i,\n",
    "        slac_sub_queue=slac_sub_queue_i,\n",
    "        )\n",
    "    dft_params_i[\"ispin\"] = 2\n",
    "\n",
    "    if os.path.exists(new_o_path):\n",
    "        # #################################################\n",
    "        with open(os.path.join(new_o_path, \"data_dict.json\"), \"w+\") as fle:\n",
    "            json.dump(data_dict_out, fle, indent=2, skipkeys=True)\n",
    "        # #################################################\n",
    "        with open(os.path.join(new_o_path, \"dft-params.json\"), \"w+\") as fle:\n",
    "            json.dump(dft_params_i, fle, indent=2, skipkeys=True)\n",
    "        # #################################################\n",
    "\n",
    "    if os.path.exists(new_bare_path):\n",
    "        # #################################################\n",
    "        with open(os.path.join(new_bare_path, \"data_dict.json\"), \"w+\") as fle:\n",
    "            json.dump(data_dict_out, fle, indent=2, skipkeys=True)\n",
    "        # #################################################\n",
    "        with open(os.path.join(new_bare_path, \"dft-params.json\"), \"w+\") as fle:\n",
    "            json.dump(dft_params_i, fle, indent=2, skipkeys=True)\n",
    "        # #################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"setup_new_jobs_from_oh.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
