{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup bare (*) slabs after the first *O slab is completed\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/run_slabs/run_bare_oh_covered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import MultiIndex\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_slab,\n",
    "    get_df_slabs_to_run,\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_anal,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_active_sites,\n",
    "    get_df_atoms_sorted_ind,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from dft_workflow_methods import get_job_spec_dft_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slac queue to submit to\n",
    "slac_sub_queue_i = \"suncat3\"  # 'suncat', 'suncat2', 'suncat3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# df_slab = get_df_slab()\n",
    "# df_slab = df_slab.set_index(\"slab_id\")\n",
    "\n",
    "# #########################################################\n",
    "df_jobs = get_df_jobs()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "# #########################################################\n",
    "df_active_sites = get_df_active_sites()\n",
    "\n",
    "# #########################################################\n",
    "df_slabs_to_run = get_df_slabs_to_run()\n",
    "df_slabs_to_run = df_slabs_to_run.set_index([\"compenv\", \"slab_id\", \"att_num\"], drop=False)\n",
    "\n",
    "# #########################################################\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering down to `oer_adsorbate` jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind = df_jobs_anal.index.to_frame()\n",
    "df_jobs_anal = df_jobs_anal.loc[\n",
    "    df_ind[df_ind.job_type == \"oer_adsorbate\"].index\n",
    "    ]\n",
    "df_jobs_anal = df_jobs_anal.droplevel(level=0)\n",
    "\n",
    "\n",
    "df_ind = df_atoms_sorted_ind.index.to_frame()\n",
    "df_atoms_sorted_ind = df_atoms_sorted_ind.loc[\n",
    "    df_ind[df_ind.job_type == \"oer_adsorbate\"].index\n",
    "    ]\n",
    "df_atoms_sorted_ind = df_atoms_sorted_ind.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/run_slabs/run_bare_oh_covered\",\n",
    "    \"out_data/dft_jobs\")\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/run_slabs/run_bare_oh_covered\",\n",
    "    \"out_data/__temp__\")\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "compenv = os.environ[\"COMPENV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods_run_slabs import get_systems_to_run_bare_and_oh\n",
    "\n",
    "indices_to_process = get_systems_to_run_bare_and_oh(df_jobs_anal)\n",
    "df_jobs_anal_i = df_jobs_anal.loc[indices_to_process]\n",
    "\n",
    "df_jobs_anal_i = df_jobs_anal_i.set_index(\n",
    "    df_jobs_anal_i.index.droplevel(level=[2, 3, ])\n",
    "    )\n",
    "df_jobs_anal_i.head()\n",
    "\n",
    "# #########################################################\n",
    "# Removing systems that were marked to be ignored\n",
    "from methods import get_systems_to_stop_run_indices\n",
    "indices_to_stop_running = get_systems_to_stop_run_indices(df_jobs_anal=df_jobs_anal)\n",
    "\n",
    "indices_to_drop = []\n",
    "for index_i in df_jobs_anal_i.index:\n",
    "    if index_i in indices_to_stop_running:\n",
    "        indices_to_drop.append(index_i)\n",
    "\n",
    "df_jobs_anal_i = df_jobs_anal_i.drop(index=indices_to_drop)\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "idx = np.intersect1d(\n",
    "    df_jobs_anal_i.index,\n",
    "    df_slabs_to_run.index,\n",
    "    )\n",
    "shared_indices = idx\n",
    "\n",
    "df_i = pd.concat([\n",
    "    df_slabs_to_run.loc[shared_indices].status,\n",
    "    df_jobs_anal_i.loc[shared_indices],\n",
    "    ], axis=1)\n",
    "df_i = df_i[df_i.status == \"ok\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for name_i, row_i in df_i.iterrows():\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    att_num_i = name_i[2]\n",
    "    # #####################################################\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_jobs_i = df_jobs.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    bulk_id_i = row_jobs_i.bulk_id\n",
    "    facet_i = row_jobs_i.facet\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_active_sites_i = df_active_sites.loc[slab_id_i]\n",
    "    # #####################################################\n",
    "    active_sites_unique_i = row_active_sites_i.active_sites_unique\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    for active_site_j in active_sites_unique_i:\n",
    "        rev = 1\n",
    "        path_i = os.path.join(\n",
    "            \"out_data/dft_jobs\", \n",
    "            compenv_i, bulk_id_i, facet_i,\n",
    "            \"bare\", \"active_site__\" + str(active_site_j).zfill(2),\n",
    "            str(att_num_i).zfill(2) + \"_attempt\",  # Attempt\n",
    "            \"_\" + str(rev).zfill(2),  # Revision\n",
    "            )\n",
    "\n",
    "        root_frag = \"dft_workflow/run_slabs/run_bare_oh_covered\"\n",
    "        path_full = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            root_frag,\n",
    "            path_i)\n",
    "\n",
    "        # print(path_full)\n",
    "        path_exists = False\n",
    "        if os.path.exists(path_full):\n",
    "            path_exists = True\n",
    "\n",
    "        # #################################################\n",
    "        data_dict_i = dict()\n",
    "        # #################################################\n",
    "        data_dict_i[\"compenv\"] = compenv_i\n",
    "        data_dict_i[\"slab_id\"] = slab_id_i\n",
    "        data_dict_i[\"att_num\"] = att_num_i\n",
    "        data_dict_i[\"active_site\"] = active_site_j\n",
    "        data_dict_i[\"path_short\"] = path_i\n",
    "        data_dict_i[\"path_full\"] = path_full\n",
    "        data_dict_i[\"path_exists\"] = path_exists\n",
    "        # #################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #################################################\n",
    "\n",
    "# #########################################################\n",
    "df_to_setup = pd.DataFrame(data_dict_list)\n",
    "df_to_setup = df_to_setup.set_index(\n",
    "    [\"compenv\", \"slab_id\", \"att_num\", \"active_site\", ], drop=False)\n",
    "\n",
    "df_to_setup_i = df_to_setup[df_to_setup.path_exists == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new bare (*) jobs setup: 0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Number of new bare (*) jobs setup:\",\n",
    "    df_to_setup_i.shape[0]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for name_i, row_i in df_to_setup_i.iterrows():\n",
    "    # #####################################################\n",
    "    name_dict_i = dict(zip(list(df_to_setup_i.index.names), name_i))\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    att_num_i = name_i[2]\n",
    "    active_site_i = name_i[3]\n",
    "    # #####################################################\n",
    "    path_full_i = row_i.path_full\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_atoms_i = df_atoms_sorted_ind.loc[\n",
    "        (compenv_i, slab_id_i, \"o\", \"NaN\", att_num_i, )]\n",
    "    # #####################################################\n",
    "    atoms_sorted_i = row_atoms_i.atoms_sorted_good\n",
    "    # #####################################################\n",
    "\n",
    "    if verbose:\n",
    "        print(name_i, \"|\", active_site_i, \"|\", path_full_i)\n",
    "        print(path_full_i)\n",
    "\n",
    "    os.makedirs(path_full_i)\n",
    "\n",
    "    # #####################################################\n",
    "    # Copy dft script to job folder\n",
    "    copyfile(\n",
    "        os.path.join(os.environ[\"PROJ_irox_oer\"], \"dft_workflow/dft_scripts/slab_dft.py\"),\n",
    "        os.path.join(path_full_i, \"model.py\"),\n",
    "        )\n",
    "\n",
    "    # #####################################################\n",
    "    # Removing atom to create \n",
    "    atoms_sorted_cpy_i = copy.deepcopy(atoms_sorted_i)\n",
    "    atoms_sorted_cpy_i.pop(i=active_site_i)\n",
    "    slab_bare_i = atoms_sorted_cpy_i\n",
    "\n",
    "    # #####################################################\n",
    "    # Copy atoms object to job folder\n",
    "    slab_bare_i.write(\n",
    "        os.path.join(path_full_i, \"init.traj\"))\n",
    "    slab_bare_i.write(\n",
    "        os.path.join(path_full_i, \"init.cif\"))\n",
    "    num_atoms_i = slab_bare_i.get_global_number_of_atoms()\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    data_dict_i.update(name_dict_i)\n",
    "    # #####################################################\n",
    "    data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "    data_dict_i[\"rev_num\"] = rev\n",
    "    data_dict_i[\"facet\"] = facet_i\n",
    "    data_dict_i[\"slab_bare\"] = slab_bare_i\n",
    "    data_dict_i[\"num_atoms\"] = num_atoms_i\n",
    "    data_dict_i[\"path_i\"] = path_i\n",
    "    data_dict_i[\"path_full\"] = path_full_i\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_new = pd.DataFrame(data_dict_list)\n",
    "\n",
    "# Create empty dataframe with columns if dataframe is empty\n",
    "if df_jobs_new.shape[0] == 0:\n",
    "    df_jobs_new = pd.DataFrame(\n",
    "        columns=[\"compenv\", \"slab_id\", \"att_num\", \"active_site\", ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing DFT parameters to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    num_atoms = row_i.num_atoms\n",
    "    path_i = row_i.path_i\n",
    "    path_full_i = row_i.path_full\n",
    "    # ####################################################\n",
    "\n",
    "    dft_params_i = get_job_spec_dft_params(\n",
    "        compenv=compenv_i,\n",
    "        slac_sub_queue=slac_sub_queue_i,\n",
    "        )\n",
    "    dft_params_i[\"ispin\"] = 2\n",
    "\n",
    "    # #####################################################\n",
    "    if verbose:\n",
    "        print(path_full_i)\n",
    "\n",
    "    with open(os.path.join(path_full_i, \"dft-params.json\"), \"w+\") as fle:\n",
    "        json.dump(dft_params_i, fle, indent=2, skipkeys=True)\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"att_num\"] = att_num_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "    data_dict_i[\"dft_params\"] = dft_params_i\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_dft_params = pd.DataFrame(data_dict_list)\n",
    "\n",
    "# Create empty dataframe with columns if dataframe is empty\n",
    "if df_dft_params.shape[0] == 0:\n",
    "    df_dft_params = pd.DataFrame(columns=[\"compenv\", \"slab_id\", \"att_num\", \"active_site\", ])\n",
    "\n",
    "keys = [\"compenv\", \"slab_id\", \"att_num\", \"active_site\"]\n",
    "df_dft_params = df_dft_params.set_index(keys, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "Run time: 0.045 min\n",
      "setup_dft_bare.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"setup_dft_bare.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
