{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup/continue jobs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/f/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/run_slabs\n",
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import dictdiffer\n",
    "import json\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.options.display.max_colwidth = 120\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_jobs_anal,\n",
    "    get_df_slab,\n",
    "    cwd,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from dft_workflow_methods import (\n",
    "    is_job_understandable,\n",
    "    job_decision,\n",
    "    transfer_job_files_from_old_to_new,\n",
    "    is_job_compl_done,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_jobs_on_clus__all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST_no_file_ops = True  # True if just testing around, False for production mode\n",
    "TEST_no_file_ops = False\n",
    "\n",
    "# Slac queue to submit to\n",
    "slac_sub_queue = \"suncat3\"  # 'suncat', 'suncat2', 'suncat3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = get_df_jobs()\n",
    "\n",
    "df_jobs_data = get_df_jobs_data(drop_cols=False)\n",
    "\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "df_slab = get_df_slab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_on_clus__all = get_df_jobs_on_clus__all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering down to `oer_adsorbate` jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this to only rerun `oer_adsorbate` jobs\n",
    "# # Otherwise both dos_bader and oer_adsorbate jobs are rerun\n",
    "\n",
    "# df_ind = df_jobs_anal.index.to_frame()\n",
    "\n",
    "# df_jobs_anal = df_jobs_anal.loc[\n",
    "#     df_ind[df_ind.job_type == \"oer_adsorbate\"].index\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing systems that were marked to be ignored\n",
    "from methods import get_systems_to_stop_run_indices\n",
    "\n",
    "indices_to_stop_running = get_systems_to_stop_run_indices(df_jobs_anal=df_jobs_anal)\n",
    "df_jobs_anal = df_jobs_anal.drop(index=indices_to_stop_running)\n",
    "\n",
    "df_resubmit = df_jobs_anal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter `df_resubmit` to only rows that are to be resubmitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids_to_force_resubmit = [\n",
    "    # Wed May 12 18:57:44 PDT 2021\n",
    "    # \"luhegesu_97\",\n",
    "\n",
    "    # Sat May 22 22:28:51 PDT 2021\n",
    "    \"renonero_79\",\n",
    "    \"sahutoho_38\",\n",
    "    \"nasowihu_34\",\n",
    "    \"muvawosa_27\",\n",
    "    \"gudehuhu_68\",\n",
    "    \"kuwimolu_53\",\n",
    "    \"digemobe_04\",\n",
    "    \"peremese_97\",\n",
    "    \"hiramama_87\",\n",
    "    \"wusifiha_94\",\n",
    "    \"tapidudi_05\",\n",
    "    \"resopuke_29\",\n",
    "    \"rinisohe_79\",\n",
    "    \"garahipo_46\",\n",
    "    \"gogihaba_12\",\n",
    "    \"ratorage_16\",\n",
    "\n",
    "\n",
    "    # # #####################################################\n",
    "    # # \"fukudiko_66\",\n",
    "    # # \"fipipida_61\",\n",
    "    # # \"tibunane_36\",\n",
    "\n",
    "    # # #####################################################\n",
    "    # \"tisakuri_50\",\n",
    "    # \"hukemena_85\",\n",
    "    # \"sewedawu_95\",\n",
    "    # \"rugagumu_31\",\n",
    "    # \"gitogahu_48\",\n",
    "    # \"bigufoha_89\",\n",
    "    # \"pubahadu_79\",\n",
    "    # \"hebomume_93\",\n",
    "    # \"makaledi_83\",\n",
    "    # \"fevupilo_12\",\n",
    "    # \"ruvanaka_31\",\n",
    "    # \"wirurabi_88\",\n",
    "    # \"ludekatu_27\",\n",
    "    # \"hogigova_47\",\n",
    "    # \"koboguhi_40\",\n",
    "    # \"kalidasa_91\",\n",
    "    # \"larodaru_75\",\n",
    "    # # ############\n",
    "    # \"nonogase_08\",\n",
    "    # \"gorofiwe_14\",\n",
    "    # \"gatevehu_95\",\n",
    "    # \"kebolapu_40\",\n",
    "    # \"pesusero_02\",\n",
    "    # # ############\n",
    "    # \"parihobe_18\",\n",
    "    # \"fuhegara_35\",\n",
    "    # \"ropirema_45\",\n",
    "    # \"sisipule_40\",\n",
    "    # \"valagane_51\",\n",
    "    # \"revotaho_43\",\n",
    "\n",
    "    # \"pigenipi_49\",\n",
    "    # \"nitisule_36\",\n",
    "\n",
    "    # # ############\n",
    "    # \"budubadi_27\",\n",
    "    # \"mudebupo_43\",\n",
    "\n",
    "    # # ############\n",
    "    # \"nunidaha_71\",\n",
    "    # \"kegifopa_85\",\n",
    "\n",
    "    # # ############\n",
    "    # \"liludigu_62\",\n",
    "    # \"sahutoho_38\",\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading resubmit data from `rerun_unfinished_slabs.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "import pickle; import os\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/run_slabs/rerun_unfinished_slabs\",\n",
    "    \"out_data\")\n",
    "path_i = os.path.join(\n",
    "    directory,\n",
    "    \"df_to_rerun__not_force_conv.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_to_rerun__not_force_conv = pickle.load(fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resubmit_tmp = copy.deepcopy(df_resubmit)\n",
    "\n",
    "# #########################################################\n",
    "mask_list = []\n",
    "\n",
    "# for i in df_resubmit.decision.tolist():\n",
    "for name_i, row_i in df_resubmit.iterrows():\n",
    "    decision_i = row_i.decision\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "\n",
    "    # if \"resubmit\" in decision_i or \\\n",
    "    #    job_id_max_i in job_ids_to_force_resubmit or \\\n",
    "    #    name_i in df_to_rerun__not_force_conv.index:\n",
    "    #     mask_list.append(True)\n",
    "\n",
    "        \n",
    "    if \"resubmit\" in decision_i or job_id_max_i in job_ids_to_force_resubmit:\n",
    "        mask_list.append(True)\n",
    "    elif name_i in df_to_rerun__not_force_conv.index:\n",
    "        row_rerun_i = df_to_rerun__not_force_conv.loc[name_i]\n",
    "        job_id_max__rerun = row_rerun_i.job_id_max\n",
    "\n",
    "        if job_id_max_i == job_id_max__rerun:\n",
    "            mask_list.append(True)\n",
    "        else:\n",
    "            mask_list.append(False)\n",
    "\n",
    "    else:\n",
    "        mask_list.append(False)\n",
    "\n",
    "df_resubmit = df_resubmit_tmp[mask_list]\n",
    "df_nosubmit = df_resubmit_tmp[np.invert(mask_list)]\n",
    "\n",
    "# print(\"df_resubmit.shape:\", df_resubmit.shape)\n",
    "# print(\"df_nosubmit.shape:\", df_nosubmit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing `systems_to_stop_running`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are jobs being left idle, nothing to do, fix it\n",
      "['pulefevo_10', 'bewowave_59']\n",
      "\n",
      "********************\n",
      "gdrive_path_i: dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/mkmsvkcyc5/110/active_site__32/01_attempt/_04\n",
      "job_id_i: pulefevo_10\n",
      "\n",
      "\n",
      "********************\n",
      "gdrive_path_i: dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/sherlock/mj7wbfb5nt/112/active_site__67/01_attempt/_06\n",
      "job_id_i: bewowave_59\n",
      "\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/spool/slurmd/job19347341/slurm_script\", line 26, in <module>\n",
      "    from ase_modules.ase_methods import clean_up_dft, get_slab_kpts\n",
      "ModuleNotFoundError: No module named 'ase_modules.ase_methods'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i = df_nosubmit[df_nosubmit.job_completely_done == False]\n",
    "\n",
    "index_mask = []\n",
    "for name_i, row_i in df_i.iterrows():\n",
    "    decision_i = row_i.decision\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "\n",
    "    if job_id_max_i not in df_jobs_on_clus__all.job_id:\n",
    "\n",
    "        if len(decision_i) == 0:\n",
    "            index_mask.append(name_i)\n",
    "        else:\n",
    "            add_name_to_mask = False\n",
    "            for decision_str_j in decision_i:\n",
    "                str_frags = [\"not understandable\", ]\n",
    "                for str_i in str_frags:\n",
    "                    if str_i in decision_str_j:\n",
    "                        add_name_to_mask = True\n",
    "\n",
    "            if add_name_to_mask:\n",
    "                index_mask.append(name_i)\n",
    "\n",
    "df_i = df_i.loc[index_mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "# Only rerunning from slab generations > 1\n",
    "def method(row_i):\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    row_jobs_i = df_jobs.loc[job_id_max_i]\n",
    "    slab_id_i = row_jobs_i.slab_id\n",
    "    row_slab_i = df_slab.loc[slab_id_i]\n",
    "    phase_i = row_slab_i.phase\n",
    "    return(phase_i)\n",
    "df_i[\"phase\"] = df_i.apply(method,axis=1)\n",
    "df_i = df_i[df_i.phase > 1]\n",
    "\n",
    "# #########################################################\n",
    "if df_i.shape[0] > 0:\n",
    "    print(\"There are jobs being left idle, nothing to do, fix it\")\n",
    "    print(df_i.job_id_max.tolist())\n",
    "    print(\"\")\n",
    "\n",
    "    job_ids = df_i.job_id_max.tolist()\n",
    "\n",
    "    df_jobs_i = df_jobs.loc[job_ids]\n",
    "\n",
    "    for job_id_i, row_i in df_jobs_i.iterrows():\n",
    "        row_path_i = df_jobs_paths.loc[job_id_i]\n",
    "        gdrive_path_i = row_path_i.gdrive_path\n",
    "\n",
    "        path_i = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            gdrive_path_i,\n",
    "            \"job.err\")\n",
    "\n",
    "        if verbose:\n",
    "            print(20 * \"*\")\n",
    "            print(\"gdrive_path_i:\", gdrive_path_i)\n",
    "            print(\"job_id_i:\", job_id_i)\n",
    "            print(\"\")\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "            if verbose:\n",
    "                my_file = Path(path_i)\n",
    "                if my_file.is_file():\n",
    "\n",
    "                    with open(path_i, \"r\") as f:\n",
    "                        lines = f.read().splitlines()\n",
    "\n",
    "                    tmp = [print(i) for i in lines[-10:]]\n",
    "                    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new job directories and initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "('dos_bader', 'nersc', 'buvivore_13', 'o', 38.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/cg8p7fxq65/010/active_site__38/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'hesegula_40', 'o', 92.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/clc2b1mavs/010/active_site__92/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'hesegula_40', 'o', 93.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/clc2b1mavs/010/active_site__93/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'hesegula_40', 'o', 94.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/clc2b1mavs/010/active_site__94/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'kererape_22', 'o', 90.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/b19q9p6k72/1-20/active_site__90/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'kererape_22', 'o', 94.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/b19q9p6k72/1-20/active_site__94/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'legofufi_61', 'o', 88.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/85z4msnl6o/311/active_site__88/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'lirilapa_78', 'o', 81.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/mj7wbfb5nt/123/active_site__81/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'lirilapa_78', 'o', 84.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/mj7wbfb5nt/123/active_site__84/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'lirilapa_78', 'o', 91.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/mj7wbfb5nt/123/active_site__91/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'pebitiru_79', 'o', 75.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/xemg9wb27y/2-1-12/active_site__75/01_attempt/_03\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'winomuvi_99', 'o', 83.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/m2bs8w82x5/112/active_site__83/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'nersc', 'winomuvi_99', 'o', 95.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/m2bs8w82x5/112/active_site__95/01_attempt/_03\n",
      "****************************************\n",
      "('dos_bader', 'sherlock', 'sodakiva_90', 'o', 52.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/sherlock/mkbj6e6e9p/010/active_site__52/01_attempt/_02\n",
      "****************************************\n",
      "('dos_bader', 'sherlock', 'tagediso_07', 'o', 42.0, 1)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/sherlock/zwvqnhbk7f/010/active_site__42/01_attempt/_02\n",
      "****************************************\n",
      "('oer_adsorbate', 'sherlock', 'begakura_66', 'bare', 61.0, 2)\n",
      "/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_slabs/run_bare_oh_covered/out_data/dft_jobs/sherlock/xlziv2zr9g/010/bare/active_site__61/02_attempt/_05\n"
     ]
    }
   ],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, (name_i, row_i) in enumerate(df_resubmit.iterrows()):\n",
    "\n",
    "    data_dict_i = dict()\n",
    "    if verbose:\n",
    "        print(40 * \"*\")\n",
    "        print(name_i)\n",
    "\n",
    "    # #####################################################\n",
    "    job_type_i = name_i[0]\n",
    "    compenv_i = name_i[1]\n",
    "    slab_id_i = name_i[2]\n",
    "    ads_i = name_i[3]\n",
    "    active_site_i = name_i[4]\n",
    "    att_num_i = name_i[5]\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    dft_params_new = row_i.dft_params_new\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_i = df_jobs[df_jobs.compenv == compenv_i]\n",
    "    row_jobs_i = df_jobs_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    rev_num = row_jobs_i.rev_num\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_paths_i = df_jobs_paths[df_jobs_paths.compenv == compenv_i]\n",
    "    row_paths_max_i = df_jobs_paths_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    gdrive_path = row_paths_max_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_data_i = df_jobs_data[df_jobs_data.compenv == compenv_i]\n",
    "    row_data_max_i = df_jobs_data_i.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    num_steps = row_data_max_i.num_steps\n",
    "    incar_params = row_data_max_i.incar_params\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    path_i = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        gdrive_path)\n",
    "\n",
    "\n",
    "    from pathlib import Path\n",
    "    outcar_is_there = False\n",
    "    my_file = Path(os.path.join(path_i, \"OUTCAR\"))\n",
    "    if my_file.is_file():\n",
    "        outcar_is_there = True\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Copy files to new job dir\n",
    "    new_path_i = \"/\".join(path_i.split(\"/\")[0:-1] + [\"_\" + str(rev_num + 1).zfill(2)])\n",
    "\n",
    "    if not TEST_no_file_ops:\n",
    "        if not os.path.exists(new_path_i):\n",
    "            print(new_path_i)\n",
    "            os.makedirs(new_path_i)\n",
    "\n",
    "            \n",
    "    files_to_transfer_for_new_job = [\n",
    "\n",
    "        [\n",
    "            os.path.join(path_i, \"model.py\"),\n",
    "            \"model.py\"],\n",
    "        [\n",
    "            \"dir_dft_params/dft-params.json\",\n",
    "             \"dft-params.json\"],\n",
    "\n",
    "        \"WAVECAR\",\n",
    "        \"dft-params.json\",\n",
    "        \"data_dict.json\",\n",
    "        ]\n",
    "\n",
    "\n",
    "    contcar_is_there = False\n",
    "    my_file = Path(os.path.join(path_i, \"CONTCAR\"))\n",
    "    if my_file.is_file():\n",
    "        contcar_is_there = True\n",
    "\n",
    "    not_ready = False\n",
    "    if num_steps > 0 and not contcar_is_there:\n",
    "        print(\"num_steps > 0 but CONTCAR is not avail\", name_i)\n",
    "        not_ready = True\n",
    "\n",
    "\n",
    "    if not not_ready:\n",
    "        # #####################################################\n",
    "        with cwd(path_i):\n",
    "            if num_steps > 0:\n",
    "                atoms = io.read(\"CONTCAR\")\n",
    "                atoms.write(\"contcar_out.traj\")\n",
    "                files_to_transfer_for_new_job.append(\n",
    "                    [\"contcar_out.traj\", \"init.traj\"])\n",
    "\n",
    "\n",
    "            else:\n",
    "                atoms = io.read(\"init.traj\")\n",
    "                files_to_transfer_for_new_job.append(\n",
    "                    \"init.traj\"\n",
    "                    )\n",
    "\n",
    "            # If spin-polarized calculation then get magmoms from prev. job and pass to new job\n",
    "            if outcar_is_there:\n",
    "                if incar_params[\"ISPIN\"] == 2:\n",
    "                    if num_steps > 0:\n",
    "                        atoms_outcar = io.read(\"OUTCAR\")\n",
    "                        magmoms_i_tmp = atoms_outcar.get_magnetic_moments()\n",
    "\n",
    "                        data_path = os.path.join(\"out_data/magmoms_out.json\")\n",
    "                        with open(data_path, \"w\") as outfile:\n",
    "                            json.dump(magmoms_i_tmp.tolist(), outfile)\n",
    "\n",
    "                        files_to_transfer_for_new_job.append(\n",
    "                            [\"out_data/magmoms_out.json\", \"magmoms.json\"])\n",
    "                # If previous job was the non-spin-pol calc, then copy through the magmoms.json file\n",
    "                elif incar_params[\"ISPIN\"] == 1:\n",
    "                    files_to_transfer_for_new_job.append(\"magmoms.json\")\n",
    "            else:\n",
    "                files_to_transfer_for_new_job.append(\"magmoms.json\")\n",
    "\n",
    "\n",
    "            num_atoms = atoms.get_global_number_of_atoms()\n",
    "\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        if not TEST_no_file_ops:\n",
    "            transfer_job_files_from_old_to_new(\n",
    "                path_i=path_i,\n",
    "                new_path_i=new_path_i,\n",
    "                files_to_transfer_for_new_job=files_to_transfer_for_new_job,\n",
    "                )\n",
    "\n",
    "        # #####################################################\n",
    "        if not TEST_no_file_ops:\n",
    "            dft_params_path_i = os.path.join(\n",
    "                new_path_i,\n",
    "                \"dft-params.json\")\n",
    "            with open(dft_params_path_i, \"r\") as fle:\n",
    "                dft_params_current = json.load(fle)\n",
    "\n",
    "            # Update previous DFT parameters with new ones\n",
    "            dft_params_current.update(dft_params_new)\n",
    "\n",
    "            with open(dft_params_path_i, \"w\") as outfile:\n",
    "                json.dump(dft_params_current, outfile, indent=2)\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i[\"path_i\"] = new_path_i\n",
    "        data_dict_i[\"num_atoms\"] = num_atoms\n",
    "        data_dict_list.append(data_dict_i)\n",
    "\n",
    "        # else:\n",
    "        #     print(\"OUTCAR file wasn't where it should be, probably need rclone\")\n",
    "        #     print(\"name_i:\", name_i)\n",
    "\n",
    "# #########################################################\n",
    "df_sub = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "Run time: 0.118 min\n",
      "setup_new_jobs.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"setup_new_jobs.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
