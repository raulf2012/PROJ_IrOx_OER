{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DOS/Bader Jobs for Finished OER Sets\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/f/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/run_slabs/setup_dos_bader\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from shutil import copyfile\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.options.display.max_colwidth = 100\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs,\n",
    "    get_df_features_targets,\n",
    "    get_df_jobs_data,\n",
    "    get_df_atoms_sorted_ind,\n",
    "    get_df_jobs_paths,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_targets = get_df_features_targets()\n",
    "df_features_targets_i = df_features_targets\n",
    "\n",
    "df_jobs = get_df_jobs()\n",
    "\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "\n",
    "df_atoms = get_df_atoms_sorted_ind()\n",
    "\n",
    "df_paths = get_df_jobs_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess `df_features_targets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raul_desktop/anaconda3/envs/PROJ_irox_oer/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2858: PerformanceWarning:\n",
      "\n",
      "indexing past lexsort depth may impact performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_features_targets_i = df_features_targets_i[\n",
    "#     df_features_targets_i.data.all_done == True]\n",
    "\n",
    "df = df_features_targets_i\n",
    "df = df[\n",
    "    (df[\"data\", \"all_done\"] == True) &\n",
    "    (df[\"data\", \"from_oh__o\"] == True) &\n",
    "    # (df[\"data\", \"from_oh__bare\"] == True) &\n",
    "    [True for i in range(len(df))]\n",
    "    ]\n",
    "df_features_targets_i = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring which systems to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_cnt: 4 index_i: ('nersc', 'gekawore_16', 84.0)\n",
      "i_cnt: 5 index_i: ('nersc', 'gekawore_16', 86.0)\n",
      "i_cnt: 6 index_i: ('nersc', 'gekawore_16', 88.0)\n",
      "i_cnt: 7 index_i: ('nersc', 'giworuge_14', 84.0)\n",
      "i_cnt: 16 index_i: ('nersc', 'legofufi_61', 88.0)\n",
      "i_cnt: 17 index_i: ('nersc', 'legofufi_61', 90.0)\n",
      "i_cnt: 18 index_i: ('nersc', 'legofufi_61', 91.0)\n",
      "i_cnt: 19 index_i: ('nersc', 'legofufi_61', 93.0)\n",
      "i_cnt: 20 index_i: ('nersc', 'legofufi_61', 95.0)\n",
      "i_cnt: 21 index_i: ('nersc', 'letapivu_80', 80.0)\n",
      "i_cnt: 22 index_i: ('nersc', 'letapivu_80', 81.0)\n",
      "i_cnt: 23 index_i: ('nersc', 'letapivu_80', 82.0)\n",
      "i_cnt: 24 index_i: ('nersc', 'letapivu_80', 83.0)\n",
      "i_cnt: 25 index_i: ('nersc', 'letapivu_80', 84.0)\n",
      "i_cnt: 26 index_i: ('nersc', 'letapivu_80', 86.0)\n",
      "i_cnt: 27 index_i: ('nersc', 'letapivu_80', 87.0)\n",
      "i_cnt: 28 index_i: ('nersc', 'lirilapa_78', 81.0)\n",
      "i_cnt: 29 index_i: ('nersc', 'lirilapa_78', 84.0)\n",
      "i_cnt: 33 index_i: ('nersc', 'mututesi_43', 81.0)\n",
      "i_cnt: 37 index_i: ('nersc', 'winomuvi_99', 82.0)\n",
      "i_cnt: 39 index_i: ('nersc', 'winomuvi_99', 90.0)\n",
      "i_cnt: 40 index_i: ('nersc', 'winomuvi_99', 93.0)\n",
      "i_cnt: 43 index_i: ('nersc', 'winomuvi_99', 96.0)\n",
      "i_cnt: 338 index_i: ('slac', 'wufulafe_03', 58.0)\n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "data_dict_dict = dict()\n",
    "indices_to_process = []\n",
    "# #########################################################\n",
    "for i_cnt, (ind_i, row_i) in enumerate(df_features_targets_i.iterrows()):\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = ind_i[0]\n",
    "    slab_id_i = ind_i[1]\n",
    "    active_site_i = ind_i[2]\n",
    "    # #####################################################\n",
    "    job_id_o_i = row_i[(\"data\", \"job_id_o\", \"\", )]\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    row_jobs_i = df_jobs.loc[job_id_o_i]\n",
    "    # #####################################################\n",
    "    active_site_i = row_jobs_i.active_site\n",
    "    bulk_id_i = row_jobs_i.bulk_id\n",
    "    facet_i = row_jobs_i.facet\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_data_i = df_jobs_data.loc[job_id_o_i]\n",
    "    # #####################################################\n",
    "    rerun_from_oh_i = row_data_i.rerun_from_oh\n",
    "    # #####################################################\n",
    "\n",
    "    assert rerun_from_oh_i, \"filtering by all_done should mean that all *O are rerun from *OH\"\n",
    "    # TEMP\n",
    "    # print(active_site_i)\n",
    "\n",
    "    assert active_site_i != \"NaN\", \"Active site should be number, rerun from *OH so should have one\"\n",
    "\n",
    "    # #####################################################\n",
    "    # Creating new directories\n",
    "    path_new_i = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        \"dft_workflow/run_dos_bader\",\n",
    "        \"run_o_covered/out_data/dft_jobs\",\n",
    "        compenv_i, bulk_id_i, facet_i,\n",
    "        \"active_site__\" + str(int(active_site_i)),\n",
    "        str(1).zfill(2) + \"_attempt\",\n",
    "        \"_01\",\n",
    "        # \"_01.tmp\",\n",
    "        )\n",
    "\n",
    "    my_file = Path(path_new_i)\n",
    "    path_does_not_exist = False\n",
    "    if not my_file.is_dir():\n",
    "        if verbose:\n",
    "            print(\"i_cnt:\", i_cnt, \"index_i:\", ind_i)\n",
    "            # print(\"path_new_i:\", path_new_i)\n",
    "\n",
    "        path_does_not_exist = True\n",
    "\n",
    "    if path_does_not_exist:\n",
    "        indices_to_process.append(ind_i)\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    data_dict_i[\"path_new\"] = path_new_i\n",
    "    data_dict_i[\"job_id_o\"] = job_id_o_i\n",
    "    data_dict_i[\"att_num\"] = 1\n",
    "    # #####################################################\n",
    "    data_dict_dict[ind_i] = data_dict_i\n",
    "    # #####################################################\n",
    "\n",
    "# #########################################################\n",
    "df_features_targets_i_2 = df_features_targets_i.loc[indices_to_process]\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 78)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features_targets_i_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for ind_i, row_i in df_features_targets_i_2.iterrows():\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = ind_i[0]\n",
    "    slab_id_i = ind_i[1]\n",
    "    active_site_i = ind_i[2]\n",
    "    # #####################################################\n",
    "    data_dict_prev = data_dict_dict[ind_i]\n",
    "    # #####################################################\n",
    "    path_new_i = data_dict_prev[\"path_new\"]\n",
    "    job_id_o_i = data_dict_prev[\"job_id_o\"]\n",
    "    att_num_i = data_dict_prev[\"att_num\"]\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_jobs_i = df_jobs.loc[job_id_o_i]\n",
    "    # #####################################################\n",
    "    active_site_i = row_jobs_i.active_site\n",
    "    bulk_id_i = row_jobs_i.bulk_id\n",
    "    facet_i = row_jobs_i.facet\n",
    "    att_num_orig_i = row_jobs_i.att_num\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    # atoms_name_i = (compenv_i, slab_id_i, \"o\", active_site_i, att_num_orig_i, )\n",
    "    atoms_name_i = (\"oer_adsorbate\", compenv_i, slab_id_i, \"o\", active_site_i, att_num_orig_i, )\n",
    "    row_atoms_i = df_atoms.loc[atoms_name_i]\n",
    "    # #####################################################\n",
    "    atoms_i = row_atoms_i.atoms_sorted_good\n",
    "    was_sorted_i = row_atoms_i.was_sorted\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_paths_i = df_paths.loc[job_id_o_i]\n",
    "    # #####################################################\n",
    "    gdrive_path_i = row_paths_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    if was_sorted_i:\n",
    "        magmoms_sorted_good = row_atoms_i.magmoms_sorted_good\n",
    "        magmoms_i = magmoms_sorted_good\n",
    "    else:\n",
    "        magmoms_i = atoms_i.get_magnetic_moments()\n",
    "\n",
    "\n",
    "    if not os.path.exists(path_new_i):\n",
    "        os.makedirs(path_new_i)\n",
    "\n",
    "        # #################################################\n",
    "        # Copy dft script to job folder\n",
    "        copyfile(\n",
    "            os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                \"dft_workflow/dft_scripts/dos_scripts\",\n",
    "                \"model_dos.py\",\n",
    "                ),\n",
    "            os.path.join(path_new_i, \"model.py\"),\n",
    "            )\n",
    "        copyfile(\n",
    "            os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                \"dft_workflow/dft_scripts/dos_scripts\",\n",
    "                \"model_dos.py\",\n",
    "                ),\n",
    "            os.path.join(path_new_i, \"model_dos.py\"),\n",
    "            )\n",
    "\n",
    "        # #################################################\n",
    "        # Copy dos__calc_settings.py to job folder\n",
    "        copyfile(\n",
    "            os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                \"dft_workflow/dft_scripts/dos_scripts\",\n",
    "                \"dos__calc_settings.py\",\n",
    "                ),\n",
    "            os.path.join(path_new_i, \"dos__calc_settings.py\"),\n",
    "            )\n",
    "\n",
    "\n",
    "        atoms_i.set_initial_magnetic_moments(magmoms_i)\n",
    "        atoms_i.write(os.path.join(path_new_i, \"init.traj\"))\n",
    "\n",
    "        num_atoms_i = atoms_i.get_global_number_of_atoms()\n",
    "\n",
    "\n",
    "        # ---------------------------------------\n",
    "        # Moving dft-params.json file to new dir\n",
    "        file__dft_params = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            gdrive_path_i,\n",
    "            \"dft-params.json\",\n",
    "            )\n",
    "\n",
    "        copyfile(\n",
    "            file__dft_params,\n",
    "            os.path.join(path_new_i, \"dft-params.json\"),\n",
    "            )\n",
    "\n",
    "        # #################################################\n",
    "        data_dict_i[\"compenv\"] = compenv_i\n",
    "        data_dict_i[\"slab_id\"] = slab_id_i\n",
    "        data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "        data_dict_i[\"att_num\"] = att_num_i\n",
    "        data_dict_i[\"rev_num\"] = 1\n",
    "        data_dict_i[\"facet\"] = facet_i\n",
    "        data_dict_i[\"slab\"] = atoms_i\n",
    "        data_dict_i[\"num_atoms\"] = num_atoms_i\n",
    "        data_dict_i[\"path_new\"] = path_new_i\n",
    "        data_dict_i[\"job_id_orig\"] = job_id_o_i\n",
    "        # #################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #################################################\n",
    "\n",
    "        data_path = os.path.join(path_new_i, \"data_dict.json\")\n",
    "        with open(data_path, \"w\") as outfile:\n",
    "            json.dump(dict(job_id_orig=job_id_o_i), outfile, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df = pd.DataFrame(data_dict_list)\n",
    "# #########################################################\n",
    "\n",
    "\n",
    "# if True:\n",
    "#     shutil.rmtree(path_new_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/f/GDrive/norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/nersc/m2bs8w82x5/112/active_site__96/01_attempt/_01'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_new_i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "Run time: 0.134 min\n",
      "setup_dos_bader.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"setup_dos_bader.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_features_targets_i.loc[\n",
    "#     # ('nersc', 'gekawore_16', 84.0)\n",
    "#     # ('nersc', 'gekawore_16', 81.0)\n",
    "#     ('nersc', 'giworuge_14', 81.0)\n",
    "#     ][\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_features_targets_i.loc[\n",
    "#     # ('nersc', 'gekawore_16', 84.0)\n",
    "#     ('nersc', 'gekawore_16', 81.0)\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df_features_targets_i.loc[\n",
    "#     # ('nersc', 'gekawore_16', 84.0)\n",
    "#     # ('nersc', 'gekawore_16', 81.0)\n",
    "#     ('nersc', 'giworuge_14', 81.0)\n",
    "#     ]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
