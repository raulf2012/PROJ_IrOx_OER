{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup initial *O slabs to run\n",
    "---"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ase import io\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_slab,\n",
    "    get_df_jobs,\n",
    "    )\n",
    "\n",
    "from proj_data import metal_atom_symbol\n",
    "\n",
    "# #########################################################\n",
    "from dft_workflow_methods import (\n",
    "    get_job_spec_dft_params,\n",
    "    get_job_spec_scheduler_params,\n",
    "    submit_job,\n",
    "    calc_wall_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slac queue to submit to\n",
    "slac_sub_queue = \"suncat3\"  # 'suncat', 'suncat2', 'suncat3'\n",
    "\n",
    "# COMPENV to submit to\n",
    "# compenv_i = \"slac\"\n",
    "# compenv_i = \"sherlock\"\n",
    "compenv_i = \"nersc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_slab = get_df_slab()\n",
    "df_slab = df_slab.set_index(\"slab_id\")\n",
    "df_slab_i = df_slab\n",
    "\n",
    "# #########################################################\n",
    "df_jobs = get_df_jobs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read `df_slabs_to_run` from `create_slabs.ipynb`, used to mark priority slabs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/creating_slabs\",\n",
    "    \"out_data\")\n",
    "\n",
    "# #########################################################\n",
    "import pickle; import os\n",
    "path_i = os.path.join(\n",
    "    directory,\n",
    "    \"df_slabs_to_run.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_slabs_to_run = pickle.load(fle)\n",
    "# #########################################################\n",
    "\n",
    "\n",
    "indices_not_good = []\n",
    "for i_cnt, row_i in df_slabs_to_run.iterrows():\n",
    "    df = df_slab_i\n",
    "    df = df[\n",
    "        (df[\"bulk_id\"] == row_i.bulk_id) &\n",
    "        (df[\"facet\"] == row_i.facet_str) &\n",
    "        [True for i in range(len(df))]\n",
    "        ]\n",
    "    if df.shape[0] == 0:\n",
    "        indices_not_good.append(i_cnt)\n",
    "\n",
    "df_slabs_to_run.loc[\n",
    "    indices_not_good\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Slabs to Run"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping slabs that have been previously done\n",
    "df_jobs_i = df_jobs[df_jobs.ads == \"o\"]\n",
    "df_slab_i = df_slab_i.drop(\n",
    "    df_jobs_i.slab_id.unique()\n",
    "    )\n",
    "\n",
    "# Doing only phase 2 slabs for now\n",
    "df_slab_i = df_slab_i[df_slab_i.phase == 2]\n",
    "\n",
    "# #########################################################\n",
    "# Selecting smallest slabs\n",
    "# df_slab_i = df_slab_i[df_slab_i.num_atoms < 80]\n",
    "\n",
    "# print(\"Just doing XRD facets for now\")\n",
    "# df_slab_i = df_slab_i[df_slab_i.source == \"xrd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering down to best slabs, no layered, all octahedra, 0.3 eV/atom above hull cutoff"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_slabs = []\n",
    "for slab_id_i, row_i in df_slab_i.iterrows():\n",
    "    # ####################################################\n",
    "    bulk_id_i = row_i.bulk_id\n",
    "    facet_i = row_i.facet\n",
    "    # ####################################################\n",
    "\n",
    "    # print(\"\")\n",
    "    # print(bulk_id_i, slab_id_i)\n",
    "\n",
    "    df = df_slabs_to_run\n",
    "    df = df[\n",
    "        (df[\"bulk_id\"] == bulk_id_i) &\n",
    "        (df[\"facet_str\"] == facet_i) &\n",
    "        [True for i in range(len(df))]\n",
    "        ]\n",
    "    if df.shape[0] > 0:\n",
    "        # print(\"Good\")\n",
    "        good_slabs.append(slab_id_i)\n",
    "\n",
    "    # elif df.shape[0] == 0:\n",
    "    #     print(\"Bad\")\n",
    "\n",
    "df_slab_i = df_slab_i.loc[\n",
    "    good_slabs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_slab_i\n",
    "df = df[\n",
    "    (df[\"num_atoms\"] <= 100) &\n",
    "    # (df[\"\"] == \"\") &\n",
    "    # (df[\"\"] == \"\") &\n",
    "    [True for i in range(len(df))]\n",
    "    ]\n",
    "df_slab_i = df\n",
    "\n",
    "df_slab_i = df_slab_i.sort_values(\"num_atoms\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slab_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slab_i.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slab_i = df_slab_i.loc[\n",
    "    [\n",
    "\n",
    "        'legofufi_61',\n",
    "        'gekawore_16',\n",
    "        'mitilaru_63',\n",
    "\n",
    "        # 'winomuvi_99',\n",
    "        # 'letapivu_80',\n",
    "        # 'giworuge_14',\n",
    "\n",
    "        # 'lirilapa_78',\n",
    "        # 'wakidowo_59',\n",
    "\n",
    "        # 'kererape_22',\n",
    "        # 'nekelele_74',\n",
    "        # 'pebitiru_79',\n",
    "\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slab_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the job folders"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_slab_i.iterrows():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    slab_id = row_i.name\n",
    "    bulk_id = row_i.bulk_id\n",
    "    facet = row_i.facet\n",
    "    slab_final = row_i.slab_final\n",
    "    num_atoms = row_i.num_atoms\n",
    "    loop_time = row_i.loop_time\n",
    "    iter_time_i = row_i.iter_time_i\n",
    "    # #####################################################\n",
    "\n",
    "    attempt = 1\n",
    "    rev = 1\n",
    "\n",
    "\n",
    "    # Checking if job dir exists for other comp. envs. (it shouldn't)\n",
    "    job_exists_in_another_compenv = False\n",
    "    path_already_exists = False\n",
    "    for compenv_j in [\"slac\", \"sherlock\", \"nersc\", ]:\n",
    "        \n",
    "        path_j = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            \"dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs\",\n",
    "            compenv_j,\n",
    "            bulk_id,\n",
    "            facet,\n",
    "            str(attempt).zfill(2) + \"_attempt\",\n",
    "            \"_\" + str(rev).zfill(2)\n",
    "            )\n",
    "        if os.path.exists(path_j) and compenv_j == compenv_i:\n",
    "            path_already_exists = True\n",
    "            print(\"This path already exists\", path_j)\n",
    "\n",
    "        elif os.path.exists(path_j):\n",
    "            job_exists_in_another_compenv = True\n",
    "            print(\"Job exists in another COMPENV\", path_j)\n",
    "\n",
    "    good_to_go = True\n",
    "    if job_exists_in_another_compenv:\n",
    "        good_to_go = False\n",
    "    if path_already_exists:\n",
    "        good_to_go = False\n",
    "\n",
    "\n",
    "    if good_to_go:\n",
    "        path_i = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            \"dft_workflow/run_slabs/run_o_covered/out_data/dft_jobs\",\n",
    "            compenv_i,\n",
    "            bulk_id,\n",
    "            facet,\n",
    "            str(attempt).zfill(2) + \"_attempt\",\n",
    "            \"_\" + str(rev).zfill(2)\n",
    "            )\n",
    "\n",
    "        print(path_i)\n",
    "        if os.path.exists(path_i):\n",
    "            print(\"TEMP | This path already exists and it shouldn't\", path_i)\n",
    "\n",
    "        if not os.path.exists(path_i):\n",
    "            os.makedirs(path_i)\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        # Copy dft script to job folder\n",
    "        # #####################################################\n",
    "        copyfile(\n",
    "            os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                \"dft_workflow/dft_scripts/slab_dft.py\"\n",
    "                ),\n",
    "            os.path.join(\n",
    "                path_i,\n",
    "                \"model.py\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        copyfile(\n",
    "            os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer\"],\n",
    "                \"dft_workflow/dft_scripts/slab_dft.py\"\n",
    "                ),\n",
    "            os.path.join(\n",
    "                path_i,\n",
    "                \"slab_dft.py\",\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        # #####################################################\n",
    "        # Copy atoms object to job folder\n",
    "        # #####################################################\n",
    "        slab_final.write(\n",
    "            os.path.join(path_i, \"init.traj\")\n",
    "            )\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i[\"slab_id\"] = slab_id\n",
    "        data_dict_i[\"bulk_id\"] = bulk_id\n",
    "        data_dict_i[\"facet\"] = facet\n",
    "        data_dict_i[\"slab_final\"] = slab_final\n",
    "        data_dict_i[\"num_atoms\"] = num_atoms\n",
    "        data_dict_i[\"attempt\"] = attempt\n",
    "        data_dict_i[\"rev\"] = rev\n",
    "        data_dict_i[\"path_i\"] = path_i\n",
    "        # #####################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_new = pd.DataFrame(data_dict_list)\n",
    "df_jobs_new = df_jobs_new.set_index(\"slab_id\")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning job specific DFT parameters"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    slab_id = row_i.name\n",
    "    num_atoms = row_i.num_atoms\n",
    "    path_i =row_i.path_i\n",
    "    # #####################################################\n",
    "\n",
    "    dft_params_dict = get_job_spec_dft_params(\n",
    "        compenv=compenv_i,\n",
    "        slac_sub_queue=\"suncat3\",\n",
    "        )\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"slab_id\"] = slab_id\n",
    "    data_dict_i[\"dft_params\"] = dft_params_dict\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "df_dft_params = pd.DataFrame(data_dict_list)\n",
    "df_dft_params = df_dft_params.set_index(\"slab_id\")\n",
    "\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "# Writing DFT params to job directory\n",
    "for slab_id, row_i in df_dft_params.iterrows():\n",
    "    # #####################################################\n",
    "    dft_params = row_i.dft_params\n",
    "    # #####################################################\n",
    "    row_slab_i = df_jobs_new.loc[slab_id]\n",
    "    path_i = row_slab_i.path_i\n",
    "    # #####################################################\n",
    "\n",
    "    with open(os.path.join(path_i, \"dft-params.json\"), \"w+\") as fle:\n",
    "        json.dump(dft_params, fle, indent=2, skipkeys=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting initial magnetic moments"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_jobs_new.iterrows():\n",
    "    # #####################################################\n",
    "    atoms = row_i.slab_final\n",
    "    path_i =row_i.path_i\n",
    "    # #####################################################\n",
    "\n",
    "    z_positions = atoms.positions[:, 2]\n",
    "    z_max = z_positions.max()\n",
    "\n",
    "    O_magmom=0.2\n",
    "    M_magmom=0.6\n",
    "    magmoms_i = []\n",
    "    for atom in atoms:\n",
    "        z_pos = atom.position[2]\n",
    "        dist_from_top = z_max - z_pos\n",
    "        # print(z_max - z_pos)\n",
    "\n",
    "        if dist_from_top < 4:\n",
    "            if atom.symbol == \"O\":\n",
    "                magmom_i = O_magmom\n",
    "            else:\n",
    "                magmom_i = M_magmom\n",
    "            magmoms_i.append(magmom_i)\n",
    "        else:\n",
    "            magmoms_i.append(0.)\n",
    "\n",
    "    data_path = os.path.join(path_i, \"magmoms.json\")\n",
    "    with open(data_path, \"w\") as outfile:\n",
    "        json.dump(magmoms_i, outfile, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Paths of new jobs:\")\n",
    "tmp = [print(i) for i in df_jobs_new.path_i.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"setup_dft.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_slab_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_slab_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_slab_i = df_slab_i.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
