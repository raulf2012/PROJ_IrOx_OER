{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct the atom indices of post-DFT atoms with `ase-sort.dat`\n",
    "---"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# #########################################################\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from vasp.vasp_methods import read_ase_sort_dat\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_paths,\n",
    "    are_dicts_the_same,\n",
    "    get_df_jobs_anal,\n",
    "    get_df_jobs_data,\n",
    "    get_df_init_slabs,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    get_unique_job_ids_ase_sort,\n",
    "    all_keys_equal_to_vals,\n",
    "    get_df_atoms_ind,\n",
    "    unique_ids_with_no_equal,\n",
    "    atoms_distance_comparison,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_jobs = get_df_jobs()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "df_jobs_anal_completed = df_jobs_anal[df_jobs_anal.job_completely_done == True]\n",
    "\n",
    "# #########################################################\n",
    "df_init_slabs = get_df_init_slabs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing rows that don't have the necessary files present locally\n",
    "\n",
    "Might need to download them with rclone"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\n",
    "    \"job_type\", \"compenv\", \"slab_id\", \"ads\", \"active_site\", \"att_num\", \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_process = []\n",
    "grouped = df_jobs_anal_completed.groupby(\n",
    "    group_cols\n",
    "    )\n",
    "for name, df_jobs_anal_i in grouped:\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_groups = df_jobs.groupby(group_cols)\n",
    "    df_jobs_i = df_jobs_groups.get_group(name)\n",
    "    # #####################################################\n",
    "\n",
    "    all_ok_to_process_list = []\n",
    "    for job_id_j in df_jobs_i.index:\n",
    "        # #################################################\n",
    "        row_paths_j = df_jobs_paths.loc[job_id_j]\n",
    "        # #################################################\n",
    "        gdrive_path_j = row_paths_j.gdrive_path\n",
    "        # #################################################\n",
    "        \n",
    "        job_ok_to_process = False\n",
    "\n",
    "        # #################################################\n",
    "        gdrive_path_j = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "            gdrive_path_j)\n",
    "\n",
    "        # #############################\n",
    "        path_tmp_0 = os.path.join(\n",
    "            gdrive_path_j,\n",
    "            \"CONTCAR\")\n",
    "        contcar_present = False\n",
    "        if Path(path_tmp_0).is_file():\n",
    "            contcar_present = True\n",
    "\n",
    "        # #############################\n",
    "        path_tmp_1 = os.path.join(\n",
    "            gdrive_path_j,\n",
    "            \".SUBMITTED\")\n",
    "        submitted_present = False\n",
    "        if Path(path_tmp_1).is_file():\n",
    "            submitted_present = True\n",
    "\n",
    "        # #############################\n",
    "        path_j = os.path.join(\n",
    "            gdrive_path_j,\n",
    "            \"ase-sort.dat\")\n",
    "        file_present = False\n",
    "        if Path(path_j).is_file():\n",
    "            file_present = True\n",
    "\n",
    "\n",
    "        if file_present:\n",
    "            job_ok_to_process = True\n",
    "        elif not contcar_present and submitted_present:\n",
    "            job_ok_to_process = True\n",
    "\n",
    "        all_ok_to_process_list.append(job_ok_to_process)\n",
    "\n",
    "    all_ok_to_process = all(all_ok_to_process_list)\n",
    "    if all_ok_to_process:\n",
    "        indices_to_process.extend(df_jobs_anal_i.index.tolist())\n",
    "    else:\n",
    "        print(name)\n",
    "        # indices_to_not_process.extend()\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_anal_completed_2 = df_jobs_anal_completed.loc[indices_to_process]\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = df_jobs_anal_completed.index.to_frame()\n",
    "# df = df_jobs_anal.index.to_frame()\n",
    "# df = df[\n",
    "#     (df[\"slab_id\"] == \"momaposi_60\") &\n",
    "#     (df[\"ads\"] == \"o\") &\n",
    "#     # (df[\"\"] == \"\") &\n",
    "#     [True for i in range(len(df))]\n",
    "#     ]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouped.get_group(\n",
    "#     ('oer_adsorbate', 'sherlock', 'momaposi_60', 'o', 50.0, 1)\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('oer_adsorbate', 'sherlock', 'momaposi_60', 'o', 50.0, 1) in indices_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_processed_indices = []\n",
    "for index_i, row_i in df_jobs_anal_completed.iterrows():\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    if index_i not in df_jobs_anal_completed_2.index:\n",
    "        not_processed_indices.append([job_id_max_i, index_i])\n",
    "\n",
    "if len(not_processed_indices) > 0:\n",
    "    print(\n",
    "        \"These systems don't have the required files locally\",\n",
    "        \"Fix with rclone\",\n",
    "        \"\",\n",
    "        sep=\"\\n\")\n",
    "    tmp = [print(i[0], \"|\", i[1]) for i in not_processed_indices]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "grouped = df_jobs_anal_completed_2.groupby(\n",
    "    [\"job_type\", \"compenv\", \"slab_id\", \"ads\", \"active_site\", \"att_num\", ])\n",
    "for name, df_jobs_anal_i in grouped:\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    job_type_i = name[0]\n",
    "    compenv_i = name[1]\n",
    "    slab_id_i = name[2]\n",
    "    ads_i = name[3]\n",
    "    active_site_i = name[4]\n",
    "    att_num_i = name[5]\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_groups = df_jobs.groupby(group_cols)\n",
    "    df_jobs_i = df_jobs_groups.get_group(name)\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    df_atoms_ind_i = get_df_atoms_ind(\n",
    "        df_jobs_i=df_jobs_i,\n",
    "        df_jobs_paths=df_jobs_paths,\n",
    "        )\n",
    "    df_atoms_ind_i = df_atoms_ind_i.dropna()\n",
    "\n",
    "    # #####################################################\n",
    "    job_ids = df_atoms_ind_i.job_id.tolist()\n",
    "    unique_job_ids = get_unique_job_ids_ase_sort(job_ids, df_atoms_ind_i)\n",
    "\n",
    "    # #####################################################\n",
    "    unique_ids_with_no_equal_i = unique_ids_with_no_equal(\n",
    "        unique_job_ids=unique_job_ids,\n",
    "        df_atoms_ind_i=df_atoms_ind_i,\n",
    "        )\n",
    "    if len(unique_ids_with_no_equal_i) > 1:\n",
    "        print(\"Big problem, I think there should only be one unique atoms mapping for any job\")\n",
    "\n",
    "    unique_id = unique_ids_with_no_equal_i[0]\n",
    "\n",
    "    # #####################################################\n",
    "    row_i = df_atoms_ind_i.loc[unique_id]\n",
    "    # #####################################################\n",
    "    atom_index_mapping_i = row_i.atom_index_mapping\n",
    "    sort_list_i = row_i.sort_list\n",
    "    resort_list_i = row_i.resort_list\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"job_type\"] = job_type_i\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"ads\"] = ads_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "    data_dict_i[\"att_num\"] = att_num_i\n",
    "    data_dict_i[\"atom_index_mapping\"] = atom_index_mapping_i\n",
    "    data_dict_i[\"sort_list\"] = sort_list_i\n",
    "    data_dict_i[\"resort_list\"] = resort_list_i\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_atoms_index = pd.DataFrame(data_dict_list)\n",
    "\n",
    "index_cols = [\n",
    "    \"job_type\",\n",
    "    \"compenv\", \"slab_id\",\n",
    "    \"ads\", \"active_site\", \"att_num\"]\n",
    "\n",
    "df_atoms_index = df_atoms_index.set_index(index_cols)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating atoms objects with correct index order and testing"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for name_i, row_i in df_jobs_anal_completed_2.iterrows():\n",
    "    if verbose:\n",
    "        print(40 * \"=\")\n",
    "        print(name_i)\n",
    "\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #########################################################\n",
    "    job_type_i = name_i[0]\n",
    "    compenv_i = name_i[1]\n",
    "    slab_id_i = name_i[2]\n",
    "    ads_i = name_i[3]\n",
    "    active_site_i = name_i[4]\n",
    "    att_num_i = name_i[5]\n",
    "    # #########################################################\n",
    "\n",
    "    # #########################################################\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    # #########################################################\n",
    "\n",
    "    # #########################################################\n",
    "    df_jobs_data_i = df_jobs_data[df_jobs_data.compenv == compenv_i]\n",
    "    row_data_i = df_jobs_data_i[df_jobs_data_i.job_id == job_id_max_i].iloc[0]\n",
    "    # #########################################################\n",
    "    final_atoms_i = row_data_i.final_atoms\n",
    "    # #########################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_atoms_index_i = df_atoms_index.loc[name_i]\n",
    "    # #####################################################\n",
    "    atom_index_mapping_i = row_atoms_index_i.atom_index_mapping\n",
    "    sort_list_i = row_atoms_index_i.sort_list\n",
    "    resort_list_i = row_atoms_index_i.resort_list\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_init_slabs_i = df_init_slabs.loc[\n",
    "        (compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)]\n",
    "    # #####################################################\n",
    "    init_atoms_i = row_init_slabs_i.init_atoms\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    # print(\"final_atoms_i.get_global_number_of_atoms():\", final_atoms_i.get_global_number_of_atoms())\n",
    "    atoms_distance_0 = atoms_distance_comparison(init_atoms_i, final_atoms_i)\n",
    "\n",
    "\n",
    "    was_sorted = False\n",
    "    atoms_sorted_good = None\n",
    "    failed_to_sort = False\n",
    "    atoms_distance_1 = None\n",
    "    magmoms_sorted_good = None\n",
    "    if atoms_distance_0 > 2:\n",
    "\n",
    "        atoms_sorted = final_atoms_i[resort_list_i]\n",
    "\n",
    "        magmoms_sorted = final_atoms_i.get_magnetic_moments()\n",
    "        magmoms_sorted = magmoms_sorted[resort_list_i]\n",
    "\n",
    "        # atoms_distance_1 = atoms_distance_comparison(slab_final_i, atoms_sorted)\n",
    "        atoms_distance_1 = atoms_distance_comparison(init_atoms_i, atoms_sorted)\n",
    "        if atoms_distance_1 < 1.5:\n",
    "\n",
    "            atoms_sorted_good = atoms_sorted\n",
    "            magmoms_sorted_good = magmoms_sorted\n",
    "\n",
    "            atoms_sorted_good.set_initial_magnetic_moments(magmoms_sorted_good)\n",
    "            was_sorted = True\n",
    "\n",
    "        else:\n",
    "            failed_to_sort = True\n",
    "            if verbose:\n",
    "                print(\"The sorted atoms and the initial slab aren't too similar\")\n",
    "                print(\"Look into this manually\")\n",
    "                print(\"name_i:\", name_i)\n",
    "\n",
    "    else:\n",
    "        atoms_sorted_good = final_atoms_i\n",
    "        atoms_distance_1 = None\n",
    "        magmoms_sorted_good = None\n",
    "\n",
    "        # if verbose:\n",
    "        #     print(atoms_distance_0)\n",
    "        #     print(\"Look into this manually if the atoms_distance is less than 2\")\n",
    "        #     print(\"I currently think that every single atoms object's indices are shuffled after DFT\")\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"job_type\"] = job_type_i\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"ads\"] = ads_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "    data_dict_i[\"att_num\"] = att_num_i\n",
    "\n",
    "    data_dict_i[\"job_id\"] = job_id_max_i\n",
    "    data_dict_i[\"was_sorted\"] = was_sorted\n",
    "    data_dict_i[\"failed_to_sort\"] = failed_to_sort\n",
    "    data_dict_i[\"atoms_sorted_good\"] = atoms_sorted_good\n",
    "    data_dict_i[\"atoms_distance_before_sorting\"] = atoms_distance_0\n",
    "    data_dict_i[\"atoms_distance_after_sorting\"] = atoms_distance_1\n",
    "    data_dict_i[\"magmoms_sorted_good\"] = magmoms_sorted_good\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_atoms_sorted = pd.DataFrame(data_dict_list)\n",
    "\n",
    "index_cols = [\n",
    "    \"job_type\",\n",
    "    \"compenv\", \"slab_id\",\n",
    "    \"ads\", \"active_site\", \"att_num\"]\n",
    "df_atoms_sorted = df_atoms_sorted.set_index(index_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb_i = pd.concat(\n",
    "    [\n",
    "        df_atoms_index,\n",
    "        df_atoms_sorted,\n",
    "        ],\n",
    "    axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling `df_atoms_index`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_analysis/atoms_indices_order\",\n",
    "    \"out_data\")\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(os.path.join(directory, \"df_atoms_sorted_ind.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_comb_i, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read `df_atoms_index` with Pickle"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_atoms_sorted_ind\n",
    "df_atoms_sorted_ind_tmp = get_df_atoms_sorted_ind()\n",
    "df_atoms_sorted_ind_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"correct_atom_indices_order.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# name_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_atoms_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# # row_atoms_index_i = \n",
    "# df_atoms_index.loc[name_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_atoms_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_atoms_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# sort_list_i = df_comb_i.sort_list.iloc[0]\n",
    "# resort_list_i = df_comb_i.resort_list.iloc[0]\n",
    "# atom_index_mapping_i = df_comb_i.atom_index_mapping.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_atoms_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_i = \n",
    "# df_jobs_groups.get_group(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# # df_jobs_i = \n",
    "\n",
    "\n",
    "# # name\n",
    "# df_jobs_groups.get_group(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_atoms_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# # TEMP\n",
    "# print(222 * \"TEMP | \")\n",
    "\n",
    "# df_jobs_anal_completed_2 = df_jobs_anal_completed_2.loc[[\n",
    "#     # ('slac', 'ralutiwa_59', 'o', 30.0, 1)\n",
    "#     ('slac', 'vapopihe_87', 'o', 23.0, 1),\n",
    "#     ]]\n",
    "# df_jobs_anal_completed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_anal_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_jobs_anal_completed_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "    # [\"compenv\", \"slab_id\", \"ads\", \"active_site\", \"att_num\", ])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
