{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect DFT data into OER sets \n",
    "---"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# #########################################################\n",
    "from IPython.display import display\n",
    "\n",
    "from plotting.my_plotly import my_plotly_plot\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs_anal,\n",
    "    get_df_jobs_data,\n",
    "    get_df_slabs_to_run,\n",
    "    get_df_jobs_oh_anal,\n",
    "    get_df_atoms_sorted_ind,\n",
    "    get_df_features_targets,\n",
    "    get_df_magmom_drift,\n",
    "    get_df_jobs,\n",
    "    get_df_struct_drift,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    get_oer_triplet__low_e,\n",
    "    get_oer_triplet__from_oh,\n",
    "    get_oer_triplet__magmom,\n",
    "\n",
    "    get_group_w_all_ads,\n",
    "    are_any_ads_done,\n",
    "    calc_ads_e,\n",
    "    get_oer_triplet,\n",
    "    )\n",
    "\n",
    "# from local_methods import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_analysis/collect_collate_dft_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_jobs = get_df_jobs()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "df_jobs_anal_i = df_jobs_anal\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "\n",
    "# #########################################################\n",
    "df_slabs_to_run = get_df_slabs_to_run()\n",
    "df_slabs_to_run = df_slabs_to_run.set_index(\n",
    "    [\"compenv\", \"slab_id\", \"att_num\"], drop=False)\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_oh_anal = get_df_jobs_oh_anal()\n",
    "df_jobs_oh_anal = df_jobs_oh_anal.set_index([\"compenv\", \"slab_id\", \"active_site\"])\n",
    "\n",
    "# #########################################################\n",
    "df_atoms_sorted = get_df_atoms_sorted_ind()\n",
    "\n",
    "# #########################################################\n",
    "df_features_targets = get_df_features_targets()\n",
    "\n",
    "# #########################################################\n",
    "df_struct_drift = get_df_struct_drift()\n",
    "\n",
    "# #########################################################\n",
    "df_magmom_drift = get_df_magmom_drift()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering `df_jobs_anal` to only `oer_adsorbate` job types"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind = df_jobs_anal_i.index.to_frame()\n",
    "\n",
    "df_jobs_anal_i = df_jobs_anal_i.loc[\n",
    "    df_ind[df_ind.job_type == \"oer_adsorbate\"].index\n",
    "    ]\n",
    "df_jobs_anal_i = df_jobs_anal_i.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter columns in `df_jobs_anal_i`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc_modules.pandas_methods import drop_columns\n",
    "\n",
    "cols_to_keep = [\n",
    "    'job_id_max',\n",
    "    # 'timed_out',\n",
    "    # 'completed',\n",
    "    # 'brmix_issue',\n",
    "    # 'job_understandable',\n",
    "    # 'decision',\n",
    "    # 'dft_params_new',\n",
    "    'job_completely_done',\n",
    "    ]\n",
    "\n",
    "df_jobs_anal_i = drop_columns(\n",
    "    df=df_jobs_anal_i,\n",
    "    columns=cols_to_keep,\n",
    "    keep_or_drop=\"keep\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafting `pot_e` and `as_is_nan` to dataframe"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    # #####################################################\n",
    "    new_column_values_dict = {\n",
    "        \"pot_e\": None,\n",
    "        \"as_is_nan\": None,\n",
    "        }\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.name[0]\n",
    "    slab_id_i = row_i.name[1]\n",
    "    ads_i = row_i.name[2]\n",
    "    active_site_i = row_i.name[3]\n",
    "    att_num_i = row_i.name[4]\n",
    "    # #####################################################\n",
    "    job_id_max_i = row_i.job_id_max\n",
    "    job_completely_done_i = row_i.job_completely_done\n",
    "    # #####################################################\n",
    "\n",
    "    as_is_nan = False\n",
    "    if active_site_i == \"NaN\":\n",
    "        as_is_nan = True\n",
    "\n",
    "    # #####################################################\n",
    "    row_data_i = df_jobs_data.loc[job_id_max_i]\n",
    "    # #####################################################\n",
    "    pot_e_i = row_data_i.pot_e\n",
    "    rerun_from_oh_i = row_data_i.rerun_from_oh\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    new_column_values_dict[\"pot_e\"] = pot_e_i\n",
    "    new_column_values_dict[\"as_is_nan\"] = as_is_nan\n",
    "    new_column_values_dict[\"rerun_from_oh\"] = rerun_from_oh_i\n",
    "    # #####################################################\n",
    "    for key, value in new_column_values_dict.items():\n",
    "        row_i[key] = value\n",
    "    # #####################################################\n",
    "    return(row_i)\n",
    "\n",
    "df_jobs_anal_i = df_jobs_anal_i.apply(\n",
    "    method,\n",
    "    axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing O slabs from dataframe"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# Remove the *O slabs for now\n",
    "# The fact that they have NaN active sites will mess up the groupby\n",
    "ads_list = df_jobs_anal_i.index.get_level_values(\"ads\").tolist()\n",
    "ads_list_no_o = [i for i in list(set(ads_list)) if i != \"o\"]\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "df_jobs_anal_no_o = df_jobs_anal_i.loc[idx[:, :, ads_list_no_o, :, :], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows whose atoms failed to sort"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atoms_sorted_i = df_atoms_sorted[df_atoms_sorted.index.to_frame().job_type == \"oer_adsorbate\"] \n",
    "df_atoms_sorted_i = df_atoms_sorted_i.droplevel(level=0)\n",
    "\n",
    "df_atoms_sorted_i = df_atoms_sorted_i[df_atoms_sorted_i.failed_to_sort == True]\n",
    "\n",
    "\n",
    "# Dropping rows that have failed to sort atoms objects\n",
    "df_jobs_anal_no_o = df_jobs_anal_no_o.drop(df_atoms_sorted_i.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `get_oer_triplet__low_e`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_i = os.path.join(\n",
    "    root_dir, \"out_data\",\n",
    "    \"df_ads__low_e.pickle\",\n",
    "    )\n",
    "\n",
    "my_file = Path(path_i)\n",
    "if my_file.is_file():\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_ads__low_e = pickle.load(fle)\n",
    "else:\n",
    "# if True:\n",
    "\n",
    "    # #########################################################\n",
    "    data_dict_list = []\n",
    "    # #########################################################\n",
    "    groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "    grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "    for name_i, group in grouped:\n",
    "\n",
    "    # # #########################################################\n",
    "    # if True:\n",
    "    #     name_i = ('sherlock', 'vipikema_98', 47.0)\n",
    "    #     # #####################################################\n",
    "    #     group = grouped.get_group(name_i)\n",
    "\n",
    "        # print(name_i)\n",
    "\n",
    "        # #####################################################\n",
    "        ads_e_o_i = None\n",
    "        ads_e_oh_i = None\n",
    "        job_id_o_i = None\n",
    "        job_id_oh_i  = None\n",
    "        job_id_bare_i = None\n",
    "        all_jobs_in_group_done = None\n",
    "        any_bare_done = None\n",
    "        any_oh_done = None\n",
    "        any_o_done = None\n",
    "        any_o_done_with_active_sites = None\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i = dict()\n",
    "        # #####################################################\n",
    "        name_dict_i = dict(zip(groupby_cols, name_i))\n",
    "        # #####################################################\n",
    "        compenv_i = name_i[0]\n",
    "        slab_id_i = name_i[1]\n",
    "        active_site_i = name_i[2]\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        out_dict = get_group_w_all_ads(\n",
    "            name=name_i,\n",
    "            group=group,\n",
    "            df_jobs_anal_i=df_jobs_anal_i,\n",
    "            )\n",
    "        group_i = out_dict[\"group_i\"]\n",
    "        any_o_done_with_active_sites = out_dict[\"any_o_done_with_active_sites\"]\n",
    "\n",
    "\n",
    "        all_jobs_in_group_done = group_i.job_completely_done.all()\n",
    "\n",
    "        # Check that potential energy is numerical\n",
    "        for i in group_i.pot_e.tolist():\n",
    "            if type(i) != float:\n",
    "                print(\"A non-numerical potential energy entered WF: \", name_i)\n",
    "\n",
    "\n",
    "        # Only consider done jobs from here\n",
    "        group_done_i = group_i[group_i.job_completely_done == True]\n",
    "\n",
    "        group_ind_i = group_done_i.index.to_frame()\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        necessary_ads_present = False\n",
    "        # #####################################################\n",
    "        o_avail = \"o\" in group_ind_i.ads.tolist()\n",
    "        oh_avail = \"oh\" in group_ind_i.ads.tolist()\n",
    "        bare_avail = \"bare\" in group_ind_i.ads.tolist()\n",
    "        # #####################################################\n",
    "        if o_avail and oh_avail and bare_avail:\n",
    "            necessary_ads_present = True\n",
    "        # #####################################################\n",
    "\n",
    "        if necessary_ads_present:\n",
    "\n",
    "            oer_trip_i = get_oer_triplet__low_e(\n",
    "                name=name_i,\n",
    "                group=group_done_i,\n",
    "                )\n",
    "\n",
    "            # oer_trip_i = get_oer_triplet(\n",
    "            #     name=name_i,\n",
    "            #     # group=group_i,\n",
    "            #     group=group_done_i,\n",
    "            #     df_jobs_oh_anal=df_jobs_oh_anal,\n",
    "            #     # heuristic__if_lower_e=False,\n",
    "            #     heuristic__if_lower_e=True,\n",
    "            #     )\n",
    "\n",
    "\n",
    "\n",
    "            # #################################################\n",
    "            idx = pd.IndexSlice\n",
    "            row_o_i = oer_trip_i.loc[idx[:, :, \"o\", :, :], :].iloc[0]\n",
    "            job_id_o_i = row_o_i.job_id_max\n",
    "\n",
    "            # #################################################\n",
    "            idx = pd.IndexSlice\n",
    "            row_oh_i = oer_trip_i.loc[idx[:, :, \"oh\", :, :], :].iloc[0]\n",
    "            job_id_oh_i = row_oh_i.job_id_max\n",
    "\n",
    "            # #################################################\n",
    "            idx = pd.IndexSlice\n",
    "            row_bare_i = oer_trip_i.loc[idx[:, :, \"bare\", :, :], :].iloc[0]\n",
    "            job_id_bare_i = row_bare_i.job_id_max\n",
    "\n",
    "            # #################################################\n",
    "            # Computing adsorption energy\n",
    "            df_ads_i = calc_ads_e(oer_trip_i.reset_index())\n",
    "            df_ads_i = df_ads_i.set_index(\"ads\", drop=False)\n",
    "\n",
    "            ads_g_o_i = df_ads_i.loc[\"o\"][\"ads_e\"]\n",
    "            ads_e_o_i = df_ads_i.loc[\"o\"][\"ads_e_elec\"]\n",
    "\n",
    "            ads_g_oh_i = df_ads_i.loc[\"oh\"][\"ads_e\"]\n",
    "            ads_e_oh_i = df_ads_i.loc[\"oh\"][\"ads_e_elec\"]\n",
    "            job_id_oh_i = df_ads_i.loc[\"oh\"][\"job_id_max\"]\n",
    "\n",
    "\n",
    "            # #################################################\n",
    "            data_dict_i.update(name_dict_i)\n",
    "            # #################################################\n",
    "            data_dict_i[\"g_o\"] = ads_g_o_i\n",
    "            data_dict_i[\"g_oh\"] = ads_g_oh_i\n",
    "            data_dict_i[\"e_o\"] = ads_e_o_i\n",
    "            data_dict_i[\"e_oh\"] = ads_e_oh_i\n",
    "            data_dict_i[\"job_id_o\"] = job_id_o_i\n",
    "            data_dict_i[\"job_id_oh\"] = job_id_oh_i \n",
    "            data_dict_i[\"job_id_bare\"] = job_id_bare_i\n",
    "            data_dict_i[\"all_done\"] = all_jobs_in_group_done\n",
    "            data_dict_i[\"any_bare_done\"] = any_bare_done\n",
    "            data_dict_i[\"any_oh_done\"] = any_oh_done\n",
    "            data_dict_i[\"any_o_done\"] = any_o_done\n",
    "            data_dict_i[\"any_o_w_as_done\"] = any_o_done_with_active_sites\n",
    "            # #################################################\n",
    "            data_dict_list.append(data_dict_i)\n",
    "            # #################################################\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    df_ads__low_e = pd.DataFrame(data_dict_list)\n",
    "    # #########################################################\n",
    "\n",
    "    # #########################################################\n",
    "    # Pickling data ###########################################\n",
    "    directory = os.path.join(\n",
    "        root_dir, \"out_data\")\n",
    "    if not os.path.exists(directory): os.makedirs(directory)\n",
    "    with open(os.path.join(directory, \"df_ads__low_e.pickle\"), \"wb\") as fle:\n",
    "        pickle.dump(df_ads__low_e, fle)\n",
    "    # #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `get_oer_triplet__from_oh`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_i = os.path.join(\n",
    "    root_dir, \"out_data\",\n",
    "    \"df_ads__from_oh.pickle\",\n",
    "    )\n",
    "\n",
    "my_file = Path(path_i)\n",
    "if my_file.is_file():\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_ads__from_oh = pickle.load(fle)\n",
    "else:\n",
    "# if True:\n",
    "\n",
    "    # #########################################################\n",
    "    data_dict_list = []\n",
    "    # #########################################################\n",
    "    groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "    grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "    for name_i, group in grouped:\n",
    "\n",
    "    # if True:\n",
    "    #     # name_i = ('sherlock', 'vipikema_98', 47.0)\n",
    "    #     # name_i = ('nersc', 'kalisule_45', 62.0)\n",
    "    #     # name_i = ('sherlock', 'momaposi_60', 50.0)\n",
    "    #     name_i = ('nersc', 'fosurufu_23', 43.0)\n",
    "    #     group = grouped.get_group(name_i)\n",
    "\n",
    "\n",
    "        # print(name_i)\n",
    "\n",
    "        # #####################################################\n",
    "        ads_e_o_i = None\n",
    "        ads_e_oh_i = None\n",
    "        job_id_o_i = None\n",
    "        job_id_oh_i  = None\n",
    "        job_id_bare_i = None\n",
    "        all_jobs_in_group_done = None\n",
    "        any_bare_done = None\n",
    "        any_oh_done = None\n",
    "        any_o_done = None\n",
    "        any_o_done_with_active_sites = None\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i = dict()\n",
    "        # #####################################################\n",
    "        name_dict_i = dict(zip(groupby_cols, name_i))\n",
    "        # #####################################################\n",
    "        compenv_i = name_i[0]\n",
    "        slab_id_i = name_i[1]\n",
    "        active_site_i = name_i[2]\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        out_dict = get_group_w_all_ads(\n",
    "            name=name_i,\n",
    "            group=group,\n",
    "            df_jobs_anal_i=df_jobs_anal_i,\n",
    "            )\n",
    "        group_i = out_dict[\"group_i\"]\n",
    "        any_o_done_with_active_sites = out_dict[\"any_o_done_with_active_sites\"]\n",
    "\n",
    "\n",
    "        all_jobs_in_group_done = group_i.job_completely_done.all()\n",
    "\n",
    "\n",
    "        # Check that potential energy is numerical\n",
    "        for i in group_i.pot_e.tolist():\n",
    "            if type(i) != float:\n",
    "                print(\"A non-numerical potential energy entered WF: \", name_i)\n",
    "\n",
    "\n",
    "        # Only consider done jobs from here\n",
    "        group_done_i = group_i[group_i.job_completely_done == True]\n",
    "\n",
    "        group_ind_i = group_done_i.index.to_frame()\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        necessary_ads_present = False\n",
    "        # #####################################################\n",
    "        o_avail = \"o\" in group_ind_i.ads.tolist()\n",
    "        oh_avail = \"oh\" in group_ind_i.ads.tolist()\n",
    "        bare_avail = \"bare\" in group_ind_i.ads.tolist()\n",
    "        # #####################################################\n",
    "        if o_avail and oh_avail and bare_avail:\n",
    "            necessary_ads_present = True\n",
    "        # #####################################################\n",
    "\n",
    "        if necessary_ads_present:\n",
    "\n",
    "            oer_trip_dict_i = get_oer_triplet__from_oh(\n",
    "                name=name_i,\n",
    "                group=group_done_i,\n",
    "                df_jobs_oh_anal=df_jobs_oh_anal,\n",
    "                )\n",
    "            oer_trip_i = oer_trip_dict_i[\"df_oer_triplet\"]\n",
    "            error = oer_trip_dict_i[\"error\"]\n",
    "            note = oer_trip_dict_i[\"note\"]\n",
    "\n",
    "            # # TEMP\n",
    "            # break\n",
    "\n",
    "\n",
    "            ads_g_o_i = None\n",
    "            ads_g_oh_i = None\n",
    "            ads_e_o_i = None\n",
    "            ads_e_oh_i = None\n",
    "            if not error:\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_o_i = oer_trip_i.loc[idx[:, :, \"o\", :, :], :].iloc[0]\n",
    "                job_id_o_i = row_o_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_oh_i = oer_trip_i.loc[idx[:, :, \"oh\", :, :], :].iloc[0]\n",
    "                job_id_oh_i = row_oh_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_bare_i = oer_trip_i.loc[idx[:, :, \"bare\", :, :], :].iloc[0]\n",
    "                job_id_bare_i = row_bare_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                # COmputing adsorption energy\n",
    "                df_ads_i = calc_ads_e(oer_trip_i.reset_index())\n",
    "                df_ads_i = df_ads_i.set_index(\"ads\", drop=False)\n",
    "\n",
    "                ads_g_o_i = df_ads_i.loc[\"o\"][\"ads_e\"]\n",
    "                ads_e_o_i = df_ads_i.loc[\"o\"][\"ads_e_elec\"]\n",
    "\n",
    "                ads_g_oh_i = df_ads_i.loc[\"oh\"][\"ads_e\"]\n",
    "                ads_e_oh_i = df_ads_i.loc[\"oh\"][\"ads_e_elec\"]\n",
    "                job_id_oh_i = df_ads_i.loc[\"oh\"][\"job_id_max\"]\n",
    "\n",
    "\n",
    "            # #################################################\n",
    "            data_dict_i.update(name_dict_i)\n",
    "            # #################################################\n",
    "            data_dict_i[\"g_o\"] = ads_g_o_i\n",
    "            data_dict_i[\"g_oh\"] = ads_g_oh_i\n",
    "            data_dict_i[\"e_o\"] = ads_e_o_i\n",
    "            data_dict_i[\"e_oh\"] = ads_e_oh_i\n",
    "            data_dict_i[\"job_id_o\"] = job_id_o_i\n",
    "            data_dict_i[\"job_id_oh\"] = job_id_oh_i \n",
    "            data_dict_i[\"job_id_bare\"] = job_id_bare_i\n",
    "            data_dict_i[\"all_done\"] = all_jobs_in_group_done\n",
    "            data_dict_i[\"any_bare_done\"] = any_bare_done\n",
    "            data_dict_i[\"any_oh_done\"] = any_oh_done\n",
    "            data_dict_i[\"any_o_done\"] = any_o_done\n",
    "            data_dict_i[\"any_o_w_as_done\"] = any_o_done_with_active_sites\n",
    "            data_dict_i[\"error\"] = error\n",
    "            data_dict_i[\"note\"] = note\n",
    "            # #################################################\n",
    "            data_dict_list.append(data_dict_i)\n",
    "            # #################################################\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    df_ads__from_oh = pd.DataFrame(data_dict_list)\n",
    "    # #########################################################\n",
    "\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    # Pickling data ###########################################\n",
    "    directory = os.path.join(\n",
    "        root_dir, \"out_data\")\n",
    "    if not os.path.exists(directory): os.makedirs(directory)\n",
    "    with open(os.path.join(directory, \"df_ads__from_oh.pickle\"), \"wb\") as fle:\n",
    "        pickle.dump(df_ads__from_oh, fle)\n",
    "    # #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oer_trip_dict_i = get_oer_triplet__from_oh(\n",
    "#     name=name_i,\n",
    "#     group=group_done_i,\n",
    "#     df_jobs_oh_anal=df_jobs_oh_anal,\n",
    "#     )\n",
    "# oer_trip_i = oer_trip_dict_i[\"df_oer_triplet\"]\n",
    "# error = oer_trip_dict_i[\"error\"]\n",
    "# note = oer_trip_dict_i[\"note\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oer_trip_dict_i\n",
    "\n",
    "# oer_trip_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `get_oer_triplet__magmom`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_i = os.path.join(\n",
    "    root_dir, \"out_data\",\n",
    "    \"df_ads__magmom.pickle\",\n",
    "    )\n",
    "\n",
    "my_file = Path(path_i)\n",
    "if my_file.is_file():\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_ads__magmom = pickle.load(fle)\n",
    "else:\n",
    "# if True:\n",
    "\n",
    "    # #########################################################\n",
    "    data_dict_list = []\n",
    "    # #########################################################\n",
    "    groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "    grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "    for name_i, group in grouped:\n",
    "\n",
    "    # if True:\n",
    "    #     # name_i = ('sherlock', 'vipikema_98', 47.0)\n",
    "    #     # name_i = ('nersc', 'kalisule_45', 62.0)\n",
    "    #     # name_i = ('sherlock', 'momaposi_60', 50.0)\n",
    "    #     name_i = ('nersc', 'fosurufu_23', 43.0)\n",
    "    #     group = grouped.get_group(name_i)\n",
    "\n",
    "\n",
    "        # print(name_i)\n",
    "\n",
    "        # #####################################################\n",
    "        ads_e_o_i = None\n",
    "        ads_e_oh_i = None\n",
    "        job_id_o_i = None\n",
    "        job_id_oh_i  = None\n",
    "        job_id_bare_i = None\n",
    "        all_jobs_in_group_done = None\n",
    "        any_bare_done = None\n",
    "        any_oh_done = None\n",
    "        any_o_done = None\n",
    "        any_o_done_with_active_sites = None\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i = dict()\n",
    "        # #####################################################\n",
    "        name_dict_i = dict(zip(groupby_cols, name_i))\n",
    "        # #####################################################\n",
    "        compenv_i = name_i[0]\n",
    "        slab_id_i = name_i[1]\n",
    "        active_site_i = name_i[2]\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        out_dict = get_group_w_all_ads(\n",
    "            name=name_i,\n",
    "            group=group,\n",
    "            df_jobs_anal_i=df_jobs_anal_i,\n",
    "            )\n",
    "        group_i = out_dict[\"group_i\"]\n",
    "        any_o_done_with_active_sites = out_dict[\"any_o_done_with_active_sites\"]\n",
    "\n",
    "\n",
    "        all_jobs_in_group_done = group_i.job_completely_done.all()\n",
    "\n",
    "\n",
    "        # Check that potential energy is numerical\n",
    "        for i in group_i.pot_e.tolist():\n",
    "            if type(i) != float:\n",
    "                print(\"A non-numerical potential energy entered WF: \", name_i)\n",
    "\n",
    "\n",
    "        # Only consider done jobs from here\n",
    "        group_done_i = group_i[group_i.job_completely_done == True]\n",
    "\n",
    "        group_ind_i = group_done_i.index.to_frame()\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        necessary_ads_present = False\n",
    "        # #####################################################\n",
    "        o_avail = \"o\" in group_ind_i.ads.tolist()\n",
    "        oh_avail = \"oh\" in group_ind_i.ads.tolist()\n",
    "        bare_avail = \"bare\" in group_ind_i.ads.tolist()\n",
    "        # #####################################################\n",
    "        if o_avail and oh_avail and bare_avail:\n",
    "            necessary_ads_present = True\n",
    "        # #####################################################\n",
    "\n",
    "        if necessary_ads_present:\n",
    "            tmp = 42\n",
    "\n",
    "            oer_trip_dict_i = get_oer_triplet__magmom(\n",
    "                name=name_i,\n",
    "                group=group_done_i,\n",
    "                df_jobs=df_jobs,\n",
    "                df_jobs_oh_anal=df_jobs_oh_anal,\n",
    "                df_magmom_drift=df_magmom_drift,\n",
    "                )\n",
    "            oer_trip_i = oer_trip_dict_i[\"df_oer_triplet\"]\n",
    "            error = oer_trip_dict_i[\"error\"]\n",
    "            note = oer_trip_dict_i[\"note\"]\n",
    "\n",
    "            # oer_trip_dict_i = get_oer_triplet__magmom(\n",
    "            #     name=name_i,\n",
    "            #     group=group_done_i,\n",
    "            #     df_jobs_oh_anal=df_jobs_oh_anal,\n",
    "            #     )\n",
    "            # oer_trip_i = oer_trip_dict_i[\"df_oer_triplet\"]\n",
    "            # error = oer_trip_dict_i[\"error\"]\n",
    "            # note = oer_trip_dict_i[\"note\"]\n",
    "\n",
    "\n",
    "            ads_g_o_i = None\n",
    "            ads_g_oh_i = None\n",
    "            ads_e_o_i = None\n",
    "            ads_e_oh_i = None\n",
    "            if not error:\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_o_i = oer_trip_i.loc[idx[:, :, \"o\", :, :], :].iloc[0]\n",
    "                job_id_o_i = row_o_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_oh_i = oer_trip_i.loc[idx[:, :, \"oh\", :, :], :].iloc[0]\n",
    "                job_id_oh_i = row_oh_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_bare_i = oer_trip_i.loc[idx[:, :, \"bare\", :, :], :].iloc[0]\n",
    "                job_id_bare_i = row_bare_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                # COmputing adsorption energy\n",
    "                df_ads_i = calc_ads_e(oer_trip_i.reset_index())\n",
    "                df_ads_i = df_ads_i.set_index(\"ads\", drop=False)\n",
    "\n",
    "                ads_g_o_i = df_ads_i.loc[\"o\"][\"ads_e\"]\n",
    "                ads_e_o_i = df_ads_i.loc[\"o\"][\"ads_e_elec\"]\n",
    "\n",
    "                ads_g_oh_i = df_ads_i.loc[\"oh\"][\"ads_e\"]\n",
    "                ads_e_oh_i = df_ads_i.loc[\"oh\"][\"ads_e_elec\"]\n",
    "                job_id_oh_i = df_ads_i.loc[\"oh\"][\"job_id_max\"]\n",
    "\n",
    "\n",
    "            # #################################################\n",
    "            data_dict_i.update(name_dict_i)\n",
    "            # #################################################\n",
    "            data_dict_i[\"g_o\"] = ads_g_o_i\n",
    "            data_dict_i[\"g_oh\"] = ads_g_oh_i\n",
    "            data_dict_i[\"e_o\"] = ads_e_o_i\n",
    "            data_dict_i[\"e_oh\"] = ads_e_oh_i\n",
    "            data_dict_i[\"job_id_o\"] = job_id_o_i\n",
    "            data_dict_i[\"job_id_oh\"] = job_id_oh_i \n",
    "            data_dict_i[\"job_id_bare\"] = job_id_bare_i\n",
    "            data_dict_i[\"all_done\"] = all_jobs_in_group_done\n",
    "            data_dict_i[\"any_bare_done\"] = any_bare_done\n",
    "            data_dict_i[\"any_oh_done\"] = any_oh_done\n",
    "            data_dict_i[\"any_o_done\"] = any_o_done\n",
    "            data_dict_i[\"any_o_w_as_done\"] = any_o_done_with_active_sites\n",
    "            data_dict_i[\"error\"] = error\n",
    "            data_dict_i[\"note\"] = note\n",
    "            # #################################################\n",
    "            data_dict_list.append(data_dict_i)\n",
    "            # #################################################\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    df_ads__magmom = pd.DataFrame(data_dict_list)\n",
    "    # #########################################################\n",
    "\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    # Pickling data ###########################################\n",
    "    directory = os.path.join(\n",
    "        root_dir, \"out_data\")\n",
    "    if not os.path.exists(directory): os.makedirs(directory)\n",
    "    with open(os.path.join(directory, \"df_ads__magmom.pickle\"), \"wb\") as fle:\n",
    "        pickle.dump(df_ads__magmom, fle)\n",
    "    # #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `my_oer_picker`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_i = os.path.join(\n",
    "    root_dir, \"out_data\",\n",
    "    \"df_ads__mine.pickle\",\n",
    "    )\n",
    "\n",
    "my_file = Path(path_i)\n",
    "if my_file.is_file():\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_ads__mine = pickle.load(fle)\n",
    "else:\n",
    "# if True:\n",
    "\n",
    "    # #########################################################\n",
    "    data_dict_list = []\n",
    "    # #########################################################\n",
    "    groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "    grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "    for name_i, group in grouped:\n",
    "\n",
    "    # if True:\n",
    "    #     # name_i = ('sherlock', 'vipikema_98', 47.0)\n",
    "    #     # name_i = ('nersc', 'kalisule_45', 62.0)\n",
    "    #     # name_i = ('sherlock', 'momaposi_60', 50.0)\n",
    "    #     name_i = ('nersc', 'fosurufu_23', 43.0)\n",
    "    #     group = grouped.get_group(name_i)\n",
    "\n",
    "\n",
    "        # print(name_i)\n",
    "\n",
    "        # #####################################################\n",
    "        ads_e_o_i = None\n",
    "        ads_e_oh_i = None\n",
    "        job_id_o_i = None\n",
    "        job_id_oh_i  = None\n",
    "        job_id_bare_i = None\n",
    "        all_jobs_in_group_done = None\n",
    "        any_bare_done = None\n",
    "        any_oh_done = None\n",
    "        any_o_done = None\n",
    "        any_o_done_with_active_sites = None\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i = dict()\n",
    "        # #####################################################\n",
    "        name_dict_i = dict(zip(groupby_cols, name_i))\n",
    "        # #####################################################\n",
    "        compenv_i = name_i[0]\n",
    "        slab_id_i = name_i[1]\n",
    "        active_site_i = name_i[2]\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        out_dict = get_group_w_all_ads(\n",
    "            name=name_i,\n",
    "            group=group,\n",
    "            df_jobs_anal_i=df_jobs_anal_i,\n",
    "            )\n",
    "        group_i = out_dict[\"group_i\"]\n",
    "        any_o_done_with_active_sites = out_dict[\"any_o_done_with_active_sites\"]\n",
    "\n",
    "\n",
    "        all_jobs_in_group_done = group_i.job_completely_done.all()\n",
    "\n",
    "\n",
    "        # Check that potential energy is numerical\n",
    "        for i in group_i.pot_e.tolist():\n",
    "            if type(i) != float:\n",
    "                print(\"A non-numerical potential energy entered WF: \", name_i)\n",
    "\n",
    "\n",
    "        # Only consider done jobs from here\n",
    "        group_done_i = group_i[group_i.job_completely_done == True]\n",
    "\n",
    "        group_ind_i = group_done_i.index.to_frame()\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        necessary_ads_present = False\n",
    "        # #####################################################\n",
    "        o_avail = \"o\" in group_ind_i.ads.tolist()\n",
    "        oh_avail = \"oh\" in group_ind_i.ads.tolist()\n",
    "        bare_avail = \"bare\" in group_ind_i.ads.tolist()\n",
    "        # #####################################################\n",
    "        if o_avail and oh_avail and bare_avail:\n",
    "            necessary_ads_present = True\n",
    "        # #####################################################\n",
    "\n",
    "        if necessary_ads_present:\n",
    "            # tmp = 42\n",
    "            # get_oer_triplet\n",
    "            # oer_trip_dict_i = get_oer_triplet__magmom(\n",
    "\n",
    "\n",
    "            oer_trip_dict_i = get_oer_triplet(\n",
    "                name=name_i,\n",
    "                group=group_done_i,\n",
    "                df_jobs_oh_anal=df_jobs_oh_anal,\n",
    "                heuristic__if_lower_e=True,\n",
    "                )\n",
    "            oer_trip_i = oer_trip_dict_i\n",
    "\n",
    "            # oer_trip_i = oer_trip_dict_i[\"df_oer_triplet\"]\n",
    "            # error = oer_trip_dict_i[\"error\"]\n",
    "            # note = oer_trip_dict_i[\"note\"]\n",
    "\n",
    "            error = False\n",
    "\n",
    "\n",
    "            ads_g_o_i = None\n",
    "            ads_g_oh_i = None\n",
    "            ads_e_o_i = None\n",
    "            ads_e_oh_i = None\n",
    "            # if not error:\n",
    "            if \"oh\" in oer_trip_i.index.to_frame()[\"ads\"].unique().tolist():\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_o_i = oer_trip_i.loc[idx[:, :, \"o\", :, :], :].iloc[0]\n",
    "                job_id_o_i = row_o_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_oh_i = oer_trip_i.loc[idx[:, :, \"oh\", :, :], :].iloc[0]\n",
    "                job_id_oh_i = row_oh_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_bare_i = oer_trip_i.loc[idx[:, :, \"bare\", :, :], :].iloc[0]\n",
    "                job_id_bare_i = row_bare_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                # COmputing adsorption energy\n",
    "                df_ads_i = calc_ads_e(oer_trip_i.reset_index())\n",
    "                df_ads_i = df_ads_i.set_index(\"ads\", drop=False)\n",
    "\n",
    "                ads_g_o_i = df_ads_i.loc[\"o\"][\"ads_e\"]\n",
    "                ads_e_o_i = df_ads_i.loc[\"o\"][\"ads_e_elec\"]\n",
    "\n",
    "                ads_g_oh_i = df_ads_i.loc[\"oh\"][\"ads_e\"]\n",
    "                ads_e_oh_i = df_ads_i.loc[\"oh\"][\"ads_e_elec\"]\n",
    "                job_id_oh_i = df_ads_i.loc[\"oh\"][\"job_id_max\"]\n",
    "\n",
    "\n",
    "            # #################################################\n",
    "            data_dict_i.update(name_dict_i)\n",
    "            # #################################################\n",
    "            data_dict_i[\"g_o\"] = ads_g_o_i\n",
    "            data_dict_i[\"g_oh\"] = ads_g_oh_i\n",
    "            data_dict_i[\"e_o\"] = ads_e_o_i\n",
    "            data_dict_i[\"e_oh\"] = ads_e_oh_i\n",
    "            data_dict_i[\"job_id_o\"] = job_id_o_i\n",
    "            data_dict_i[\"job_id_oh\"] = job_id_oh_i \n",
    "            data_dict_i[\"job_id_bare\"] = job_id_bare_i\n",
    "            data_dict_i[\"all_done\"] = all_jobs_in_group_done\n",
    "            data_dict_i[\"any_bare_done\"] = any_bare_done\n",
    "            data_dict_i[\"any_oh_done\"] = any_oh_done\n",
    "            data_dict_i[\"any_o_done\"] = any_o_done\n",
    "            data_dict_i[\"any_o_w_as_done\"] = any_o_done_with_active_sites\n",
    "            data_dict_i[\"error\"] = error\n",
    "            # data_dict_i[\"note\"] = note\n",
    "            # #################################################\n",
    "            data_dict_list.append(data_dict_i)\n",
    "            # #################################################\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    df_ads__mine = pd.DataFrame(data_dict_list)\n",
    "    # #########################################################\n",
    "\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    # Pickling data ###########################################\n",
    "    directory = os.path.join(\n",
    "        root_dir, \"out_data\")\n",
    "    if not os.path.exists(directory): os.makedirs(directory)\n",
    "    with open(os.path.join(directory, \"df_ads__mine.pickle\"), \"wb\") as fle:\n",
    "        pickle.dump(df_ads__mine, fle)\n",
    "    # #########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `my_oer_picker_2`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_i = os.path.join(\n",
    "    root_dir, \"out_data\",\n",
    "    \"df_ads__mine_2.pickle\",\n",
    "    )\n",
    "\n",
    "my_file = Path(path_i)\n",
    "if my_file.is_file():\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_ads__mine_2 = pickle.load(fle)\n",
    "else:\n",
    "# if True:\n",
    "\n",
    "    # #########################################################\n",
    "    data_dict_list = []\n",
    "    # #########################################################\n",
    "    groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "    grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "    for name_i, group in grouped:\n",
    "\n",
    "    # if True:\n",
    "    #     # name_i = ('sherlock', 'vipikema_98', 47.0)\n",
    "    #     # name_i = ('nersc', 'kalisule_45', 62.0)\n",
    "    #     # name_i = ('sherlock', 'momaposi_60', 50.0)\n",
    "    #     name_i = ('nersc', 'fosurufu_23', 43.0)\n",
    "    #     group = grouped.get_group(name_i)\n",
    "\n",
    "\n",
    "        # print(name_i)\n",
    "\n",
    "        # #####################################################\n",
    "        ads_e_o_i = None\n",
    "        ads_e_oh_i = None\n",
    "        job_id_o_i = None\n",
    "        job_id_oh_i  = None\n",
    "        job_id_bare_i = None\n",
    "        all_jobs_in_group_done = None\n",
    "        any_bare_done = None\n",
    "        any_oh_done = None\n",
    "        any_o_done = None\n",
    "        any_o_done_with_active_sites = None\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        data_dict_i = dict()\n",
    "        # #####################################################\n",
    "        name_dict_i = dict(zip(groupby_cols, name_i))\n",
    "        # #####################################################\n",
    "        compenv_i = name_i[0]\n",
    "        slab_id_i = name_i[1]\n",
    "        active_site_i = name_i[2]\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        out_dict = get_group_w_all_ads(\n",
    "            name=name_i,\n",
    "            group=group,\n",
    "            df_jobs_anal_i=df_jobs_anal_i,\n",
    "            )\n",
    "        group_i = out_dict[\"group_i\"]\n",
    "        any_o_done_with_active_sites = out_dict[\"any_o_done_with_active_sites\"]\n",
    "\n",
    "\n",
    "        all_jobs_in_group_done = group_i.job_completely_done.all()\n",
    "\n",
    "\n",
    "        # Check that potential energy is numerical\n",
    "        for i in group_i.pot_e.tolist():\n",
    "            if type(i) != float:\n",
    "                print(\"A non-numerical potential energy entered WF: \", name_i)\n",
    "\n",
    "\n",
    "        # Only consider done jobs from here\n",
    "        group_done_i = group_i[group_i.job_completely_done == True]\n",
    "\n",
    "        group_ind_i = group_done_i.index.to_frame()\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        necessary_ads_present = False\n",
    "        # #####################################################\n",
    "        o_avail = \"o\" in group_ind_i.ads.tolist()\n",
    "        oh_avail = \"oh\" in group_ind_i.ads.tolist()\n",
    "        bare_avail = \"bare\" in group_ind_i.ads.tolist()\n",
    "        # #####################################################\n",
    "        if o_avail and oh_avail and bare_avail:\n",
    "            necessary_ads_present = True\n",
    "        # #####################################################\n",
    "\n",
    "        if necessary_ads_present:\n",
    "            # tmp = 42\n",
    "            # get_oer_triplet\n",
    "            # oer_trip_dict_i = get_oer_triplet__magmom(\n",
    "\n",
    "\n",
    "            oer_trip_dict_i = get_oer_triplet(\n",
    "                name=name_i,\n",
    "                group=group_done_i,\n",
    "                df_jobs_oh_anal=df_jobs_oh_anal,\n",
    "                heuristic__if_lower_e=False,\n",
    "                )\n",
    "            oer_trip_i = oer_trip_dict_i\n",
    "\n",
    "            # oer_trip_i = oer_trip_dict_i[\"df_oer_triplet\"]\n",
    "            # error = oer_trip_dict_i[\"error\"]\n",
    "            # note = oer_trip_dict_i[\"note\"]\n",
    "\n",
    "            error = False\n",
    "\n",
    "\n",
    "            ads_g_o_i = None\n",
    "            ads_g_oh_i = None\n",
    "            ads_e_o_i = None\n",
    "            ads_e_oh_i = None\n",
    "            # if not error:\n",
    "            if \"oh\" in oer_trip_i.index.to_frame()[\"ads\"].unique().tolist():\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_o_i = oer_trip_i.loc[idx[:, :, \"o\", :, :], :].iloc[0]\n",
    "                job_id_o_i = row_o_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_oh_i = oer_trip_i.loc[idx[:, :, \"oh\", :, :], :].iloc[0]\n",
    "                job_id_oh_i = row_oh_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_bare_i = oer_trip_i.loc[idx[:, :, \"bare\", :, :], :].iloc[0]\n",
    "                job_id_bare_i = row_bare_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                # COmputing adsorption energy\n",
    "                df_ads_i = calc_ads_e(oer_trip_i.reset_index())\n",
    "                df_ads_i = df_ads_i.set_index(\"ads\", drop=False)\n",
    "\n",
    "                ads_g_o_i = df_ads_i.loc[\"o\"][\"ads_e\"]\n",
    "                ads_e_o_i = df_ads_i.loc[\"o\"][\"ads_e_elec\"]\n",
    "\n",
    "                ads_g_oh_i = df_ads_i.loc[\"oh\"][\"ads_e\"]\n",
    "                ads_e_oh_i = df_ads_i.loc[\"oh\"][\"ads_e_elec\"]\n",
    "                job_id_oh_i = df_ads_i.loc[\"oh\"][\"job_id_max\"]\n",
    "\n",
    "\n",
    "            # #################################################\n",
    "            data_dict_i.update(name_dict_i)\n",
    "            # #################################################\n",
    "            data_dict_i[\"g_o\"] = ads_g_o_i\n",
    "            data_dict_i[\"g_oh\"] = ads_g_oh_i\n",
    "            data_dict_i[\"e_o\"] = ads_e_o_i\n",
    "            data_dict_i[\"e_oh\"] = ads_e_oh_i\n",
    "            data_dict_i[\"job_id_o\"] = job_id_o_i\n",
    "            data_dict_i[\"job_id_oh\"] = job_id_oh_i \n",
    "            data_dict_i[\"job_id_bare\"] = job_id_bare_i\n",
    "            data_dict_i[\"all_done\"] = all_jobs_in_group_done\n",
    "            data_dict_i[\"any_bare_done\"] = any_bare_done\n",
    "            data_dict_i[\"any_oh_done\"] = any_oh_done\n",
    "            data_dict_i[\"any_o_done\"] = any_o_done\n",
    "            data_dict_i[\"any_o_w_as_done\"] = any_o_done_with_active_sites\n",
    "            data_dict_i[\"error\"] = error\n",
    "            # data_dict_i[\"note\"] = note\n",
    "            # #################################################\n",
    "            data_dict_list.append(data_dict_i)\n",
    "            # #################################################\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    df_ads__mine_2 = pd.DataFrame(data_dict_list)\n",
    "    # #########################################################\n",
    "\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    # Pickling data ###########################################\n",
    "    directory = os.path.join(\n",
    "        root_dir, \"out_data\")\n",
    "    if not os.path.exists(directory): os.makedirs(directory)\n",
    "    with open(os.path.join(directory, \"df_ads__mine_2.pickle\"), \"wb\") as fle:\n",
    "        pickle.dump(df_ads__mine_2, fle)\n",
    "    # #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ads__mine\n",
    "# df_ads__mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oer_trip_dict_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ads__from_oh.loc[('nersc', 'kalisule_45', 62.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name_i, row_mine_i in df_ads__mine_2.iterrows():\n",
    "    job_id_o_1_i = row_mine_i.job_id_o\n",
    "\n",
    "    row_from_oh_i = df_ads__from_oh.loc[name_i]\n",
    "    job_id_o_2_i =row_from_oh_i.job_id_o\n",
    "\n",
    "    if not job_id_o_1_i == job_id_o_2_i:\n",
    "        print(\"\")\n",
    "        print(name_i)\n",
    "        print(job_id_o_1_i, job_id_o_2_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different methods"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads__magmom = df_ads__magmom.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "df_ads__from_oh = df_ads__from_oh.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "df_ads__low_e = df_ads__low_e.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "df_ads__mine = df_ads__mine.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "df_ads__mine_2 = df_ads__mine_2.set_index([\"compenv\", \"slab_id\", \"active_site\", ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices = df_ads__low_e.index.tolist() + \\\n",
    "    df_ads__from_oh.index.tolist() + \\\n",
    "    df_ads__magmom.index.tolist()\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(all_indices)\n",
    "idx = idx.drop_duplicates()\n",
    "\n",
    "unique_indices = idx.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for name_i in unique_indices:\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "    rows_dict = dict()\n",
    "\n",
    "    row_magmom__exists = False\n",
    "    if name_i in df_ads__magmom.index:\n",
    "        row_magmom_i = df_ads__magmom.loc[name_i]\n",
    "        rows_dict[\"magmom\"] = row_magmom_i\n",
    "        row_magmom__exists = True\n",
    "\n",
    "    row_from_oh__exists = False\n",
    "    if name_i in df_ads__from_oh.index:\n",
    "        row_from_oh_i = df_ads__from_oh.loc[name_i]\n",
    "        rows_dict[\"from_oh\"] = row_from_oh_i\n",
    "        row_from_oh__exists = True\n",
    "\n",
    "    row_low_e__exists = False\n",
    "    if name_i in df_ads__low_e.index:\n",
    "        row_low_e_i = df_ads__low_e.loc[name_i]\n",
    "        rows_dict[\"low_e\"] = row_low_e_i\n",
    "        row_low_e__exists = True\n",
    "\n",
    "    row_mine__exists = False\n",
    "    if name_i in df_ads__mine.index:\n",
    "        row_mine_i = df_ads__mine.loc[name_i]\n",
    "        rows_dict[\"mine\"] = row_mine_i\n",
    "        row_mine__exists = True\n",
    "\n",
    "    row_mine_2__exists = False\n",
    "    if name_i in df_ads__mine_2.index:\n",
    "        row_mine_2_i = df_ads__mine_2.loc[name_i]\n",
    "        rows_dict[\"mine_2\"] = row_mine_2_i\n",
    "        row_mine_2__exists = True\n",
    "\n",
    "    # print(\n",
    "\n",
    "    #     # \"\\n\",\n",
    "    #     \"row_magmom__exists:  \", row_magmom__exists,\n",
    "\n",
    "    #     \"\\n\",\n",
    "    #     \"row_from_oh__exists: \", row_from_oh__exists,\n",
    "\n",
    "    #     \"\\n\",\n",
    "    #     \"row_low_e__exists:   \", row_low_e__exists,\n",
    "\n",
    "    #     sep=\"\")\n",
    "    # print(20 * \"-\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    comparisons = dict()\n",
    "    for key_i, row_i in rows_dict.items():\n",
    "\n",
    "        for key_j, row_j in rows_dict.items():\n",
    "            sorted_keys_ij = tuple(np.sort([key_i, key_j]))\n",
    "\n",
    "            if key_i == key_j:\n",
    "                continue\n",
    "\n",
    "            if sorted_keys_ij not in comparisons:\n",
    "\n",
    "                job_id_o__same = False\n",
    "                if rows_dict[key_i].job_id_o == rows_dict[key_j].job_id_o:\n",
    "                    job_id_o__same = True\n",
    "\n",
    "                job_id_oh__same = False\n",
    "                if rows_dict[key_i].job_id_oh == rows_dict[key_j].job_id_oh:\n",
    "                    job_id_oh__same = True\n",
    "\n",
    "                job_id_bare__same = False\n",
    "                if rows_dict[key_i].job_id_bare == rows_dict[key_j].job_id_bare:\n",
    "                    job_id_bare__same = True\n",
    "\n",
    "                data_ij = dict(\n",
    "                    job_id_o__same=job_id_o__same,\n",
    "                    job_id_oh__same=job_id_oh__same,\n",
    "                    job_id_bare__same=job_id_bare__same,\n",
    "                    )\n",
    "\n",
    "                comparisons[sorted_keys_ij] = data_ij\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "    # #####################################################\n",
    "    for key_i, data_ij in comparisons.items():\n",
    "        name_pre = \"__\".join(list(key_i))\n",
    "        for key_j, val_j in data_ij.items():\n",
    "            new_name = name_pre + \"__\" + key_j\n",
    "            data_dict_i[new_name] = val_j\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "# #########################################################\n",
    "df_oer_trip_comp = pd.DataFrame(data_dict_list)\n",
    "df_oer_trip_comp = df_oer_trip_comp.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = []\n",
    "for col_i in df_oer_trip_comp.columns:\n",
    "    modes = col_i.split(\"__\")[0:2]\n",
    "\n",
    "    new_col_i = (\n",
    "        \"__\".join(modes),\n",
    "        col_i.split(\"__\")[2],\n",
    "        )\n",
    "    new_cols.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_cols)\n",
    "\n",
    "df_oer_trip_comp.columns = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_0_levels = list(df_oer_trip_comp.columns.levels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level_0_i in list(df_oer_trip_comp.columns.levels[0]):\n",
    "    df_oer_trip_comp_i = df_oer_trip_comp[level_0_i]\n",
    "    df_oer_trip_comp[(level_0_i, \"all_True\")] = df_oer_trip_comp_i.all(axis=1)\n",
    "\n",
    "cols_tmp = []\n",
    "for i in list(df_oer_trip_comp.columns.levels[0]):\n",
    "    cols_tmp.append((i, \"all_True\", ))\n",
    "\n",
    "all_all_True_col = df_oer_trip_comp[\n",
    "    cols_tmp\n",
    "    ].all(axis=1)\n",
    "\n",
    "df_oer_trip_comp[(\"all_all_True\", \"\", )] = all_all_True_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df_oer_trip_comp[df_oer_trip_comp.all_all_True == True]\n",
    "\n",
    "print(\n",
    "    df_tmp.shape[0],\n",
    "    \" systems have identical OER sets regardless of what method is used\",\n",
    "    sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    df_oer_trip_comp.shape[0],\n",
    "    \" TOTAL SYSTEMS\",\n",
    "    \"\\n\",\n",
    "    20 * \"-\",\n",
    "    sep=\"\")\n",
    "\n",
    "for main_0_lev_i in main_0_levels:\n",
    "\n",
    "    df_tmp = df_oer_trip_comp[\n",
    "        (main_0_lev_i, \"all_True\", )\n",
    "        ]\n",
    "\n",
    "    print(\n",
    "        df_tmp[df_tmp == True].shape[0],\n",
    "        \" systems have identical OER sets for \",\n",
    "        main_0_lev_i,\n",
    "        sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "shared_scatter_props = go.Scatter(\n",
    "    mode=\"markers\",\n",
    "    marker=go.scatter.Marker(\n",
    "        opacity=0.7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "y_array = df_ads__low_e.g_o\n",
    "x_array = df_ads__low_e.g_oh\n",
    "trace_i = go.Scatter(\n",
    "    x=x_array,\n",
    "    y=y_array,\n",
    "    name=\"low_e\",\n",
    "    )\n",
    "trace_i.update(\n",
    "    dict1=shared_scatter_props.to_plotly_json(),\n",
    "    )\n",
    "data.append(trace_i)\n",
    "\n",
    "# #########################################################\n",
    "y_array = df_ads__from_oh.g_o\n",
    "x_array = df_ads__from_oh.g_oh\n",
    "trace_i = go.Scatter(\n",
    "    x=x_array,\n",
    "    y=y_array,\n",
    "    name=\"from_oh\",\n",
    "    )\n",
    "trace_i.update(\n",
    "    dict1=shared_scatter_props.to_plotly_json(),\n",
    "    )\n",
    "data.append(trace_i)\n",
    "\n",
    "# #########################################################\n",
    "y_array = df_ads__magmom.g_o\n",
    "x_array = df_ads__magmom.g_oh\n",
    "trace_i = go.Scatter(\n",
    "    x=x_array,\n",
    "    y=y_array,\n",
    "    name=\"magmom\",\n",
    "    )\n",
    "trace_i.update(\n",
    "    dict1=shared_scatter_props.to_plotly_json(),\n",
    "    )\n",
    "data.append(trace_i)\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing all triplet combinations"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_i = os.path.join(\n",
    "    root_dir, \"out_data\",\n",
    "    \"df_dict.pickle\",\n",
    "    )\n",
    "\n",
    "my_file = Path(path_i)\n",
    "if my_file.is_file():\n",
    "    with open(path_i, \"rb\") as fle:\n",
    "        df_dict = pickle.load(fle)\n",
    "else:\n",
    "\n",
    "\n",
    "    # #########################################################\n",
    "    data_dict_list = []\n",
    "    df_dict = dict()\n",
    "    # #########################################################\n",
    "    groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "    grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "    for name_i, group in grouped:\n",
    "\n",
    "    # if True:\n",
    "    #     name_i = ('sherlock', 'vipikema_98', 47.0)\n",
    "    #     group = grouped.get_group(name_i)\n",
    "\n",
    "        # print(20 * \"-\")\n",
    "        # print(name_i)\n",
    "\n",
    "        # #####################################################\n",
    "        ads_e_o_i = None\n",
    "        ads_e_oh_i = None\n",
    "        job_id_o_i = None\n",
    "        job_id_oh_i  = None\n",
    "        job_id_bare_i = None\n",
    "        all_jobs_in_group_done = None\n",
    "        any_bare_done = None\n",
    "        any_oh_done = None\n",
    "        any_o_done = None\n",
    "        any_o_done_with_active_sites = None\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        # #####################################################\n",
    "        name_dict_i = dict(zip(groupby_cols, name_i))\n",
    "        # #####################################################\n",
    "        compenv_i = name_i[0]\n",
    "        slab_id_i = name_i[1]\n",
    "        active_site_i = name_i[2]\n",
    "        # #####################################################\n",
    "\n",
    "\n",
    "        out_dict = get_group_w_all_ads(\n",
    "            name=name_i,\n",
    "            group=group,\n",
    "            df_jobs_anal_i=df_jobs_anal_i,\n",
    "            )\n",
    "        group_i = out_dict[\"group_i\"]\n",
    "        any_o_done_with_active_sites = out_dict[\"any_o_done_with_active_sites\"]\n",
    "\n",
    "\n",
    "        all_jobs_in_group_done = group_i.job_completely_done.all()\n",
    "\n",
    "        # Check that potential energy is numerical\n",
    "        for i in group_i.pot_e.tolist():\n",
    "            if type(i) != float:\n",
    "                print(\"A non-numerical potential energy entered WF: \", name_i)\n",
    "\n",
    "\n",
    "        # Only consider done jobs from here\n",
    "        group_done_i = group_i[group_i.job_completely_done == True]\n",
    "\n",
    "        group_ind_i = group_done_i.index.to_frame()\n",
    "\n",
    "\n",
    "        # #####################################################\n",
    "        necessary_ads_present = False\n",
    "        # #####################################################\n",
    "        o_avail = \"o\" in group_ind_i.ads.tolist()\n",
    "        oh_avail = \"oh\" in group_ind_i.ads.tolist()\n",
    "        bare_avail = \"bare\" in group_ind_i.ads.tolist()\n",
    "        # #####################################################\n",
    "        if o_avail and oh_avail and bare_avail:\n",
    "            necessary_ads_present = True\n",
    "        # #####################################################\n",
    "\n",
    "        if necessary_ads_present:\n",
    "\n",
    "            all_triplets = list(itertools.combinations(group_done_i.job_id_max.tolist(), 3))\n",
    "\n",
    "            data_dict_list = []\n",
    "            good_triplets = []\n",
    "            for trip_i in all_triplets:\n",
    "                df_i = pd.concat([\n",
    "                    group_done_i.index.to_frame(),\n",
    "                    group_done_i],\n",
    "                    axis=1)\n",
    "\n",
    "                df_i = df_i.set_index(\"job_id_max\")\n",
    "\n",
    "                df_trip_i = df_i.loc[list(trip_i)]\n",
    "\n",
    "                num_uniq_ads = len(list(df_trip_i.ads.unique()))\n",
    "\n",
    "                if num_uniq_ads == 3:\n",
    "                    good_triplets.append(trip_i)\n",
    "\n",
    "\n",
    "            # good_triplets = [\n",
    "            #     ('nowowesi_15', 'nihihagu_67', 'fufohoru_09'),\n",
    "            #     ('nowowesi_15', 'kofakibu_00', 'kogabeku_65'),\n",
    "            #     # ('nowowesi_15', 'kofakibu_00', 'kenewina_92'),\n",
    "            #     # ('pekukele_64', 'nihihagu_67', 'kenewina_92'),\n",
    "            #     # ('pekukele_64', 'kofakibu_00', 'kenewina_92'),\n",
    "            #     ]\n",
    "            for trip_i in good_triplets:\n",
    "                # print(trip_i)\n",
    "\n",
    "                df = group_done_i\n",
    "                df = df[\n",
    "                    (df[\"job_id_max\"].isin(list(trip_i))) &\n",
    "                    [True for i in range(len(df))]\n",
    "                    ]\n",
    "                oer_trip_i = df\n",
    "\n",
    "                # from IPython.display import display\n",
    "                # display(oer_trip_i)\n",
    "\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_o_i = oer_trip_i.loc[idx[:, :, \"o\", :, :], :].iloc[0]\n",
    "                job_id_o_i = row_o_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_oh_i = oer_trip_i.loc[idx[:, :, \"oh\", :, :], :].iloc[0]\n",
    "                job_id_oh_i = row_oh_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                idx = pd.IndexSlice\n",
    "                row_bare_i = oer_trip_i.loc[idx[:, :, \"bare\", :, :], :].iloc[0]\n",
    "                job_id_bare_i = row_bare_i.job_id_max\n",
    "\n",
    "                # #################################################\n",
    "                # Computing adsorption energy\n",
    "                df_ads_i = calc_ads_e(oer_trip_i.reset_index())\n",
    "                df_ads_i = df_ads_i.set_index(\"ads\", drop=False)\n",
    "\n",
    "                ads_g_o_i = df_ads_i.loc[\"o\"][\"ads_e\"]\n",
    "                ads_e_o_i = df_ads_i.loc[\"o\"][\"ads_e_elec\"]\n",
    "\n",
    "                ads_g_oh_i = df_ads_i.loc[\"oh\"][\"ads_e\"]\n",
    "                ads_e_oh_i = df_ads_i.loc[\"oh\"][\"ads_e_elec\"]\n",
    "                job_id_oh_i = df_ads_i.loc[\"oh\"][\"job_id_max\"]\n",
    "\n",
    "\n",
    "                # #############################################\n",
    "                data_dict_i = dict()\n",
    "                # #############################################\n",
    "                data_dict_i.update(name_dict_i)\n",
    "                # #############################################\n",
    "                data_dict_i[\"g_o\"] = ads_g_o_i\n",
    "                data_dict_i[\"g_oh\"] = ads_g_oh_i\n",
    "                data_dict_i[\"e_o\"] = ads_e_o_i\n",
    "                data_dict_i[\"e_oh\"] = ads_e_oh_i\n",
    "                data_dict_i[\"job_id_o\"] = job_id_o_i\n",
    "                data_dict_i[\"job_id_oh\"] = job_id_oh_i \n",
    "                data_dict_i[\"job_id_bare\"] = job_id_bare_i\n",
    "                data_dict_i[\"all_done\"] = all_jobs_in_group_done\n",
    "                data_dict_i[\"any_bare_done\"] = any_bare_done\n",
    "                data_dict_i[\"any_oh_done\"] = any_oh_done\n",
    "                data_dict_i[\"any_o_done\"] = any_o_done\n",
    "                data_dict_i[\"any_o_w_as_done\"] = any_o_done_with_active_sites\n",
    "                # #############################################\n",
    "                data_dict_list.append(data_dict_i)\n",
    "                # #############################################\n",
    "\n",
    "\n",
    "        # #########################################################\n",
    "        df_ads_i = pd.DataFrame(data_dict_list)\n",
    "        # #########################################################\n",
    "\n",
    "        name_str_i = [str(i) for i in list(name_i)]\n",
    "        name_str_i = \"__\".join(name_str_i)\n",
    "\n",
    "        df_dict[name_str_i] = df_ads_i\n",
    "\n",
    "\n",
    "\n",
    "        # #########################################################\n",
    "        # Pickling data ###########################################\n",
    "        directory = os.path.join(\n",
    "            root_dir, \"out_data\")\n",
    "        if not os.path.exists(directory): os.makedirs(directory)\n",
    "        with open(os.path.join(directory, \"df_dict.pickle\"), \"wb\") as fle:\n",
    "            pickle.dump(df_dict, fle)\n",
    "        # #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads__from_oh = df_ads__from_oh.set_index([\"compenv\", \"slab_id\", \"active_site\", ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "import pickle; import os\n",
    "path_i = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/oer_analysis/oer_scaling\", \n",
    "    \"out_data/trace_poly_1.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    trace_poly_1 = pickle.load(fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_deviated_systems = [\n",
    "\n",
    "    ('sherlock', 'vipikema_98', 47.0),\n",
    "    ('sherlock', 'wafitemi_24', 29.0),\n",
    "    ('sherlock', 'kapapohe_58', 29.0),\n",
    "    ('sherlock', 'sifebelo_94', 63.0),\n",
    "    ('sherlock', 'momaposi_60', 54.0),\n",
    "    ('sherlock', 'kamevuse_75', 53.0),\n",
    "    ('sherlock', 'vegarebo_06', 50.0),\n",
    "    ('slac', 'dotivela_46', 26.0),\n",
    "    ('nersc', 'kererape_22', 88.0),\n",
    "    ('slac', 'damidiwi_47', 29.0),\n",
    "    ('sherlock', 'vegarebo_06', 48.0),\n",
    "    ('nersc', 'legofufi_61', 91.0),\n",
    "    ('sherlock', 'filetumi_93', 67.0),\n",
    "    ('slac', 'paritile_76', 40.0),\n",
    "    ('nersc', 'dakoputu_58', 76.0),\n",
    "    ('slac', 'damidiwi_47', 28.0),\n",
    "    ('sherlock', 'hahesegu_39', 20.0),\n",
    "    ('sherlock', 'vipikema_98', 48.0),\n",
    "    ('sherlock', 'mibumime_94', 61.0),\n",
    "    ('nersc', 'kererape_22', 94.0),\n",
    "    ('sherlock', 'sitilowi_31', 38.0),\n",
    "    ('sherlock', 'mibumime_94', 60.0),\n",
    "    ('slac', 'fevahaso_90', 27.0),\n",
    "    ('slac', 'sunuheka_77', 51.0),\n",
    "    ('slac', 'gulipita_22', 47.0),\n",
    "    ('sherlock', 'hahesegu_39', 21.0),\n",
    "    ('sherlock', 'gavibawi_45', 40.0),\n",
    "    ('slac', 'powodupo_20', 26.0),\n",
    "    ('nersc', 'dakoputu_58', 74.0),\n",
    "    ('sherlock', 'bekusuvu_00', 67.0),\n",
    "    ('slac', 'relovalu_12', 24.0),\n",
    "    ('sherlock', 'ripirefu_15', 67.0),\n",
    "    ('slac', 'hefikala_18', 64.0),\n",
    "    ('nersc', 'winomuvi_99', 83.0),\n",
    "    ('sherlock', 'filetumi_93', 65.0),\n",
    "    ('sherlock', 'vevoraso_36', 24.0),\n",
    "    ('sherlock', 'sihisalu_64', 68.0),\n",
    "    ('sherlock', 'tagediso_07', 42.0),\n",
    "    ('sherlock', 'ripirefu_15', 49.0),\n",
    "    ('slac', 'vomelawi_63', 66.0),\n",
    "\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "most_deviated_systems = [\n",
    " ('sherlock', 'vipikema_98', 47.0),\n",
    " ('sherlock', 'wafitemi_24', 29.0),\n",
    " ('sherlock', 'kapapohe_58', 29.0),\n",
    " ('sherlock', 'sifebelo_94', 63.0),\n",
    " ('sherlock', 'momaposi_60', 54.0),\n",
    " ('sherlock', 'kamevuse_75', 53.0),\n",
    " ('sherlock', 'vegarebo_06', 50.0),\n",
    " ('slac', 'dotivela_46', 26.0),\n",
    " ('nersc', 'kererape_22', 88.0),\n",
    " ('slac', 'damidiwi_47', 29.0),\n",
    " ('sherlock', 'vegarebo_06', 48.0),\n",
    " ('nersc', 'legofufi_61', 91.0),\n",
    " ('sherlock', 'filetumi_93', 67.0),\n",
    " ('slac', 'paritile_76', 40.0),\n",
    " ('nersc', 'dakoputu_58', 76.0),\n",
    " ('slac', 'damidiwi_47', 28.0),\n",
    " ('sherlock', 'hahesegu_39', 20.0),\n",
    " ('sherlock', 'vipikema_98', 48.0),\n",
    " ('sherlock', 'mibumime_94', 61.0),\n",
    " ('nersc', 'kererape_22', 94.0),\n",
    " ('sherlock', 'sitilowi_31', 38.0),\n",
    " ('sherlock', 'mibumime_94', 60.0),\n",
    " ('slac', 'fevahaso_90', 27.0),\n",
    " ('slac', 'sunuheka_77', 51.0),\n",
    " ('slac', 'gulipita_22', 47.0),\n",
    " ('sherlock', 'hahesegu_39', 21.0),\n",
    " ('sherlock', 'gavibawi_45', 40.0),\n",
    " ('slac', 'powodupo_20', 26.0),\n",
    " ('nersc', 'dakoputu_58', 74.0),\n",
    " ('sherlock', 'bekusuvu_00', 67.0),\n",
    " ('slac', 'relovalu_12', 24.0),\n",
    " ('sherlock', 'ripirefu_15', 67.0),\n",
    " ('slac', 'hefikala_18', 64.0),\n",
    " ('nersc', 'winomuvi_99', 83.0),\n",
    " ('sherlock', 'filetumi_93', 65.0),\n",
    " ('sherlock', 'vevoraso_36', 24.0),\n",
    " ('sherlock', 'sihisalu_64', 68.0),\n",
    " ('sherlock', 'tagediso_07', 42.0),\n",
    " ('sherlock', 'ripirefu_15', 49.0),\n",
    " ('slac', 'vomelawi_63', 66.0),\n",
    " ('sherlock', 'novoloko_50', 20.0),\n",
    " ('sherlock', 'lufinanu_76', 46.0),\n",
    " ('sherlock', 'bekusuvu_00', 69.0),\n",
    " ('sherlock', 'vevarehu_32', 63.0),\n",
    " ('sherlock', 'newopedu_17', 33.0),\n",
    " ('sherlock', 'fugorumi_32', 42.0),\n",
    " ('slac', 'dipamife_45', 22.0),\n",
    " ('slac', 'diwarise_06', 32.0),\n",
    " ('sherlock', 'gihiseru_17', 28.0),\n",
    " ('sherlock', 'tanewani_59', 50.0),\n",
    " ('sherlock', 'gavibawi_45', 42.0),\n",
    " ('sherlock', 'vevarehu_32', 65.0),\n",
    " ('slac', 'lagubapi_05', 39.0),\n",
    " ('sherlock', 'filetumi_93', 60.0),\n",
    " ('nersc', 'letapivu_80', 85.0),\n",
    " ('slac', 'gigisanu_24', 32.0),\n",
    " ('slac', 'nuriramu_38', 32.0),\n",
    " ('nersc', 'giworuge_14', 85.0),\n",
    " ('nersc', 'giworuge_14', 81.0),\n",
    " ('sherlock', 'mabivuso_96', 50.0),\n",
    " ('slac', 'vepufiga_56', 24.0),\n",
    " ('sherlock', 'posifuvi_45', 21.0),\n",
    " ('slac', 'seravuha_97', 41.0),\n",
    " ('nersc', 'legofufi_61', 88.0),\n",
    " ('sherlock', 'tanewani_59', 53.0),\n",
    " ('sherlock', 'kobehubu_94', 52.0),\n",
    " ('slac', 'sesiguva_21', 16.0),\n",
    " ('slac', 'vovumota_03', 32.0),\n",
    " ('sherlock', 'mabivuso_96', 48.0),\n",
    " ('sherlock', 'mokapipu_61', 61.0),\n",
    " ('sherlock', 'logusole_78', 41.0),\n",
    " ('sherlock', 'tesameli_14', 50.0),\n",
    " ('sherlock', 'pegapesa_22', 16.0),\n",
    " ('slac', 'wihuwone_95', 26.0),\n",
    " ('sherlock', 'dimafowe_05', 20.0),\n",
    " ('sherlock', 'lenabefe_62', 49.0),\n",
    " ('slac', 'votafefa_68', 35.0),\n",
    " ('sherlock', 'lenabefe_62', 48.0),\n",
    " ('sherlock', 'sifebelo_94', 65.0),\n",
    " ('sherlock', 'pidanule_44', 41.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\"slac\", \"dotivela_46\", 26., )\n",
    "\n",
    "\n",
    "df_dict[\n",
    "    \"slac__dotivela_46__26.0\"\n",
    "    ].loc[[4, 12]]\n",
    "    # ].loc[[0, 8]]\n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_scatter_props = go.Scatter(\n",
    "    mode=\"markers+lines\",\n",
    "    marker=go.scatter.Marker(\n",
    "        opacity=0.7,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "traces_to_add_at_end = []\n",
    "\n",
    "data = []\n",
    "# iterator = enumerate(random.sample(list(df_dict.keys()), 10))\n",
    "iterator = enumerate(most_deviated_systems)\n",
    "for i_cnt, name_str_i in iterator:\n",
    "    if type(name_str_i) is tuple:\n",
    "        name_str_i = \"__\".join(\n",
    "            [str(i) for i in list(name_str_i)]\n",
    "            )\n",
    "\n",
    "    # #####################################################\n",
    "    name_i = name_str_i.split(\"__\")[0:2] + [float(name_str_i.split(\"__\")[2])]\n",
    "    name_i = tuple(name_i)\n",
    "    # #####################################################\n",
    "\n",
    "    df_ads_i = df_dict[\n",
    "        name_str_i\n",
    "        ]\n",
    "\n",
    "    trace_i = go.Scatter(\n",
    "        x=df_ads_i.sort_values(\"g_oh\").g_oh,\n",
    "        y=df_ads_i.sort_values(\"g_oh\").g_o,\n",
    "        name=name_str_i + \"_X\",\n",
    "        legendgroup=name_str_i,\n",
    "        )\n",
    "    trace_i.update(\n",
    "        dict1=shared_scatter_props.to_plotly_json(),\n",
    "        )\n",
    "    data.append(trace_i)\n",
    "\n",
    "\n",
    "    if name_i in df_features_targets.index:\n",
    "        trace_i = go.Scatter(\n",
    "            x=[df_features_targets.loc[name_i][(\"targets\", \"g_oh\", \"\", )]],\n",
    "            y=[df_features_targets.loc[name_i][(\"targets\", \"g_o\", \"\", )]],\n",
    "            # y=[df_ads__from_oh.loc[name_i].g_o, ],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=12, color=\"black\", ),\n",
    "            name=name_str_i + \"_XX\",\n",
    "            legendgroup=name_str_i,\n",
    "            )\n",
    "        traces_to_add_at_end.append(trace_i)\n",
    "\n",
    "    # if name_i in df_ads__from_oh.index:\n",
    "    #     trace_i = go.Scatter(\n",
    "    #         x=[df_ads__from_oh.loc[name_i].g_oh, ],\n",
    "    #         y=[df_ads__from_oh.loc[name_i].g_o, ],\n",
    "    #         mode=\"markers\",\n",
    "    #         marker=dict(size=12, color=\"black\", ),\n",
    "    #         name=name_str_i + \"_XX\",\n",
    "    #         )\n",
    "    #     traces_to_add_at_end.append(trace_i)\n",
    "\n",
    "\n",
    "\n",
    "traces = data + traces_to_add_at_end + [trace_poly_1]\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=traces\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving figure"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_plotly_plot(\n",
    "    figure=fig,\n",
    "    # save_dir=None,\n",
    "    # place_in_out_plot=True,\n",
    "    plot_name=\"scaling_plot__all_oer_triplets\",\n",
    "    write_html=True,\n",
    "    try_orca_write=True,\n",
    "    )\n",
    "\n",
    "fig.write_json(\n",
    "    os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"dft_workflow/job_analysis/collect_collate_dft_data\",\n",
    "        \"out_plot/scaling_plot__all_oer_triplets.json\"))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from methods import get_df_struct_drift\n",
    "\n",
    "# df_struct_drift = get_df_struct_drift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ads_out[df_ads_out.ads == \"bare\"]\n",
    "\n",
    "var = \"o\"\n",
    "row_o = df_ads_out.query('ads == @var')\n",
    "job_id_o = row_o.iloc[0].job_id_max\n",
    "\n",
    "var = \"oh\"\n",
    "row_oh = df_ads_out.query('ads == @var')\n",
    "job_id_oh = row_oh.iloc[0].job_id_max\n",
    "\n",
    "var = \"bare\"\n",
    "row_bare = df_ads_out.query('ads == @var')\n",
    "job_id_bare = row_bare.iloc[0].job_id_max\n",
    "\n",
    "job_id_bare\n",
    "job_id_o\n",
    "job_id_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [job_id_bare, job_id_o]\n",
    "\n",
    "job_ids_str = \"__\".join(list(np.sort(job_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_struct_drift = df_struct_drift.set_index(\"pair_str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_drift = df_struct_drift.loc[job_ids_str]\n",
    "\n",
    "row_drift.mean_displacement"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data to pickle"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_analysis/collect_collate_dft_data\",\n",
    "    \"out_data\")\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(os.path.join(directory, \"df_ads.pickle\"), \"wb\") as fle:\n",
    "    pickle.dump(df_ads, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "from methods import get_df_ads\n",
    "\n",
    "df_ads_tmp = get_df_ads()\n",
    "df_ads_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"collect_collate_dft.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# trace_i.update(\n",
    "#     dict1=shared_scatter_props.to_plotly_json(),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# group_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# group_done_i = group_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# oer_trip_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# all_triplets = list(itertools.combinations(group_done_i.job_id_max.tolist(), 3))\n",
    "\n",
    "# data_dict_list = []\n",
    "# good_triplets = []\n",
    "# for trip_i in all_triplets:\n",
    "#     df_i = pd.concat([\n",
    "#         group_done_i.index.to_frame(),\n",
    "#         group_done_i],\n",
    "#         axis=1)\n",
    "\n",
    "#     df_i = df_i.set_index(\"job_id_max\")\n",
    "\n",
    "#     df_trip_i = df_i.loc[list(trip_i)]\n",
    "\n",
    "#     num_uniq_ads = len(list(df_trip_i.ads.unique()))\n",
    "\n",
    "#     if num_uniq_ads == 3:\n",
    "#         good_triplets.append(trip_i)\n",
    "\n",
    "\n",
    "# data_dict_list = []\n",
    "# for trip_i in good_triplets:\n",
    "#     # print(20 * \"-\")\n",
    "#     # print(trip_i)\n",
    "\n",
    "\n",
    "#     job_id_o = None\n",
    "#     job_id_oh = None\n",
    "#     job_id_bare = None\n",
    "#     for job_id_i in trip_i:\n",
    "#         if df_jobs.loc[job_id_i].ads == \"o\":\n",
    "#             job_id_o = job_id_i\n",
    "#         elif df_jobs.loc[job_id_i].ads == \"oh\":\n",
    "#             job_id_oh = job_id_i\n",
    "#         elif df_jobs.loc[job_id_i].ads == \"bare\":\n",
    "#             job_id_bare = job_id_i\n",
    "#         else:\n",
    "#             print(\"This isn't good sidjfisdj89\")\n",
    "\n",
    "#     assert job_id_bare is not None, \"TEMP\"\n",
    "#     assert job_id_o is not None, \"TEMP\"\n",
    "#     assert job_id_oh is not None, \"TEMP\"\n",
    "\n",
    "\n",
    "#     # #####################################\n",
    "#     pair_oh_bare = np.sort(\n",
    "#         [job_id_oh, job_id_bare, ]\n",
    "#         )\n",
    "#     pair_oh_bare_sort_i = tuple(pair_oh_bare)\n",
    "\n",
    "#     pair_oh_bare_sort_str_i = \"__\".join(pair_oh_bare_sort_i)\n",
    "\n",
    "\n",
    "#     # #####################################\n",
    "#     pair_o_bare = np.sort(\n",
    "#         [job_id_o, job_id_bare, ]\n",
    "#         )\n",
    "#     pair_o_bare_sort_i = tuple(pair_o_bare)\n",
    "\n",
    "#     pair_o_bare_sort_str_i = \"__\".join(pair_o_bare_sort_i)\n",
    "\n",
    "#     # #####################################\n",
    "#     df_magmom_drift_i = df_magmom_drift.loc[[\n",
    "#         pair_oh_bare_sort_str_i,\n",
    "#         pair_o_bare_sort_str_i,\n",
    "#         ]]\n",
    "\n",
    "#     magmom_diff_metric = df_magmom_drift_i[\"sum_abs_d_magmoms__nonocta_pa\"].sum()\n",
    "\n",
    "#     # print(\n",
    "#     #     magmom_diff_metric\n",
    "#     #     )\n",
    "\n",
    "#     # #####################################################\n",
    "#     data_dict_i = dict()\n",
    "#     # #####################################################\n",
    "#     # data_dict_i[\"triplet\"] = \n",
    "#     data_dict_i[\"job_id_o\"] = job_id_o\n",
    "#     data_dict_i[\"job_id_oh\"] = job_id_oh\n",
    "#     data_dict_i[\"job_id_bare\"] = job_id_bare\n",
    "#     data_dict_i[\"magmom_diff_metric\"] = magmom_diff_metric\n",
    "#     # #####################################################\n",
    "#     data_dict_list.append(data_dict_i)\n",
    "#     # #####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_trip_magmom_i = pd.DataFrame(data_dict_list)\n",
    "\n",
    "# row_best_i = df_trip_magmom_i.sort_values(\"magmom_diff_metric\").iloc[0]\n",
    "\n",
    "# # row_best_i.job_id_o\n",
    "# # row_best_i.job_id_oh\n",
    "# # row_best_i.job_id_bare\n",
    "\n",
    "# group_done_tmp = group_done_i.set_index(\"job_id_max\", drop=False)\n",
    "\n",
    "# df_ads_out = pd.concat([\n",
    "#     group_done_i[group_done_i.job_id_max == row_best_i.job_id_o],\n",
    "#     group_done_i[group_done_i.job_id_max == row_best_i.job_id_oh],\n",
    "#     group_done_i[group_done_i.job_id_max == row_best_i.job_id_bare],\n",
    "\n",
    "#     # group_done_tmp.loc[[row_best_i.job_id_o]],\n",
    "#     # group_done_tmp.loc[[row_best_i.job_id_oh]],\n",
    "#     # group_done_tmp.loc[[row_best_i.job_id_bare]],\n",
    "\n",
    "#     ], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# fifasula_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# detumalu_52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_features_targets.loc[name_i][(\"targets\", \"g_o\", \"\", )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# sorted_keys_ij in comparisons.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# # sorted_keys_ij = \n",
    "\n",
    "# # tuple(np.sort(key_i, key_j))\n",
    "\n",
    "# np.sort([key_i, key_j])\n",
    "\n",
    "# key_i\n",
    "\n",
    "# key_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# # key\n",
    "\n",
    "# if row_i.job_id_o == row_j.job_id_o:\n",
    "#     tmp = 42\n",
    "\n",
    "#     print(key_i, key_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# if row_magmom__exists:\n",
    "#     tmp = 42\n",
    "\n",
    "\n",
    "# if row_from_oh__exists:\n",
    "#     tmp = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# row_magmom_i.job_id_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {},
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# row_from_oh_i.job_id_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_oer_trip_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# new_col_i\n",
    "# new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# main_0_lev_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_tmp =\n",
    "\n",
    "# df_oer_trip_comp[df_oer_trip_comp.all_all_True == True]\n",
    "\n",
    "# df_oer_trip_comp.columns.levels[0]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
