{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect DFT data into *, *O, *OH collections\n",
    "---\n",
    "\n",
    "Notes:\n",
    "  * If there exists only a single slab for a particular adsorbate, and that slab has a averaged absolute magmom per atom of less than XXX, then we should check if there are slabs of different adsorbates in that set to tranplant the magmoms from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/job_analysis/compare_magmoms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# #########################################################\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs_anal,\n",
    "    get_df_atoms_sorted_ind,\n",
    "    get_df_magmoms,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_jobs_oh_anal,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    read_magmom_comp_data,\n",
    "    save_magmom_comp_data,\n",
    "    process_group_magmom_comp,\n",
    "    get_oer_set,\n",
    "    analyze_O_in_set,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "# verbose = True\n",
    "\n",
    "redo_all_jobs = False\n",
    "# redo_all_jobs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "df_jobs_anal_i = df_jobs_anal\n",
    "\n",
    "# #########################################################\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()\n",
    "\n",
    "# #########################################################\n",
    "magmom_data_dict = read_magmom_comp_data()\n",
    "\n",
    "# #########################################################\n",
    "df_magmoms = get_df_magmoms()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "\n",
    "# #########################################################\n",
    "df_magmoms = df_magmoms.set_index(\"job_id\")\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_oh_anal = get_df_jobs_oh_anal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind = df_jobs_anal_i.index.to_frame()\n",
    "df_jobs_anal_i = df_jobs_anal_i.loc[\n",
    "    df_ind[df_ind.job_type == \"oer_adsorbate\"].index\n",
    "    ]\n",
    "df_jobs_anal_i = df_jobs_anal_i.droplevel(level=0)\n",
    "\n",
    "df_ind = df_atoms_sorted_ind.index.to_frame()\n",
    "df_atoms_sorted_ind = df_atoms_sorted_ind.loc[\n",
    "    df_ind[df_ind.job_type == \"oer_adsorbate\"].index\n",
    "    ]\n",
    "df_atoms_sorted_ind = df_atoms_sorted_ind.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing data objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing `df_jobs_anal` (only completed job sets, filter out *O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_slab\n",
    "df_slab = get_df_slab()\n",
    "\n",
    "slab_ids_phase_2 = df_slab[df_slab.phase > 0].slab_id.tolist()\n",
    "\n",
    "# df_m2.loc[\n",
    "#     df_m2.slab_id.isin(slab_ids_phase_2)\n",
    "#     ]\n",
    "\n",
    "df_index_i = df_jobs_anal_i.index.to_frame()\n",
    "df_jobs_anal_i = df_jobs_anal_i.loc[\n",
    "    df_index_i.slab_id.isin(slab_ids_phase_2).index\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# Only completed jobs will be considered\n",
    "df_jobs_anal_i = df_jobs_anal_i[df_jobs_anal_i.job_completely_done == True]\n",
    "\n",
    "# #########################################################\n",
    "# Dropping rows that failed atoms sort, now it's just one job that blew up \n",
    "# job_id = \"dubegupi_27\"\n",
    "df_failed_to_sort = df_atoms_sorted_ind[\n",
    "    df_atoms_sorted_ind.failed_to_sort == True]\n",
    "df_jobs_anal_i = df_jobs_anal_i.drop(labels=df_failed_to_sort.index)\n",
    "\n",
    "# #########################################################\n",
    "# Remove the *O slabs for now\n",
    "# The fact that they have NaN active sites will mess up the groupby\n",
    "ads_list = df_jobs_anal_i.index.get_level_values(\"ads\").tolist()\n",
    "ads_list_no_o = [i for i in list(set(ads_list)) if i != \"o\"]\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "df_jobs_anal_no_o = df_jobs_anal_i.loc[idx[:, :, ads_list_no_o, :, :], :]\n",
    "\n",
    "# #########################################################\n",
    "# Only keep OER job sets that have all adsorbates present and completed\n",
    "indices_to_keep = []\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "for name_i, group in grouped:\n",
    "\n",
    "    # print(\"TEMP\")\n",
    "    # index_i = ('slac', 'fagumoha_68', 'oh', 62.0, 3)\n",
    "    # if index_i in group.index:\n",
    "    #     print(name_i)\n",
    "\n",
    "    group_index = group.index.to_frame()\n",
    "    ads_list = list(group_index.ads.unique())\n",
    "    oh_present = \"oh\" in ads_list\n",
    "    bare_present = \"bare\" in ads_list\n",
    "    all_req_ads_present = oh_present and bare_present\n",
    "    if all_req_ads_present:\n",
    "        indices_to_keep.extend(group.index.tolist())\n",
    "\n",
    "df_jobs_anal_no_o_all_ads_pres = df_jobs_anal_no_o.loc[\n",
    "    indices_to_keep    \n",
    "    ]\n",
    "df_i = df_jobs_anal_no_o_all_ads_pres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process `df_jobs_oh_anal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_oh_anal = df_jobs_oh_anal.set_index(\n",
    "    [\"compenv\", \"slab_id\", \"active_site\", ], drop=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if there are OER sets that have slabs with magmom 0'ed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutoff for how low the magmoms of slab can go before I rerun with different spin\n",
    "magmom_cutoff = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "verbose_local = False\n",
    "# #########################################################\n",
    "\n",
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_i.groupby(groupby_cols)\n",
    "for i_cnt, (name_i, group) in enumerate(grouped):\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    if verbose_local:\n",
    "        print(40 * \"*\")\n",
    "        print(\"name_i:\", name_i)\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    group_i = get_oer_set(\n",
    "        group=group,\n",
    "        compenv=compenv_i,\n",
    "        slab_id=slab_id_i,\n",
    "        df_jobs_anal=df_jobs_anal,\n",
    "        )\n",
    "\n",
    "    # #####################################################\n",
    "    magmom_data_out = analyze_O_in_set(\n",
    "        data_dict_i,\n",
    "        group_i,\n",
    "        df_magmoms,\n",
    "        magmom_cutoff=magmom_cutoff,\n",
    "        compenv=compenv_i,\n",
    "        slab_id=slab_id_i,\n",
    "        active_site=active_site_i,\n",
    "        )\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i.update(magmom_data_out)\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "# #########################################################\n",
    "df_m = pd.DataFrame(data_dict_list)\n",
    "df_m = df_m.set_index([\"compenv\", \"slab_id\", \"active_site\", ], drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_diff_0 = df_jobs_oh_anal.index.difference(df_m.index)\n",
    "index_diff_1 = df_m.index.difference(df_jobs_oh_anal.index)\n",
    "\n",
    "mess_i = \"This shouldn't be, look into it\"\n",
    "# assert index_diff_1.shape[0] == 0, mess_i\n",
    "\n",
    "# #########################################################\n",
    "shared_index = df_jobs_oh_anal.index.intersection(df_m.index)\n",
    "\n",
    "df_jobs_oh_anal = df_jobs_oh_anal.loc[shared_index]\n",
    "df_m = df_m.loc[shared_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_0 = list(df_m.columns)\n",
    "list_1 = list(df_jobs_oh_anal.columns)\n",
    "\n",
    "shared_cols = list(set(list_0).intersection(list_1))\n",
    "\n",
    "df_list = [\n",
    "    df_m.drop(columns=shared_cols),\n",
    "    df_jobs_oh_anal,\n",
    "    ]\n",
    "\n",
    "df_m2 = pd.concat(df_list, axis=1)\n",
    "df_m2 = df_m2.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m3 = df_m2[\n",
    "    # (df_m2[\"*O_w_low_magmoms\"] == True) & \\\n",
    "    # (df_m2[\"*O_w_not_low_magmoms\"] == False) & \\\n",
    "    (df_m2[\"all_oh_attempts_done\"] == True) & \\\n",
    "    (df_m2[\"all_jobs_bad\"] == False) & \\\n",
    "    [True for i in range(len(df_m2))]\n",
    "    ]\n",
    "# df_m3 = df_m3[\n",
    "#     df_m3.all_jobs_bad == False\n",
    "#              ]\n",
    "\n",
    "data_dict_list = []\n",
    "for i_cnt, row_i in df_m3.iterrows():\n",
    "    data_dict_i = dict()\n",
    "    \n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    slab_id_i = row_i.slab_id\n",
    "    active_site_i = row_i.active_site\n",
    "    all_oh_attempts_done_i = row_i.all_oh_attempts_done\n",
    "    job_ids_sorted_energy_i = row_i.job_ids_sorted_energy\n",
    "    job_id_most_stable_i = row_i.job_id_most_stable\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_magmoms_i = df_magmoms.loc[job_id_most_stable_i]\n",
    "    # #####################################################\n",
    "    sum_abs_magmoms_pa_i = row_magmoms_i.sum_abs_magmoms_pa\n",
    "    # #####################################################\n",
    "\n",
    "    # print(\"sum_abs_magmoms_pa_i:\", sum_abs_magmoms_pa_i)\n",
    "\n",
    "    rerun_from_oh = True\n",
    "\n",
    "    # rerun_from_oh = False\n",
    "    # # if sum_abs_magmoms_pa_i > magmom_cutoff:\n",
    "    # if sum_abs_magmoms_pa_i > 0.07:\n",
    "    #     rerun_from_oh = True\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "    data_dict_i[\"rerun_from_oh\"] = rerun_from_oh\n",
    "    # #####################################################\n",
    "    data_dict_i.update(row_i.to_dict())\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "df_rerun_from_oh = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "import os; import pickle\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_analysis/compare_magmoms\",\n",
    "    \"out_data\")\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "path_i = os.path.join(directory, \"df_rerun_from_oh.pickle\")\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_rerun_from_oh, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compenv</th>\n",
       "      <th>slab_id</th>\n",
       "      <th>active_site</th>\n",
       "      <th>rerun_from_oh</th>\n",
       "      <th>*O_w_low_magmoms</th>\n",
       "      <th>*O_w_not_low_magmoms</th>\n",
       "      <th>all_oh_attempts_done</th>\n",
       "      <th>job_id_most_stable</th>\n",
       "      <th>all_jobs_bad</th>\n",
       "      <th>job_ids_sorted_energy</th>\n",
       "      <th>job_ids_w_missing_Os</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nersc</td>\n",
       "      <td>dakoputu_58</td>\n",
       "      <td>74.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>buvawasa_30</td>\n",
       "      <td>False</td>\n",
       "      <td>[buvawasa_30, lalanota_37, miwanuho_78]</td>\n",
       "      <td>[wepewido_07]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nersc</td>\n",
       "      <td>dakoputu_58</td>\n",
       "      <td>75.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>kutabadu_95</td>\n",
       "      <td>False</td>\n",
       "      <td>[kutabadu_95, dulorome_96, wihowapo_67, satefu...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  compenv      slab_id  active_site  rerun_from_oh  *O_w_low_magmoms  \\\n",
       "0   nersc  dakoputu_58         74.0           True             False   \n",
       "1   nersc  dakoputu_58         75.0           True             False   \n",
       "\n",
       "   *O_w_not_low_magmoms  all_oh_attempts_done job_id_most_stable  \\\n",
       "0                 False                  True        buvawasa_30   \n",
       "1                 False                  True        kutabadu_95   \n",
       "\n",
       "   all_jobs_bad                              job_ids_sorted_energy  \\\n",
       "0         False            [buvawasa_30, lalanota_37, miwanuho_78]   \n",
       "1         False  [kutabadu_95, dulorome_96, wihowapo_67, satefu...   \n",
       "\n",
       "  job_ids_w_missing_Os  \n",
       "0        [wepewido_07]  \n",
       "1                   []  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from methods import get_df_rerun_from_oh\n",
    "df_rerun_from_oh_tmp = get_df_rerun_from_oh()\n",
    "df_rerun_from_oh_tmp.iloc[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the slabs with the smallest magmoms to file to manually inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df_magmoms[df_magmoms.sum_abs_magmoms_pa > 1e-5]\n",
    "df_i = df_i.sort_values(\"sum_abs_magmoms_pa\", ascending=True)\n",
    "\n",
    "for i_cnt, (job_id_i, row_i) in enumerate(df_i.iloc[0:20].iterrows()):\n",
    "\n",
    "    # #####################################################\n",
    "    row_paths_i = df_jobs_paths.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    gdrive_path_i = row_paths_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    path_i = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        gdrive_path_i,\n",
    "        \"final_with_calculator.traj\")\n",
    "\n",
    "    directory = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"dft_workflow/job_analysis/compare_magmoms\",\n",
    "        \"__temp__/low_magmom_slabs\")\n",
    "\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        directory,\n",
    "        str(i_cnt).zfill(3) + \"_\" + job_id_i + \".traj\")\n",
    "\n",
    "    shutil.copyfile(\n",
    "        path_i,\n",
    "        out_path)\n",
    "\n",
    "# df_i.iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "Run time: 0.222 min\n",
      "analyse_jobs.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"analyse_jobs.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
