{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect DFT data into *, *O, *OH collections\n",
    "---"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# #########################################################\n",
    "from IPython.display import display\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs_anal,\n",
    "    get_df_atoms_sorted_ind,\n",
    "    get_df_init_slabs,\n",
    "\n",
    "    get_other_job_ids_in_set,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    read_magmom_comp_data,\n",
    "    save_magmom_comp_data,\n",
    "    process_group_magmom_comp,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "from methods import get_df_struct_drift, get_df_magmom_drift\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redo_all_jobs = False\n",
    "# redo_all_jobs = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_jobs = get_df_jobs()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "\n",
    "# #########################################################\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()\n",
    "df_atoms_sorted_ind_2 = df_atoms_sorted_ind.set_index(\"job_id\")\n",
    "\n",
    "# #########################################################\n",
    "df_init_slabs = get_df_init_slabs()\n",
    "\n",
    "# #########################################################\n",
    "magmom_data_dict = read_magmom_comp_data()\n",
    "\n",
    "# #########################################################\n",
    "df_struct_drift = get_df_struct_drift()\n",
    "df_struct_drift = df_struct_drift.set_index(\"pair_str\", drop=False)\n",
    "\n",
    "# #########################################################\n",
    "df_magmom_drift_prev = get_df_magmom_drift()\n",
    "df_magmom_drift_prev = df_magmom_drift_prev.set_index(\"pair_str\", drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter down to `oer_adsorbate` systems"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ind = df_jobs_anal.index.to_frame()\n",
    "df_jobs_anal = df_jobs_anal.loc[\n",
    "    df_ind[df_ind.job_type == \"oer_adsorbate\"].index\n",
    "    ]\n",
    "df_jobs_anal = df_jobs_anal.droplevel(level=0)\n",
    "\n",
    "\n",
    "df_ind = df_atoms_sorted_ind.index.to_frame()\n",
    "df_atoms_sorted_ind = df_atoms_sorted_ind.loc[\n",
    "    df_ind[df_ind.job_type == \"oer_adsorbate\"].index\n",
    "    ]\n",
    "df_atoms_sorted_ind = df_atoms_sorted_ind.droplevel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "# Only completed jobs will be considered\n",
    "df_jobs_anal_i = df_jobs_anal[df_jobs_anal.job_completely_done == True]\n",
    "\n",
    "# #########################################################\n",
    "# Dropping rows that failed atoms sort, now it's just one job that blew up \n",
    "# job_id = \"dubegupi_27\"\n",
    "df_failed_to_sort = df_atoms_sorted_ind[\n",
    "    df_atoms_sorted_ind.failed_to_sort == True]\n",
    "\n",
    "# df_jobs_anal_i = df_jobs_anal_i.drop(labels=df_failed_to_sort.index)\n",
    "# df_jobs_anal_i = df_jobs_anal_i.drop(labels=df_failed_to_sort.droplevel(level=0).index)\n",
    "df_jobs_anal_i = df_jobs_anal_i.drop(labels=df_failed_to_sort.index)\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "# Remove the *O slabs for now\n",
    "# The fact that they have NaN active sites will mess up the groupby\n",
    "ads_list = df_jobs_anal_i.index.get_level_values(\"ads\").tolist()\n",
    "ads_list_no_o = [i for i in list(set(ads_list)) if i != \"o\"]\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "df_jobs_anal_no_o = df_jobs_anal_i.loc[idx[:, :, ads_list_no_o, :, :], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "indices_to_keep = []\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_jobs_anal_no_o.groupby(groupby_cols)\n",
    "# for name_i, group in grouped:\n",
    "\n",
    "if True:\n",
    "    name_i = ('nersc', 'hadogato_47', 88.0)\n",
    "    group = grouped.get_group(name_i)\n",
    "\n",
    "    group_index = group.index.to_frame()\n",
    "    ads_list = list(group_index.ads.unique())\n",
    "    oh_present = \"oh\" in ads_list\n",
    "    bare_present = \"bare\" in ads_list\n",
    "    all_req_ads_present = oh_present and bare_present\n",
    "    if all_req_ads_present:\n",
    "        indices_to_keep.extend(group.index.tolist())\n",
    "\n",
    "df_jobs_anal_no_o_all_ads_pres = df_jobs_anal_no_o.loc[\n",
    "    indices_to_keep    \n",
    "    ]\n",
    "df_i = df_jobs_anal_no_o_all_ads_pres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magmom comparison"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "groups_to_process = []\n",
    "# #########################################################\n",
    "groupby_cols = [\"compenv\", \"slab_id\", \"active_site\", ]\n",
    "grouped = df_i.groupby(groupby_cols)\n",
    "# #########################################################\n",
    "iterator = tqdm(grouped, desc=\"1st loop\")\n",
    "for i_cnt, (name_i, group) in enumerate(iterator):\n",
    "\n",
    "#     print(name_i)\n",
    "\n",
    "# if True:\n",
    "#     # name_i = ('sherlock', 'batipoha_75', 36.0)\n",
    "#     # name_i = ('sherlock', 'fogalonu_46', 16.0)\n",
    "#     # name_i = ('nersc', 'gekawore_16', 86.0)\n",
    "#     name_i = ('nersc', 'hadogato_47', 88.0)\n",
    "#     group = grouped.get_group(name_i)\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "    df_index = df_jobs_anal_i.index.to_frame()\n",
    "\n",
    "    df_index_i = df_index[\n",
    "        (df_index.compenv == compenv_i) & \\\n",
    "        (df_index.slab_id == slab_id_i) & \\\n",
    "        (df_index.ads == \"o\") & \\\n",
    "        [True for i in range(len(df_index))]\n",
    "        ]\n",
    "\n",
    "    row_o_i = df_jobs_anal_i.loc[\n",
    "        df_index_i.index    \n",
    "        ]\n",
    "\n",
    "    group_w_o = pd.concat([group, row_o_i, ], axis=0)\n",
    "    # #####################################################\n",
    "    job_ids_list = group_w_o.job_id_max.tolist()\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Deciding whether to reprocess the job or not\n",
    "    # #####################################################\n",
    "    out_dict_i = magmom_data_dict.get(name_i, None)\n",
    "    # #####################################################\n",
    "    if out_dict_i is None:\n",
    "        run_job = True\n",
    "    else:\n",
    "        run_job = False\n",
    "\n",
    "        job_ids_prev = out_dict_i.get(\"job_ids\", None)\n",
    "        if job_ids_prev is None:\n",
    "            run_job = True\n",
    "        else:\n",
    "            if list(np.sort(job_ids_prev)) != list(np.sort(job_ids_list)):\n",
    "                run_job = True\n",
    "\n",
    "    if redo_all_jobs:\n",
    "        run_job = True\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Testing whether all entries in df_atoms_sorted_ind exist\n",
    "    job_ids_list = list(set(group_w_o.job_id_max.tolist()))\n",
    "\n",
    "    all_pairs_ready_list = []\n",
    "    for job_id_i in job_ids_list:\n",
    "        job_id_in_sorted = False\n",
    "        if job_id_i in df_atoms_sorted_ind.job_id.tolist():\n",
    "            job_id_in_sorted = True\n",
    "        all_pairs_ready_list.append(job_id_in_sorted)\n",
    "    all_pairs_ready = all(all_pairs_ready_list)\n",
    "\n",
    "    if not all_pairs_ready:\n",
    "        print(\"Not all job_ids have row in df_atoms_sorted_ind:\", name_i)\n",
    "        run_job = False\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    if run_job:\n",
    "        # print(\"This is good:\", name_i)\n",
    "\n",
    "        groups_to_process.append(name_i)\n",
    "\n",
    "        # COMMENT THIS OUT TO RUN PARALLEL!!!!!!!\n",
    "\n",
    "#         out_dict_i = process_group_magmom_comp(\n",
    "#             name=name_i,\n",
    "#             group=group_w_o,\n",
    "#             write_atoms_objects=False,\n",
    "#             verbose=False,\n",
    "#             )\n",
    "\n",
    "#         save_magmom_comp_data(name_i, out_dict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running magmom comparison in parallel"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method_wrap(input_dict):\n",
    "    group_w_o = input_dict[\"group_w_o\"]\n",
    "    name_i = input_dict[\"name_i\"]\n",
    "\n",
    "    out_dict_i = process_group_magmom_comp(\n",
    "        name=name_i,\n",
    "        group=group_w_o,\n",
    "        write_atoms_objects=False,\n",
    "        verbose=False,\n",
    "        )\n",
    "\n",
    "    save_magmom_comp_data(name_i, out_dict_i)\n",
    "\n",
    "\n",
    "input_list = []\n",
    "for name_i in groups_to_process:\n",
    "    # #####################################################\n",
    "    group_i = grouped.get_group(name_i)\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "    df_index = df_jobs_anal_i.index.to_frame()\n",
    "\n",
    "    df_index_i = df_index[\n",
    "        (df_index.compenv == compenv_i) & \\\n",
    "        (df_index.slab_id == slab_id_i) & \\\n",
    "        (df_index.ads == \"o\") & \\\n",
    "        [True for i in range(len(df_index))]\n",
    "        ]\n",
    "\n",
    "    row_o_i = df_jobs_anal_i.loc[\n",
    "        df_index_i.index    \n",
    "        ]\n",
    "\n",
    "    group_w_o = pd.concat([group_i, row_o_i, ], axis=0)\n",
    "\n",
    "    input_dict_i = dict(\n",
    "        group_w_o=group_w_o,\n",
    "        name_i=name_i,\n",
    "        )\n",
    "    input_list.append(input_dict_i)\n",
    "\n",
    "variables_dict = dict()\n",
    "traces_all = Pool().map(\n",
    "    partial(\n",
    "        method_wrap,  # METHOD\n",
    "        **variables_dict,  # KWARGS\n",
    "        ),\n",
    "    input_list,\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying which slabs have zero magmoms"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for name_i, row_i in df_jobs_anal_i.iterrows():\n",
    "    # #########################################################\n",
    "    data_dict_i = dict()\n",
    "    # #########################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    ads_i = name_i[2]\n",
    "    active_site_i = name_i[3]\n",
    "    att_num_i = name_i[4]\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    job_id_i = row_i.job_id_max\n",
    "    # #####################################################\n",
    "    \n",
    "    # #####################################################\n",
    "    process_row = False\n",
    "    if name_i in df_atoms_sorted_ind.index:\n",
    "        process_row = True\n",
    "        row_atoms_i = df_atoms_sorted_ind.loc[name_i]\n",
    "        # #################################################\n",
    "        atoms = row_atoms_i.atoms_sorted_good\n",
    "        magmoms_i = row_atoms_i.magmoms_sorted_good\n",
    "        # #################################################\n",
    "\n",
    "            # (compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)]\n",
    "    # row_atoms_i = df_atoms_sorted_ind.loc[\n",
    "    #     (compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)]\n",
    "    \n",
    "    if process_row:\n",
    "        if atoms.calc != None:\n",
    "            magmoms_i = atoms.get_magnetic_moments()\n",
    "        else:\n",
    "            magmoms_i = magmoms_i\n",
    "\n",
    "        sum_magmoms_i = np.sum(magmoms_i)\n",
    "        sum_abs_magmoms = np.sum(np.abs(magmoms_i))\n",
    "\n",
    "        # #########################################################\n",
    "        data_dict_i[\"compenv\"] = compenv_i\n",
    "        data_dict_i[\"slab_id\"] = slab_id_i\n",
    "        data_dict_i[\"ads\"] = ads_i\n",
    "        data_dict_i[\"active_site\"] = active_site_i\n",
    "        data_dict_i[\"att_num\"] = att_num_i\n",
    "        # #########################################################\n",
    "        data_dict_i[\"job_id\"] = job_id_i\n",
    "        data_dict_i[\"sum_magmoms\"] = sum_magmoms_i\n",
    "        data_dict_i[\"sum_abs_magmoms\"] = sum_abs_magmoms\n",
    "        # #########################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #########################################################\n",
    "\n",
    "df_magmoms = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing magmoms by number of atoms"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    slab_id_i = row_i.slab_id\n",
    "    ads_i = row_i.ads\n",
    "    active_site_i = row_i.active_site\n",
    "    att_num_i = row_i.att_num\n",
    "    sum_magmoms_i = row_i.sum_magmoms\n",
    "    sum_abs_magmoms_i = row_i.sum_abs_magmoms\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    name_i = (compenv_i, slab_id_i, ads_i, active_site_i, att_num_i)\n",
    "    row_slab_i = df_init_slabs.loc[name_i]\n",
    "    # #####################################################\n",
    "    num_atoms_i = row_slab_i.num_atoms\n",
    "    # #####################################################\n",
    "\n",
    "    sum_magmoms_pa = sum_magmoms_i / num_atoms_i\n",
    "    sum_abs_magmoms_pa = sum_abs_magmoms_i / num_atoms_i\n",
    "\n",
    "    # #####################################################\n",
    "    row_i[\"sum_magmoms_pa\"] = sum_magmoms_pa\n",
    "    row_i[\"sum_abs_magmoms_pa\"] = sum_abs_magmoms_pa\n",
    "    # #####################################################\n",
    "    return(row_i)\n",
    "    # #####################################################\n",
    "\n",
    "df_magmoms = df_magmoms.apply(\n",
    "    method,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further analysis of magmom comparison (collapse into dataframe)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for name_i in magmom_data_dict.keys():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "    magmom_data_dict[name_i].keys()\n",
    "\n",
    "    from IPython.display import display\n",
    "    df_magmoms_comp_i = magmom_data_dict[name_i][\"df_magmoms_comp\"]\n",
    "    # display(df_magmoms_comp_i)\n",
    "    min_val_i = df_magmoms_comp_i.sum_norm_abs_magmom_diff.min()\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"name\"] = name_i\n",
    "    data_dict_i[\"min_sum_norm_abs_magmom_diff\"] = min_val_i\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "df = pd.DataFrame(data_dict_list)\n",
    "df.min_sum_norm_abs_magmom_diff.min()\n",
    "\n",
    "df = df.sort_values(\"min_sum_norm_abs_magmom_diff\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magmom Comparison"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing *O calcs that don't have an active site\n",
    "# It messes up the groupby\n",
    "df_jobs_i = df_jobs[df_jobs.active_site != \"NaN\"]\n",
    "\n",
    "\n",
    "# Only doing oer_adsorbate calculations\n",
    "df_jobs_i = df_jobs_i[df_jobs_i.job_type == \"oer_adsorbate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\n",
    "    'job_type', 'compenv', 'slab_id',\n",
    "    'bulk_id', 'active_site', 'facet',\n",
    "    ]\n",
    "\n",
    "systems_already_processed = []\n",
    "systems_to_process = []\n",
    "\n",
    "df_list = []\n",
    "grouped = df_jobs_i.groupby(group_cols)\n",
    "iterator = tqdm(grouped, desc=\"1st loop\")\n",
    "for i_cnt, (name_i, group_i) in enumerate(iterator):\n",
    "\n",
    "    # #####################################################\n",
    "    name_dict_i = dict(zip(\n",
    "        group_cols,\n",
    "        list(name_i)))\n",
    "    # #####################################################\n",
    "\n",
    "    row_tmp = group_i[group_i.ads != \"o\"].iloc[0]\n",
    "\n",
    "    group_i_2 = get_other_job_ids_in_set(\n",
    "        row_tmp.name,\n",
    "        df_jobs=df_jobs,\n",
    "        oer_set=True,\n",
    "        only_last_rev=True)\n",
    "\n",
    "    group_i_3 = pd.merge(\n",
    "        group_i_2,\n",
    "        df_jobs_data[[\"final_atoms\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    group_i_3 = group_i_3.dropna(subset=[\"final_atoms\", ])\n",
    "\n",
    "\n",
    "    all_binary_pairs = list(combinations(\n",
    "        group_i_3.index.tolist(), 2))\n",
    "\n",
    "    # mean_displacement_dict = dict()\n",
    "\n",
    "    # data_dict_list = []\n",
    "\n",
    "    good_binary_pairs = []\n",
    "    for pair_j in all_binary_pairs:\n",
    "        # #################################################\n",
    "        job_id_0 = pair_j[0]\n",
    "        job_id_1 = pair_j[1]\n",
    "        # #################################################\n",
    "        row_jobs_0 = df_jobs.loc[job_id_0]\n",
    "        row_jobs_1 = df_jobs.loc[job_id_1]\n",
    "        # #################################################\n",
    "        ads_0 = row_jobs_0.ads\n",
    "        ads_1 = row_jobs_1.ads\n",
    "\n",
    "        if ads_0 != ads_1:\n",
    "            good_binary_pairs.append(pair_j)\n",
    "\n",
    "    # #####################################################\n",
    "    for pair_j in good_binary_pairs:\n",
    "        pair_str_sorted = \"__\".join(list(np.sort(pair_j)))\n",
    "\n",
    "        if pair_str_sorted in df_magmom_drift_prev.pair_str.unique().tolist():\n",
    "            systems_already_processed.append(pair_str_sorted)\n",
    "        else:\n",
    "            systems_to_process.append(pair_str_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMP\n",
    "# print(111 * \"TEMP | \")\n",
    "\n",
    "# systems_to_process = random.sample(\n",
    "#     systems_to_process, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# systems_to_process = [\n",
    "#     'gadahake_36__nenisefu_41',\n",
    "#     'dosapofu_77__vulopuno_60',\n",
    "#     'pebelena_58__rerawubo_36',\n",
    "#     'bibupufo_15__robivufe_82',\n",
    "#     'bibuhuvi_91__niwatiwo_14',\n",
    "#     'bitawobo_02__wewukufo_42',\n",
    "#     'gubipugu_00__himesabi_01',\n",
    "#     'pesipuho_48__puvafuwe_56',\n",
    "#     'dubagito_87__valatumu_96',\n",
    "#     'gotuderi_74__vetabawu_03',\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\n",
    "    'job_type', 'compenv', 'slab_id',\n",
    "    'bulk_id', 'active_site', 'facet',\n",
    "    ]\n",
    "\n",
    "df_list = []\n",
    "grouped = df_jobs_i.groupby(group_cols)\n",
    "iterator = tqdm(grouped, desc=\"1st loop\")\n",
    "\n",
    "for i_cnt, (name_i, group_i) in enumerate(iterator):\n",
    "\n",
    "    # #####################################################\n",
    "    name_dict_i = dict(zip(\n",
    "        group_cols,\n",
    "        list(name_i)))\n",
    "    # #####################################################\n",
    "\n",
    "    row_tmp = group_i[group_i.ads != \"o\"].iloc[0]\n",
    "\n",
    "    group_i_2 = get_other_job_ids_in_set(\n",
    "        row_tmp.name,\n",
    "        df_jobs=df_jobs,\n",
    "        oer_set=True,\n",
    "        only_last_rev=True)\n",
    "\n",
    "    group_i_3 = pd.merge(\n",
    "        group_i_2,\n",
    "        df_jobs_data[[\"final_atoms\"]],\n",
    "        how=\"left\",\n",
    "        left_index=True,\n",
    "        right_index=True)\n",
    "    group_i_3 = group_i_3.dropna(subset=[\"final_atoms\", ])\n",
    "\n",
    "\n",
    "    all_binary_pairs = list(combinations(\n",
    "        group_i_3.index.tolist(), 2))\n",
    "\n",
    "    mean_displacement_dict = dict()\n",
    "\n",
    "    data_dict_list = []\n",
    "\n",
    "\n",
    "    good_binary_pairs = []\n",
    "    for pair_j in all_binary_pairs:\n",
    "        # #################################################\n",
    "        job_id_0 = pair_j[0]\n",
    "        job_id_1 = pair_j[1]\n",
    "        # #################################################\n",
    "        row_jobs_0 = df_jobs.loc[job_id_0]\n",
    "        row_jobs_1 = df_jobs.loc[job_id_1]\n",
    "        # #################################################\n",
    "        ads_0 = row_jobs_0.ads\n",
    "        ads_1 = row_jobs_1.ads\n",
    "\n",
    "        if ads_0 != ads_1:\n",
    "            good_binary_pairs.append(pair_j)\n",
    "\n",
    "\n",
    "    for pair_j in good_binary_pairs:\n",
    "        # print(pair_j)\n",
    "\n",
    "        # #################################################\n",
    "        job_id_0 = pair_j[0]\n",
    "        job_id_1 = pair_j[1]\n",
    "        # #################################################\n",
    "        row_jobs_0 = df_jobs.loc[job_id_0]\n",
    "        row_jobs_1 = df_jobs.loc[job_id_1]\n",
    "        # #################################################\n",
    "        ads_0 = row_jobs_0.ads\n",
    "        ads_1 = row_jobs_1.ads\n",
    "        att_num_0 = row_jobs_0.att_num\n",
    "        att_num_1 = row_jobs_1.att_num\n",
    "        active_site_0 = row_jobs_0.active_site\n",
    "        active_site_1 = row_jobs_1.active_site\n",
    "        # #################################################\n",
    "\n",
    "        # #############################################\n",
    "        row_atoms_0 = df_atoms_sorted_ind_2.loc[job_id_0]\n",
    "        row_atoms_1 = df_atoms_sorted_ind_2.loc[job_id_1]\n",
    "        # #############################################\n",
    "        atoms_0 = row_atoms_0.atoms_sorted_good\n",
    "        atoms_1 = row_atoms_1.atoms_sorted_good\n",
    "        magmoms_sorted_good_0 = row_atoms_0.magmoms_sorted_good\n",
    "        magmoms_sorted_good_1 = row_atoms_1.magmoms_sorted_good\n",
    "        # #############################################\n",
    "\n",
    "        pair_str_sorted = \"__\".join(list(np.sort(pair_j)))\n",
    "\n",
    "\n",
    "        # # TEMP\n",
    "        # systems_to_process = [\n",
    "        #     'fukohesi_27__vunosepi_77',\n",
    "        #     ]\n",
    "\n",
    "        if pair_str_sorted in systems_to_process:\n",
    "\n",
    "            if pair_str_sorted == 'fukohesi_27__vunosepi_77':\n",
    "                print(\"Found fukohesi_27__vunosepi_77\")\n",
    "\n",
    "            # if atoms_0 is None or atoms_1 is None:\n",
    "            if atoms_0 is not None and atoms_1 is not None:\n",
    "\n",
    "                if magmoms_sorted_good_0 is None:\n",
    "                    magmoms_0 = atoms_0.get_magnetic_moments()\n",
    "                else:\n",
    "                    magmoms_0 = magmoms_sorted_good_0\n",
    "\n",
    "                if magmoms_sorted_good_1 is None:\n",
    "                    magmoms_1 = atoms_1.get_magnetic_moments()\n",
    "                else:\n",
    "                    magmoms_1 = magmoms_sorted_good_1\n",
    "\n",
    "\n",
    "\n",
    "                # #############################################\n",
    "                # Running analysis\n",
    "                # #############################################\n",
    "                root_dir = os.path.join(\n",
    "                    os.environ[\"PROJ_irox_oer\"],\n",
    "                    \"dft_workflow/job_analysis/slab_struct_drift\",\n",
    "                    \"out_data/df_match_files\")\n",
    "                if not os.path.exists(root_dir):\n",
    "                    os.makedirs(root_dir)\n",
    "\n",
    "                path_i = os.path.join(\n",
    "                    root_dir, pair_str_sorted + \".pickle\")\n",
    "                if Path(path_i).is_file():\n",
    "                    with open(path_i, \"rb\") as fle:\n",
    "                        df_match = pickle.load(fle)\n",
    "                else:\n",
    "                    print(\"Running:\", pair_j)\n",
    "                    df_match = match_atoms(atoms_0, atoms_1)\n",
    "\n",
    "                    pickle_path = os.path.join(\n",
    "                        root_dir, pair_str_sorted + \".pickle\")\n",
    "                    with open(pickle_path, \"wb\") as fle:\n",
    "                        pickle.dump(df_match, fle)\n",
    "\n",
    "                # df_match_2 = df_match[df_match.closest_distance > 0.000001]\n",
    "                # mean_displacement = df_match_2[\"closest_distance\"].mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # #########################################\n",
    "                name_new_i = (name_i[1], name_i[2], name_i[4], )\n",
    "                if name_new_i not in magmom_data_dict.keys():\n",
    "                    tmp = 42\n",
    "                    # if verbose:\n",
    "                    #     print(\"name_new_i not in magmom_data_dict\")\n",
    "                    #     print(name_new_i)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    magmom_data_i = magmom_data_dict[name_new_i]\n",
    "                    # #####################################\n",
    "                    # df_magmoms_comp = magmom_data_i[\"df_magmoms_comp\"]\n",
    "                    # good_triplet_comb = magmom_data_i[\"good_triplet_comb\"]\n",
    "                    # job_ids = magmom_data_i[\"job_ids\"]\n",
    "                    pair_wise_magmom_comp_data = magmom_data_i[\"pair_wise_magmom_comp_data\"]\n",
    "                    # #####################################\n",
    "                    # pair_data_i = pair_wise_magmom_comp_data[\n",
    "                    #     tuple(np.sort(pair_j))]\n",
    "                    pair_sort = tuple(np.sort(pair_j))\n",
    "                    pair_rev_sort = tuple(np.sort(pair_j)[::-1])\n",
    "                    if pair_sort in list(pair_wise_magmom_comp_data.keys()):\n",
    "                        pair_key = pair_sort\n",
    "                    elif pair_rev_sort in list(pair_wise_magmom_comp_data.keys()):\n",
    "                        pair_key = pair_rev_sort\n",
    "                    else:\n",
    "                        pair_key = None\n",
    "                        print(\"Oh oh issue\")\n",
    "\n",
    "                    pair_data_i = pair_wise_magmom_comp_data[pair_key]\n",
    "                    # #####################################\n",
    "                    delta_magmoms_i = pair_data_i[\"delta_magmoms\"]\n",
    "                    tot_abs_magmom_diff_i = pair_data_i[\"tot_abs_magmom_diff\"]\n",
    "                    norm_abs_magmom_diff_i = pair_data_i[\"norm_abs_magmom_diff\"]\n",
    "                    ads_indices_not_used_i = pair_data_i[\"ads_indices_not_used\"]\n",
    "                    atoms_ave_i = pair_data_i[\"atoms_ave\"]\n",
    "\n",
    "                    delta_magmoms_unsorted_i = pair_data_i[\"delta_magmoms_unsorted\"]\n",
    "                    # #####################################\n",
    "\n",
    "                    row_struct_drift = df_struct_drift.loc[pair_str_sorted]\n",
    "                    octahedra_atoms = row_struct_drift.octahedra_atoms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # #####################################\n",
    "                    # Get the summed absolute magmom atom-diffs\n",
    "                    d_magmoms = []\n",
    "                    for ind_i in list(atoms_0.constraints[0].get_indices()):\n",
    "\n",
    "                        delta_magmom_i = None\n",
    "                        for delta_magmom_j in delta_magmoms_i:\n",
    "                            tmp = 42\n",
    "\n",
    "                            if delta_magmom_j[0] == ind_i:\n",
    "                                delta_magmom_i = delta_magmom_j\n",
    "\n",
    "                        d_magmom_i = delta_magmom_i[1]\n",
    "                        d_magmoms.append(d_magmom_i)\n",
    "\n",
    "                    sum_abs_d_magmoms__constrained = np.sum(\n",
    "                        np.abs(d_magmoms)\n",
    "                        )    \n",
    "\n",
    "                    sum_abs_d_magmoms__constrained_pa = sum_abs_d_magmoms__constrained / len(d_magmoms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # #####################################\n",
    "                    nonoctahedra_indices = []\n",
    "                    for atom_i in atoms_0:\n",
    "                        if not atom_i.index in octahedra_atoms:\n",
    "                            nonoctahedra_indices.append(atom_i.index)\n",
    "\n",
    "                    d_magmoms = []\n",
    "                    for ind_i in list(nonoctahedra_indices):\n",
    "                        delta_magmom_i = None\n",
    "                        for delta_magmom_j in delta_magmoms_i:\n",
    "                            if delta_magmom_j[0] == ind_i:\n",
    "                                delta_magmom_i = delta_magmom_j\n",
    "\n",
    "                        d_magmom_i = delta_magmom_i[1]\n",
    "                        d_magmoms.append(d_magmom_i)\n",
    "\n",
    "                    sum_abs_d_magmoms__nonocta = np.sum(\n",
    "                        np.abs(d_magmoms)\n",
    "                        )\n",
    "                    sum_abs_d_magmoms__nonocta_pa = sum_abs_d_magmoms__nonocta / len(d_magmoms)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # #####################################\n",
    "                    d_magmoms = []\n",
    "                    for ind_i in octahedra_atoms:\n",
    "                        delta_magmom_i = None\n",
    "                        for delta_magmom_j in delta_magmoms_i:\n",
    "                            if delta_magmom_j[0] == ind_i:\n",
    "                                delta_magmom_i = delta_magmom_j\n",
    "\n",
    "                        d_magmom_i = delta_magmom_i[1]\n",
    "                        d_magmoms.append(d_magmom_i)\n",
    "\n",
    "                    sum_abs_d_magmoms__octa = np.sum(\n",
    "                        np.abs(d_magmoms)\n",
    "                        )\n",
    "                    sum_abs_d_magmoms__octa_pa = sum_abs_d_magmoms__octa / len(d_magmoms)\n",
    "\n",
    "\n",
    "                    # #####################################\n",
    "                    data_dict_i = dict()\n",
    "                    # #####################################\n",
    "                    data_dict_i[\"pair_str\"] = pair_str_sorted\n",
    "                    data_dict_i[\"job_id_0\"] = pair_j[0]\n",
    "                    data_dict_i[\"job_id_1\"] = pair_j[1]\n",
    "                    data_dict_i[\"job_ids\"] = list(pair_j)\n",
    "                    data_dict_i[\"ads_0\"] = ads_0\n",
    "                    data_dict_i[\"ads_1\"] = ads_1\n",
    "                    data_dict_i[\"att_num_0\"] = att_num_0\n",
    "                    data_dict_i[\"att_num_1\"] = att_num_1\n",
    "                    data_dict_i[\"sum_abs_d_magmoms__constrained\"] = sum_abs_d_magmoms__constrained\n",
    "                    data_dict_i[\"sum_abs_d_magmoms__constrained_pa\"] = sum_abs_d_magmoms__constrained_pa\n",
    "                    data_dict_i[\"sum_abs_d_magmoms__nonocta\"] = sum_abs_d_magmoms__nonocta\n",
    "                    data_dict_i[\"sum_abs_d_magmoms__nonocta_pa\"] = sum_abs_d_magmoms__nonocta_pa\n",
    "                    data_dict_i[\"sum_abs_d_magmoms__octa\"] = sum_abs_d_magmoms__octa\n",
    "                    data_dict_i[\"sum_abs_d_magmoms__octa_pa\"] = sum_abs_d_magmoms__octa_pa\n",
    "                    # #####################################\n",
    "                    data_dict_list.append(data_dict_i)\n",
    "                    # #####################################\n",
    "\n",
    "                df_tmp = pd.DataFrame(data_dict_list)\n",
    "                df_list.append(df_tmp)\n",
    "\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_magmom_drift = pd.concat(df_list, axis=0)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_magmom_drift_new = pd.concat([\n",
    "    df_magmom_drift_prev,\n",
    "    df_magmom_drift,\n",
    "    ], axis=0)\n",
    "\n",
    "df_magmom_drift_new = df_magmom_drift_new[\n",
    "    ~df_magmom_drift_new.index.duplicated(keep=\"first\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data to pickle"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_analysis/compare_magmoms\")\n",
    "directory = os.path.join(root_dir, \"out_data\")\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# #########################################################\n",
    "path_i = os.path.join(directory, \"df_magmoms.pickle\")\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_magmoms, fle)\n",
    "\n",
    "# #########################################################\n",
    "path_i = os.path.join(directory, \"df_magmom_drift.pickle\")\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_magmom_drift_new, fle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_magmom_drift\n",
    "\n",
    "df_magmom_drift_tmp = get_df_magmom_drift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_magmoms\n",
    "\n",
    "df_magmoms_tmp = get_df_magmoms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"compare_magmoms.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_magmom_drift_new.shape\n",
    "\n",
    "# # (85207, 14)\n",
    "# # (85207, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# group_i_3.shape\n",
    "# group_i_2.shape\n",
    "\n",
    "# group_i_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_atoms_sorted_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_magmom_drift_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# df_magmom_drift_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {}
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
