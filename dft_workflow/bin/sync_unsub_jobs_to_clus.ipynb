{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync recently created, unsubmitted jobs to GDrive\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/f/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# #########################################################\n",
    "from dft_workflow_methods import (\n",
    "    get_path_rel_to_proj,\n",
    "    get_job_paths_info,\n",
    "    get_job_spec_dft_params,\n",
    "    get_job_spec_scheduler_params,\n",
    "    submit_job,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "\n",
    "compenv = os.environ[\"COMPENV\"]\n",
    "\n",
    "if compenv == \"wsl\":\n",
    "    root_dir = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        \"dft_workflow\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sync script if on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compenv != \"wsl\":\n",
    "    bash_file_path = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"scripts/rclone_commands/rclone_proj_repo.sh\")\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [bash_file_path],\n",
    "        stdout=subprocess.PIPE)\n",
    "\n",
    "\n",
    "    out_path = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"dft_workflow/bin\")\n",
    "    bash_script_path = os.path.join(\n",
    "        out_path,\n",
    "        \"out_data/bash_sync_out.sh\")\n",
    "\n",
    "    os.chmod(bash_script_path, 0o777)\n",
    "\n",
    "    result = subprocess.run(\n",
    "        [bash_script_path, ],\n",
    "        shell=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dft_workflow_methods import parse_job_dirs\n",
    "\n",
    "df = parse_job_dirs(root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    new_column_values_dict = {\n",
    "        \"compenv\": None,\n",
    "        }\n",
    "\n",
    "    cand_clusters = []\n",
    "    clusters_list = [\"nersc\", \"sherlock\", \"slac\", ]\n",
    "    for i in row_i.path_job_root.split(\"/\"):\n",
    "        if i in clusters_list:\n",
    "            cand_clusters.append(i)\n",
    "\n",
    "    if len(cand_clusters) == 1:\n",
    "        cluster_i = cand_clusters[0]\n",
    "        new_column_values_dict[\"compenv\"] = cluster_i\n",
    "    else:\n",
    "        if os.environ[\"COMPENV\"] == \"wsl\":\n",
    "            print(\"Couldn't parse cluster from path\")\n",
    "            print(cand_clusters)\n",
    "\n",
    "    # #####################################################\n",
    "    for key, value in new_column_values_dict.items():\n",
    "        row_i[key] = value\n",
    "    return(row_i)\n",
    "\n",
    "df_i = df\n",
    "df_i = df_i.apply(\n",
    "    method,\n",
    "    axis=1)\n",
    "df = df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc_modules.pandas_methods import reorder_df_columns\n",
    "\n",
    "col_order = [\n",
    "    \"compenv\",\n",
    "    \"is_submitted\",\n",
    "    \"att_num\",\n",
    "    \"rev_num\",\n",
    "    \"is_rev_dir\",\n",
    "    \"is_attempt_dir\",\n",
    "\n",
    "    \"path_full\",\n",
    "    \"path_rel_to_proj\",\n",
    "    \"path_job_root\",\n",
    "    \"path_job_root_w_att_rev\",\n",
    "    \"path_job_root_w_att\",\n",
    "    \"gdrive_path\",\n",
    "    ]\n",
    "df = reorder_df_columns(col_order, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing rclone commands to run on the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "sherlock\n",
      "========================================\n",
      "rclone copy  \\\n",
      "$rclone_gdrive_stanford:norskov_research_storage/00_projects/PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/sherlock/mj7wbfb5nt/112/active_site__67/01_attempt/_06 \\\n",
      "$PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/mj7wbfb5nt/112/active_site__67/01_attempt/_06\n",
      "$PROJ_irox_oer/dft_workflow/run_dos_bader/run_o_covered/out_data/dft_jobs/mj7wbfb5nt/112/active_site__67/01_attempt/_06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_i = df[df.is_submitted == False]\n",
    "\n",
    "if compenv == \"wsl\":\n",
    "    bash_comm_files_line_list = []\n",
    "    grouped = df_i.groupby([\"compenv\", ])\n",
    "    for i_cnt, (name, group) in enumerate(grouped):\n",
    "        if verbose:\n",
    "            print(40 * \"=\")\n",
    "            print(name)\n",
    "            print(40 * \"=\")\n",
    "\n",
    "        if i_cnt == 0:\n",
    "            bash_if_statement = 'if [[ \"$COMPENV\" == \"' + name + '\" ]]; then'\n",
    "        else:\n",
    "            bash_if_statement = 'elif [[ \"$COMPENV\" == \"' + name + '\" ]]; then'\n",
    "\n",
    "        bash_comm_files_line_list.append(bash_if_statement)\n",
    "\n",
    "        for name_i, row_i in group.iterrows():\n",
    "            # #########################################################\n",
    "            path_job_root_w_att_rev = row_i.path_job_root_w_att_rev\n",
    "            # #########################################################\n",
    "\n",
    "            # #########################################################\n",
    "            # Constructing path on cluster (remove cluster from path)\n",
    "            clust_path_list = []\n",
    "            for i in path_job_root_w_att_rev.split(\"/\"):\n",
    "                clusters_list = [\"nersc\", \"sherlock\", \"slac\", ]\n",
    "\n",
    "                if i not in clusters_list:\n",
    "                    clust_path_list.append(i)\n",
    "\n",
    "            clust_path = \"/\".join(clust_path_list)\n",
    "\n",
    "            if verbose is False:\n",
    "                quiet_prt = \"--quiet \"\n",
    "            else:\n",
    "                quiet_prt = \"\"\n",
    "\n",
    "            # #########################################################\n",
    "            # Constructing Rclone command\n",
    "            rclone_comm = \"\" + \\\n",
    "                \"rclone copy \" + \\\n",
    "                quiet_prt + \\\n",
    "                \" \\\\\" + \\\n",
    "                \"\\n\" + \\\n",
    "                \"$rclone_gdrive_stanford:norskov_research_storage/00_projects/PROJ_irox_oer/\" + \\\n",
    "                path_job_root_w_att_rev + \\\n",
    "                \" \\\\\" + \\\n",
    "                \"\\n\" + \\\n",
    "                \"$PROJ_irox_oer/\" + \\\n",
    "                clust_path + \\\n",
    "                \"\"\n",
    "\n",
    "\n",
    "            # rclone_comm += \"--quiet\"\n",
    "\n",
    "            if verbose:\n",
    "                print(rclone_comm)\n",
    "    \n",
    "            tmp = \"$PROJ_irox_oer/\" + clust_path\n",
    "            if verbose:\n",
    "                print(tmp)\n",
    "\n",
    "                # \" \\\\\" + \\\n",
    "\n",
    "            # bash_comm_files_line_list.append(\"    \" + rclone_comm)\n",
    "            bash_comm_files_line_list.append(rclone_comm)\n",
    "\n",
    "            # print(rclone_comm)\n",
    "        if verbose:\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    bash_comm_files_line_list.append(\"fi\")\n",
    "\n",
    "    my_list = bash_comm_files_line_list\n",
    "    out_path = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"dft_workflow/bin\")\n",
    "    out_file = os.path.join(\n",
    "        out_path,\n",
    "        \"out_data/bash_sync_out.sh\")\n",
    "    with open(out_file, \"w\") as fle:\n",
    "        for item in my_list:\n",
    "            fle.write(\"%s\\n\" % item)\n",
    "    # os.chmod(out_file, 777)\n",
    "    os.chmod(out_file, 0o777)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rclone local dirs to gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdrive_daemon = os.environ.get(\"GDRIVE_DAEMON\", False)\n",
    "\n",
    "gdrive_daemon = os.environ.get(\"GDRIVE_DAEMON\", False)\n",
    "if gdrive_daemon == \"True\":\n",
    "    gdrive_daemon = True\n",
    "elif gdrive_daemon == \"False\":\n",
    "    gdrive_daemon = False\n",
    "\n",
    "if compenv == \"wsl\" and not gdrive_daemon:\n",
    "    print(\"Syncing new files to GDrive using rclone\")\n",
    "    variables_dict = dict(kwarg_0=\"kwarg_0\")\n",
    "\n",
    "    def method_wrap(\n",
    "        input_dict,\n",
    "        kwarg_0=None,\n",
    "        ):\n",
    "        bash_comm_i = input_dict[\"bash_comm\"]\n",
    "        result = subprocess.run(\n",
    "            bash_comm_i.split(\" \"),\n",
    "            stdout=subprocess.PIPE)\n",
    "\n",
    "    input_list = []\n",
    "    for ind_i, row_i in df_i.iterrows():\n",
    "        path_job_root_w_att_rev = row_i.path_job_root_w_att_rev\n",
    "        if verbose:\n",
    "            print(path_job_root_w_att_rev)\n",
    "\n",
    "        rclone_comm_flat = \"\" + \\\n",
    "            \"rclone copy\" + \\\n",
    "            \" \" + \\\n",
    "            os.environ[\"PROJ_irox_oer_gdrive\"] + \"/\" + \\\n",
    "            path_job_root_w_att_rev + \\\n",
    "            \" \" + \\\n",
    "            os.environ[\"rclone_gdrive_stanford\"] + \":norskov_research_storage/00_projects/PROJ_irox_oer/\" + \\\n",
    "            path_job_root_w_att_rev + \\\n",
    "            \"\"\n",
    "\n",
    "        input_dict_i = dict(bash_comm=rclone_comm_flat)\n",
    "        input_list.append(input_dict_i)\n",
    "\n",
    "\n",
    "    traces_all = Pool().map(\n",
    "        partial(\n",
    "            method_wrap,  # METHOD\n",
    "            **variables_dict,  # KWARGS\n",
    "            ),\n",
    "        input_list,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "Run time: 0.528 min\n",
      "sync_unsub_jobs_to_clus.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"sync_unsub_jobs_to_clus.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df.path_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # root_dir\n",
    "\n",
    "# for i_cnt, row_i in df.iterrows():\n",
    "#     tmp = 42\n",
    "\n",
    "#     path_full_i = row_i.path_full\n",
    "\n",
    "#     if \"dos_bader\" in path_full_i:\n",
    "#         print(i_cnt)\n",
    "#         print(path_full_i)\n",
    "\n",
    "# # i\n",
    "# # row_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# compenv\n",
    "\n",
    "# gdrive_daemon"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
