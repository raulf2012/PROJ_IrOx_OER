{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from ase import io\n",
    "\n",
    "# #########################################################\n",
    "from dft_workflow_methods import get_path_rel_to_proj\n",
    "from dft_workflow_methods import get_job_paths_info\n",
    "from dft_workflow_methods import get_job_spec_dft_params, get_job_spec_scheduler_params\n",
    "from dft_workflow_methods import submit_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "\n",
    "compenv = os.environ[\"COMPENV\"]\n",
    "\n",
    "if compenv == \"wsl\":\n",
    "    root_dir = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        \"dft_workflow/run_slabs/run_o_covered\")    \n",
    "\n",
    "slac_sub_queue = \"suncat3\"  # 'suncat', 'suncat2', 'suncat3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.argv = [\n",
    "#     '/scratch/users/flores12/PROJ_IrOx_OER/dft_workflow/bin/run_unsub_jobs.py',\n",
    "#     'run',\n",
    "#     'frac_of_jobs_to_run=1.',\n",
    "#     'verbose=False',\n",
    "#     ]\n",
    "\n",
    "\n",
    "arg_dict = dict()\n",
    "for i in sys.argv:\n",
    "    if \"=\" in i:\n",
    "        i_split = i.split(\"=\")\n",
    "        arg_dict[i_split[0]] = i_split[1]\n",
    "\n",
    "# #########################################################\n",
    "# Verbosity\n",
    "verbose_i = arg_dict.get(\"verbose\", True)\n",
    "if verbose_i == \"False\":\n",
    "    verbose = False\n",
    "elif verbose_i == \"True\":\n",
    "    verbose = True\n",
    "elif verbose_i is True:\n",
    "    verbose = True\n",
    "\n",
    "# #########################################################\n",
    "# Fraction of jobs to submit\n",
    "frac_of_jobs_to_run_i = arg_dict.get(\"frac_of_jobs_to_run\", 0.)\n",
    "frac_of_jobs_to_run = float(frac_of_jobs_to_run_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir: /media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow/run_slabs/run_o_covered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "last_2_dirs = \"/\".join(root_dir.split(\"/\")[-2:])\n",
    "if last_2_dirs == \"dft_workflow/bin\":\n",
    "    root_dir = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"dft_workflow\")\n",
    "if verbose:\n",
    "    print(\"root_dir:\", root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Usage:\n",
      "  PROJ_irox_oer__comm_jobs_run_unsub_jobs run frac_of_jobs_to_run=0.2\n",
      "\n",
      "\n",
      "sys.argv: ['/home/raulf2012/anaconda3/envs/PROJ_irox_oer/lib/python3.6/site-packages/ipykernel_launcher.py', '-f', '/home/raulf2012/.local/share/jupyter/runtime/kernel-50cd7d57-23a1-46c4-8f22-2c12b3b786c1.json']\n",
      "\n",
      "Run script with 'run' flag\n",
      "run_unsub_jobs run\n"
     ]
    }
   ],
   "source": [
    "if verbose:\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"Usage:\")\n",
    "    print(\"  PROJ_irox_oer__comm_jobs_run_unsub_jobs run frac_of_jobs_to_run=0.2\")\n",
    "    print(\"\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"sys.argv:\", sys.argv)\n",
    "    print(\"\")\n",
    "\n",
    "# if sys.argv[-1] == \"run\":\n",
    "if \"run\" in sys.argv:\n",
    "    run_jobs = True\n",
    "    if verbose:\n",
    "        print(\"running unsubmitted jobs\")\n",
    "else:\n",
    "    run_jobs = False\n",
    "    if verbose:\n",
    "        print(\"Run script with 'run' flag\")\n",
    "        print(\"run_unsub_jobs run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dft_workflow_methods import parse_job_dirs\n",
    "\n",
    "df = parse_job_dirs(root_dir=root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't catch the compenv and set  wall_time_factor correctly\n",
      "\n",
      "\n",
      "Jobs to submit:\n"
     ]
    }
   ],
   "source": [
    "df_not_sub = df[df.is_submitted == False]\n",
    "df_not_sub = df_not_sub[df_not_sub.is_empty == False]\n",
    "\n",
    "\n",
    "out_dict = get_job_spec_scheduler_params(compenv=compenv)\n",
    "wall_time_factor = out_dict[\"wall_time_factor\"]\n",
    "\n",
    "if verbose:\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(\"Jobs to submit:\")\n",
    "    for path_i in df_not_sub.path_job_root_w_att_rev.tolist():\n",
    "        print(path_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of jobs to submit: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"Number of jobs to submit:\", df_not_sub.shape[0])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_cnt, row_i in df_not_sub.iterrows():\n",
    "    # #######################################\n",
    "    path_i = row_i.path_full\n",
    "    path_job_root_w_att_rev = row_i.path_job_root_w_att_rev\n",
    "    # #######################################\n",
    "\n",
    "    atoms_path_i = os.path.join(path_i, \"init.traj\")\n",
    "    atoms = io.read(atoms_path_i)\n",
    "    num_atoms = atoms.get_global_number_of_atoms()\n",
    "\n",
    "    # #####################################################\n",
    "    if random.random() <= frac_of_jobs_to_run:\n",
    "        run_job_i = True\n",
    "    else:\n",
    "        run_job_i = False\n",
    "\n",
    "    if run_jobs and run_job_i:\n",
    "\n",
    "        print(40 * \"*\")\n",
    "        print(\"Submitting:\", path_job_root_w_att_rev)\n",
    "        submit_job(\n",
    "            path_i=path_i,\n",
    "            num_atoms=num_atoms,\n",
    "            wall_time_factor=wall_time_factor,\n",
    "            queue=slac_sub_queue,\n",
    "            )\n",
    "\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(\"IIJIDFJIJISDF(SD*(DF(S(JS(DF)(SIDFD)))))\")\n",
    "# print(\"\")\n",
    "\n",
    "# tmp = [print(i) for i in df.path_rel_to_proj.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# last_2_dirs = \"/\".join(root_dir.split(\"/\")[-2:])\n",
    "# if last_2_dirs == \"dft_workflow/bin\":\n",
    "#     root_dir = os.path.join(\n",
    "#         os.environ[\"PROJ_irox_oer\"],\n",
    "#         \"dft_workflow\")\n",
    "\n",
    "# if verbose:\n",
    "#     print(\"root_dir:\", root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# frac_of_jobs_to_run = 1.\n",
    "# for i in sys.argv:\n",
    "#     # i = \"frac_of_jobs_to_run=0.2\"\n",
    "#     if \"frac_of_jobs_to_run\" in i:\n",
    "#         frac_of_jobs_to_run = i.split(\"=\")[-1]\n",
    "#         frac_of_jobs_to_run = float(frac_of_jobs_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# print(\"\")\n",
    "# print(\"frac_of_jobs_to_run:\", frac_of_jobs_to_run)\n",
    "# print(\"run_jobs:\", run_jobs)\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
