{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Job Directories\n",
    "---\n",
    "\n",
    "Meant to be run within one of the computer clusters on which jobs are run (Nersc, Sherlock, Slac). Will `os.walk` through `jobs_root_dir` and cobble together all the job directories and then upload the data to Dropbox.\n",
    "This script is meant primarily to get simple job information, for more detailed info run the `parse_job_data.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/job_processing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import time; ti = time.time()\n",
    "import shutil\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "# #########################################################\n",
    "from misc_modules.pandas_methods import reorder_df_columns\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    is_attempt_dir,\n",
    "    is_rev_dir,\n",
    "    get_job_paths_info,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "compenv = os.environ.get(\"COMPENV\", \"wsl\")\n",
    "\n",
    "if compenv == \"wsl\":\n",
    "    jobs_root_dir = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        \"dft_workflow\")\n",
    "elif compenv == \"nersc\" or compenv == \"sherlock\" or compenv == \"slac\":\n",
    "    jobs_root_dir = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"dft_workflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering prelim info, get all base job dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_rel_to_proj(full_path):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    #| - get_path_rel_to_proj\n",
    "    subdir = full_path\n",
    "\n",
    "    PROJ_dir = os.environ[\"PROJ_irox_oer\"]\n",
    "\n",
    "    search_term = PROJ_dir.split(\"/\")[-1]\n",
    "    ind_tmp = subdir.find(search_term)\n",
    "    if ind_tmp == -1:\n",
    "        search_term = \"PROJ_irox_oer\"\n",
    "        ind_tmp = subdir.find(search_term)\n",
    "\n",
    "    path_rel_to_proj = subdir[ind_tmp:]\n",
    "    path_rel_to_proj = \"/\".join(path_rel_to_proj.split(\"/\")[1:])\n",
    "\n",
    "    return(path_rel_to_proj)\n",
    "    #__|"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for job dirs from the following dir:\n",
      "/media/raulf2012/research_backup/PROJ_irox_oer_gdrive/dft_workflow\n"
     ]
    }
   ],
   "source": [
    "if verbose:\n",
    "    print(\n",
    "        \"Scanning for job dirs from the following dir:\",\n",
    "        \"\\n\",\n",
    "        jobs_root_dir,\n",
    "        sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial scan of root dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for subdir, dirs, files in os.walk(jobs_root_dir):\n",
    "\n",
    "    data_dict_i = dict()\n",
    "    data_dict_i[\"path_full\"] = subdir\n",
    "\n",
    "    last_dir = jobs_root_dir.split(\"/\")[-1]\n",
    "    path_i = os.path.join(last_dir, subdir[len(jobs_root_dir) + 1:])\n",
    "    # path_i = subdir[len(jobs_root_dir) + 1:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if \"dft_jobs\" not in subdir:\n",
    "        continue\n",
    "    if \".old\" in subdir:\n",
    "        continue\n",
    "    if path_i == \"\":\n",
    "        continue\n",
    "\n",
    "\n",
    "    # # TEMP\n",
    "    # # print(\"TEMP\")\n",
    "    # # frag_i = \"slac/mwmg9p7s6o/11-20\"\n",
    "    # # frag_i = \"slac/mwmg9p7s6o/11-20/bare/active_site__26/01_attempt\"\n",
    "    # frag_i = \"run_dos_bader\"\n",
    "    # if frag_i not in subdir:\n",
    "    #     # break\n",
    "    #     continue\n",
    "\n",
    "    #     print(1 * \"Got through | \")\n",
    "    #     print(subdir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # if verbose:\n",
    "    #     print(path_i)\n",
    "\n",
    "\n",
    "    path_rel_to_proj = get_path_rel_to_proj(subdir)\n",
    "    out_dict = get_job_paths_info(path_i)\n",
    "\n",
    "    # Only add job directory if it's been submitted\n",
    "    my_file = Path(os.path.join(subdir, \".SUBMITTED\"))\n",
    "    submitted = False\n",
    "    if my_file.is_file():\n",
    "        submitted = True\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i.update(out_dict)\n",
    "    data_dict_i[\"path_rel_to_proj\"] = path_rel_to_proj\n",
    "    data_dict_i[\"submitted\"] = submitted\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "if len(data_dict_list) == 0:\n",
    "    df_cols = [\n",
    "        \"path_full\",\n",
    "        \"path_rel_to_proj\",\n",
    "        \"path_job_root\",\n",
    "        \"path_job_root_w_att_rev\",\n",
    "        \"att_num\",\n",
    "        \"rev_num\",\n",
    "        \"is_rev_dir\",\n",
    "        \"is_attempt_dir\",\n",
    "        \"path_job_root_w_att\",\n",
    "        \"gdrive_path\",\n",
    "        \"submitted\",\n",
    "        ]\n",
    "\n",
    "    df = pd.DataFrame(columns=df_cols)\n",
    "else:\n",
    "    df = pd.DataFrame(data_dict_list)\n",
    "    df = df[~df.path_job_root_w_att_rev.isna()]\n",
    "    df = df.drop_duplicates(subset=[\"path_job_root_w_att_rev\", ], keep=\"first\")\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "assert df.index.is_unique, \"Index must be unique here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get facet and bulk from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_facet_bulk_id(row_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    new_column_values_dict = {\n",
    "        \"bulk_id\": None,\n",
    "        \"facet\": None,\n",
    "        }\n",
    "\n",
    "    # #####################################################\n",
    "    path_job_root = row_i.path_job_root\n",
    "    # #####################################################\n",
    "\n",
    "    # print(path_job_root)\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # Check if the job is a *O calc (different than other adsorbates)\n",
    "    if \"run_o_covered\" in path_job_root or \"run_o_covered\" in jobs_root_dir:\n",
    "\n",
    "        path_split = path_job_root.split(\"/\")\n",
    "\n",
    "        ads_i = \"o\"\n",
    "        if \"active_site__\" in path_job_root:\n",
    "\n",
    "            facet_i = path_split[-2]\n",
    "            bulk_id_i = path_split[-3]\n",
    "\n",
    "            active_site_parsed = False\n",
    "            for i in path_split:\n",
    "                if \"active_site__\" in i:\n",
    "                    active_site_path_seg = i.split(\"_\")\n",
    "                    active_site_i = active_site_path_seg[-1]\n",
    "                    active_site_i = int(active_site_i)\n",
    "                    active_site_parsed = True\n",
    "            if not active_site_parsed:\n",
    "                print(\"PROBLEM | Couldn't parse active site for following dir:\")\n",
    "                print(path_job_root)\n",
    "\n",
    "        else:\n",
    "            # path_split = path_job_root.split(\"/\")\n",
    "\n",
    "            facet_i = path_split[-1]\n",
    "            bulk_id_i = path_split[-2]\n",
    "            # active_site_i = None\n",
    "            active_site_i = np.nan\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    elif \"run_bare_oh_covered\" in path_job_root or \"run_bare_oh_covered\" in jobs_root_dir:\n",
    "        path_split = path_job_root.split(\"/\")\n",
    "\n",
    "        if \"/bare/\" in path_job_root:\n",
    "            ads_i = \"bare\"\n",
    "        elif \"/oh/\" in path_job_root:\n",
    "            ads_i = \"oh\"\n",
    "        else:\n",
    "            print(\"Couldn't parse the adsorbate from here\")\n",
    "            ads_i = None\n",
    "\n",
    "        active_site_parsed = False\n",
    "        for i in path_split:\n",
    "            if \"active_site__\" in i:\n",
    "                active_site_path_seg = i.split(\"_\")\n",
    "                active_site_i = active_site_path_seg[-1]\n",
    "                active_site_i = int(active_site_i)\n",
    "                active_site_parsed = True\n",
    "        if not active_site_parsed:\n",
    "            print(\"PROBLEM | Couldn't parse active site for following dir:\")\n",
    "            print(path_job_root)\n",
    "\n",
    "        facet_i = path_split[-3]\n",
    "        bulk_id_i = path_split[-4]\n",
    "        # ads_i = \"bare\"\n",
    "\n",
    "        # Check that the parsed facet makes sense\n",
    "        char_list_new = []\n",
    "        for char_i in facet_i:\n",
    "            if char_i != \"-\":\n",
    "                char_list_new.append(char_i)\n",
    "        facet_new_i = \"\".join(char_list_new)\n",
    "\n",
    "        # all_facet_chars_are_numeric = all([i.isnumeric() for i in facet_i])\n",
    "        # all_facet_chars_are_numeric = all([i.isnumeric() for i in facet_i])\n",
    "        all_facet_chars_are_numeric = all([i.isnumeric() for i in facet_new_i])\n",
    "        mess_i = \"All characters of parsed facet must be numeric\"\n",
    "        assert all_facet_chars_are_numeric, mess_i\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    elif \"run_oh_covered\" in path_job_root or \"run_oh_covered\" in jobs_root_dir:\n",
    "        path_split = path_job_root.split(\"/\")\n",
    "\n",
    "        if \"/bare/\" in path_job_root:\n",
    "            ads_i = \"bare\"\n",
    "        elif \"/oh/\" in path_job_root:\n",
    "            ads_i = \"oh\"\n",
    "        else:\n",
    "            print(\"Couldn't parse the adsorbate from here\")\n",
    "            ads_i = None\n",
    "\n",
    "        active_site_parsed = False\n",
    "        for i in path_split:\n",
    "            if \"active_site__\" in i:\n",
    "                active_site_path_seg = i.split(\"_\")\n",
    "                active_site_i = active_site_path_seg[-1]\n",
    "                active_site_i = int(active_site_i)\n",
    "                active_site_parsed = True\n",
    "        if not active_site_parsed:\n",
    "            print(\"PROBLEM | Couldn't parse active site for following dir:\")\n",
    "            print(path_job_root)\n",
    "\n",
    "        facet_i = path_split[-3]\n",
    "        bulk_id_i = path_split[-4]\n",
    "\n",
    "        # Check that the parsed facet makes sense\n",
    "        char_list_new = []\n",
    "        for char_i in facet_i:\n",
    "            if char_i != \"-\":\n",
    "                char_list_new.append(char_i)\n",
    "        facet_new_i = \"\".join(char_list_new)\n",
    "\n",
    "        # all_facet_chars_are_numeric = all([i.isnumeric() for i in facet_i])\n",
    "        all_facet_chars_are_numeric = all([i.isnumeric() for i in facet_new_i])\n",
    "        mess_i = \"All characters of parsed facet must be numeric\"\n",
    "        assert all_facet_chars_are_numeric, mess_i\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    else:\n",
    "        print(\"Couldn't figure out what to do here\")\n",
    "        print(path_job_root)\n",
    "\n",
    "        facet_i = None\n",
    "        bulk_id_i = None\n",
    "        ads_i = None\n",
    "\n",
    "        pass\n",
    "\n",
    "    # #####################################################\n",
    "    new_column_values_dict[\"facet\"] = facet_i\n",
    "    new_column_values_dict[\"bulk_id\"] = bulk_id_i\n",
    "    new_column_values_dict[\"ads\"] = ads_i\n",
    "    new_column_values_dict[\"active_site\"] = active_site_i\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    for key, value in new_column_values_dict.items():\n",
    "        row_i[key] = value\n",
    "\n",
    "    return(row_i)\n",
    "\n",
    "df = df.apply(\n",
    "    get_facet_bulk_id,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.att_num = df.att_num.astype(int)\n",
    "df.rev_num = df.rev_num.astype(int)\n",
    "\n",
    "# df[\"compenv\"] = compenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get job type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_type(row_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    new_column_values_dict = {\n",
    "        \"job_type\":  None,\n",
    "        }\n",
    "\n",
    "    # #####################################################\n",
    "    path_job_root = row_i.path_job_root\n",
    "    # #####################################################\n",
    "\n",
    "    # print(path_job_root)\n",
    "\n",
    "    if \"run_dos_bader\" in path_job_root:\n",
    "        job_type_i = \"dos_bader\"\n",
    "    elif \"dft_workflow/run_slabs\" in path_job_root:\n",
    "        job_type_i = \"oer_adsorbate\"\n",
    "        \n",
    "\n",
    "    # #####################################################\n",
    "    new_column_values_dict[\"job_type\"] = job_type_i\n",
    "    # #####################################################\n",
    "    for key, value in new_column_values_dict.items():\n",
    "        row_i[key] = value\n",
    "    # #####################################################\n",
    "    return(row_i)\n",
    "    # #####################################################\n",
    "\n",
    "df = df.apply(\n",
    "    get_job_type,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_order = [\n",
    "    \"job_type\",\n",
    "    \"compenv\",\n",
    "\n",
    "    \"bulk_id\",\n",
    "    \"facet\",\n",
    "    \"ads\",\n",
    "\n",
    "    \"submitted\",\n",
    "\n",
    "    \"att_num\",\n",
    "    \"rev_num\",\n",
    "    \"num_revs\",\n",
    "\n",
    "    \"is_rev_dir\",\n",
    "    \"is_attempt_dir\",\n",
    "\n",
    "    \"path_job_root\",\n",
    "    \"path_job_root_w_att_rev\",\n",
    "    ]\n",
    "\n",
    "df = reorder_df_columns(new_col_order, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data and uploading to Dropbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"dft_workflow/job_processing\",\n",
    "    \"out_data\")\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "file_name_i = \"df_jobs_base_\" + compenv + \".pickle\"\n",
    "file_path_i = os.path.join(directory, file_name_i)\n",
    "with open(file_path_i, \"wb\") as fle:\n",
    "    pickle.dump(df, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash_comm: rclone copyto /home/raulf2012/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/job_processing/out_data/df_jobs_base_wsl.pickle raul_dropbox:01_norskov/00_git_repos/PROJ_IrOx_OER/dft_workflow/job_processing/out_data/df_jobs_base_wsl.pickle\n"
     ]
    }
   ],
   "source": [
    "db_path = os.path.join(\n",
    "    \"01_norskov/00_git_repos/PROJ_IrOx_OER\",\n",
    "    \"dft_workflow/job_processing/out_data\" ,\n",
    "    file_name_i)\n",
    "\n",
    "rclone_remote = os.environ.get(\"rclone_dropbox\", \"raul_dropbox\")\n",
    "bash_comm = \"rclone copyto \" + file_path_i + \" \" + rclone_remote + \":\" + db_path\n",
    "if verbose:\n",
    "    print(\"bash_comm:\", bash_comm)\n",
    "\n",
    "if compenv != \"wsl\":\n",
    "    os.system(bash_comm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n",
      "All done!\n",
      "Run time: 0.629 min\n",
      "parse_job_dirs.ipynb\n",
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n"
     ]
    }
   ],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"parse_job_dirs.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DEPRECATED | Moved to fix_gdrive_conflicts.ipynb\n",
    "\n",
    "### Removing paths that have the GDrive duplicate syntax in them ' (1)'\n",
    "\n",
    "# for ind_i, row_i in df.iterrows():\n",
    "#     path_full_i = row_i.path_full\n",
    "\n",
    "#     if \" (\" in path_full_i:\n",
    "#         print(\n",
    "#             path_full_i,\n",
    "#             sep=\"\")\n",
    "\n",
    "#         # #################################################\n",
    "#         found_wrong_level = False\n",
    "#         path_level_list = []\n",
    "#         for i in path_full_i.split(\"/\"):\n",
    "#             if not found_wrong_level:\n",
    "#                 path_level_list.append(i)\n",
    "#             if \" (\" in i:\n",
    "#                 found_wrong_level = True\n",
    "#         path_upto_error = \"/\".join(path_level_list)\n",
    "\n",
    "#         my_file = Path(path_full_i)\n",
    "#         if my_file.is_dir():\n",
    "#             size_i = os.path.getsize(path_full_i)\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "\n",
    "#         # If it's a small file size then it probably just has the init files and we're good to delete the dir\n",
    "#         # Seems that all files are 512 bytes in size (I think it's bytes)\n",
    "#         if size_i < 550:\n",
    "#             my_file = Path(path_upto_error)\n",
    "#             if my_file.is_dir():\n",
    "#                 print(\"Removing dir:\", path_upto_error)\n",
    "#                 # shutil.rmtree(path_upto_error)\n",
    "#         else:\n",
    "#             print(100 * \"Issue | \")\n",
    "#             print(path_full_i)\n",
    "#             print(path_full_i)\n",
    "#             print(path_full_i)\n",
    "#             print(path_full_i)\n",
    "#             print(path_full_i)\n",
    "\n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Removing files with ' (' in name (GDrive duplicates)\n",
    "\n",
    "# for subdir, dirs, files in os.walk(jobs_root_dir):\n",
    "#     for file_i in files:\n",
    "#         if \" (\" in file_i:\n",
    "#             file_path_i = os.path.join(subdir, file_i)\n",
    "\n",
    "#             print(\n",
    "#                 \"Removing:\",\n",
    "#                 file_path_i)\n",
    "#             # os.remove(file_path_i)\n",
    "\n",
    "# # os.path.join(subdir, file_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# df[df.job_type == \"dos_bader\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# assert False"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
