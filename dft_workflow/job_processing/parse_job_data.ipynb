{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parses Job Data\n",
    "---\n",
    "\n",
    "Applied job analaysis scripts to job directories and compiles.\n",
    "\n",
    "This script when rerunning all jobs took 82.928 min on Wed Feb  3 16:27:22 PST 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "# #########################################################\n",
    "from misc_modules.pandas_methods import reorder_df_columns\n",
    "from vasp.vasp_methods import read_incar, get_irr_kpts_from_outcar\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs_paths,\n",
    "    get_df_jobs_data_clusters,\n",
    "    )\n",
    "from methods import get_df_jobs_data\n",
    "\n",
    "from local_methods import (\n",
    "    parse_job_err,\n",
    "    parse_finished_file,\n",
    "    parse_job_state,\n",
    "    is_job_submitted,\n",
    "    get_isif_from_incar,\n",
    "    get_number_of_ionic_steps,\n",
    "    analyze_oszicar,\n",
    "    read_data_pickle,\n",
    "    get_final_atoms,\n",
    "    get_init_atoms,\n",
    "    get_magmoms_from_job,\n",
    "    get_ads_from_path,\n",
    "    )\n",
    "from local_methods import is_job_started\n",
    "from local_methods import get_forces_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun job parsing on all existing jobs, needed if job parsing methods are updated\n",
    "rerun_all_jobs = False\n",
    "# rerun_all_jobs = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compenv = os.environ.get(\"COMPENV\", None)\n",
    "if compenv != \"wsl\":\n",
    "    rerun_all_jobs = True\n",
    "\n",
    "if rerun_all_jobs:\n",
    "    print(\"rerun_all_jobs=True\")\n",
    "    # print(\"Remember to turn off this flag under normal operation\")\n",
    "\n",
    "PROJ_irox_oer_gdrive = os.environ[\"PROJ_irox_oer_gdrive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_jobs_paths = get_df_jobs_paths()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs = get_df_jobs(exclude_wsl_paths=True)\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_data_clusters = get_df_jobs_data_clusters()\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_data_old = get_df_jobs_data(exclude_wsl_paths=True, drop_cols=False)\n",
    "\n",
    "# #########################################################\n",
    "# Checking if in local env\n",
    "if compenv == \"wsl\":\n",
    "    df_jobs_i = df_jobs\n",
    "else:\n",
    "    df_jobs_i = df_jobs[df_jobs.compenv == compenv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting job state loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = []\n",
    "for job_id_i, row_i in df_jobs_i.iterrows():\n",
    "    data_dict_i = dict()\n",
    "\n",
    "    # #####################################################\n",
    "    compenv_i = row_i.compenv\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    job_id = row_i.job_id\n",
    "    att_num = row_i.att_num\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_paths_i = df_jobs_paths[\n",
    "        df_jobs_paths.compenv == compenv_i]\n",
    "    row_jobs_paths_i = df_jobs_paths_i.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    gdrive_path = row_jobs_paths_i.gdrive_path\n",
    "    path_job_root_w_att_rev = row_jobs_paths_i.path_job_root_w_att_rev\n",
    "    # #####################################################\n",
    "\n",
    "    data_dict_i[\"job_id\"] = job_id\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"att_num\"] = att_num\n",
    "\n",
    "    if compenv == \"wsl\":\n",
    "        path_full_i = os.path.join(\n",
    "            PROJ_irox_oer_gdrive,\n",
    "            gdrive_path)\n",
    "    else:\n",
    "        path_full_i = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer\"],\n",
    "            path_job_root_w_att_rev)\n",
    "\n",
    "    # #################################################\n",
    "    job_state_i = parse_job_state(path_full_i)\n",
    "    data_dict_i.update(job_state_i)\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #################################################\n",
    "\n",
    "\n",
    "df_jobs_state = pd.DataFrame(data_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dir_data_dict(path_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # ########################################################\n",
    "    import os\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "\n",
    "    data_path = os.path.join(\n",
    "        path_i, \"data_dict.json\")\n",
    "    my_file = Path(data_path)\n",
    "    if my_file.is_file():\n",
    "        with open(data_path, \"r\") as fle:\n",
    "            data_dict_i = json.load(fle)\n",
    "    else:\n",
    "        data_dict_i = dict()\n",
    "    # ########################################################\n",
    "    \n",
    "    return(data_dict_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the main loop on parse_job_data.py\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting the main loop on parse_job_data.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMP\n",
    "# print(222 * \"TEMP | \")\n",
    "\n",
    "# df_jobs_i = df_jobs_i.loc[[\n",
    "#     # \"seratado_15\",\n",
    "#     # \"gegupagu_35\",\n",
    "#     \"pulefevo_10\",\n",
    "#     ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_jobs_i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1c84637c2bab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrows_from_prev_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_dict_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mjob_id_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_jobs_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_id_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_jobs_i' is not defined"
     ]
    }
   ],
   "source": [
    "rows_from_clusters = []\n",
    "rows_from_prev_df = []\n",
    "data_dict_list = []\n",
    "for job_id_i, row_i in df_jobs_i.iterrows():\n",
    "    # print(job_id_i)\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    bulk_id = row_i.bulk_id\n",
    "    slab_id = row_i.slab_id\n",
    "    job_id = row_i.job_id\n",
    "    facet = row_i.facet\n",
    "    ads = row_i.ads\n",
    "    compenv_i = row_i.compenv\n",
    "    active_site_i = row_i.active_site\n",
    "    att_num = row_i.att_num\n",
    "    rev_num = row_i.rev_num\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_jobs_paths_i = df_jobs_paths.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    path_job_root_w_att_rev = row_jobs_paths_i.path_job_root_w_att_rev\n",
    "    gdrive_path_i = row_jobs_paths_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    df_jobs_data_clusters_i = df_jobs_data_clusters[\n",
    "        df_jobs_data_clusters.compenv == compenv_i]\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "\n",
    "    # gdrive_path_i = df_jobs_paths.loc[job_id_i].gdrive_path\n",
    "\n",
    "    incar_path = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        gdrive_path_i,\n",
    "        \"INCAR\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # Deciding to run job or grabbing it from elsewhere\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    run_job_i = True\n",
    "    # job_grabbed_from_clusters = False\n",
    "    job_grabbed_from_prev_df = False\n",
    "\n",
    "    if rerun_all_jobs:\n",
    "        run_job_i = True\n",
    "    else:\n",
    "\n",
    "        if job_id_i in df_jobs_data_clusters_i.index:\n",
    "\n",
    "            run_job_i = False\n",
    "            job_grabbed_from_clusters = True\n",
    "\n",
    "            # #############################################\n",
    "            row_cluster_i = df_jobs_data_clusters_i.loc[job_id_i]\n",
    "            # #############################################\n",
    "            incar_params_i = row_cluster_i.incar_params\n",
    "            completed_i = row_cluster_i.completed\n",
    "            # #############################################\n",
    "\n",
    "            finished_path = os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "                gdrive_path_i, \".FINISHED\")\n",
    "\n",
    "            job_finished = False\n",
    "            my_file = Path(finished_path)\n",
    "            if my_file.is_file():\n",
    "                job_finished = True\n",
    "                # print(\"Finished is there\")\n",
    "\n",
    "            # If these conditions just rerun job\n",
    "            if not completed_i and job_finished:\n",
    "                run_job_i = True    \n",
    "                job_grabbed_from_clusters = False\n",
    "            elif incar_params_i is None:\n",
    "                run_job_i = True    \n",
    "                job_grabbed_from_clusters = False\n",
    "\n",
    "\n",
    "            if not run_job_i and job_grabbed_from_clusters:\n",
    "                if verbose:\n",
    "                    print(job_id_i, \"Grabbing from df_jobs_data_clusters\")\n",
    "                rows_from_clusters.append(row_cluster_i)\n",
    "\n",
    "        # if not job_grabbed_from_clusters and job_id_i in df_jobs_data_old.index:\n",
    "        elif job_id_i in df_jobs_data_old.index:\n",
    "\n",
    "            run_job_i = False\n",
    "            # job_grabbed_from_clusters = True\n",
    "\n",
    "            row_from_prev_df = df_jobs_data_old.loc[job_id_i]\n",
    "\n",
    "\n",
    "            # #############################################\n",
    "            # If the prev INCAR params is None but the incar file is there then rerun\n",
    "            incar_params_i = row_from_prev_df.incar_params\n",
    "            # incar_file_and_df_dont_match = False\n",
    "            data_in_df_and_dir_dont_match = False\n",
    "            if incar_params_i is None:\n",
    "                my_file = Path(incar_path)\n",
    "                if my_file.is_file():\n",
    "                    data_in_df_and_dir_dont_match = True\n",
    "                    # incar_file_and_df_dont_match = True\n",
    "                    run_job_i = True\n",
    "\n",
    "            # #############################################\n",
    "            # If row is shown as not completed but the .FINISHED.new file is there then rerun\n",
    "            completed_i = row_from_prev_df.completed\n",
    "            finished_file_path_i = os.path.join(\n",
    "                os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "                gdrive_path_i,\n",
    "                \".FINISHED.new\")\n",
    "            my_file = Path(finished_file_path_i)\n",
    "\n",
    "            # print(\"WHAT SDIFJDSIF SDFJ\")\n",
    "            # print(\"completed_i:\", completed_i)\n",
    "            # print(\"my_file.is_file():\", my_file.is_file())\n",
    "\n",
    "            # if completed_i is False and my_file.is_file():\n",
    "            if not completed_i and my_file.is_file():\n",
    "                # print(\"ISJHFISDIJFIJSDI\")\n",
    "                data_in_df_and_dir_dont_match = True\n",
    "                run_job_i = True\n",
    "\n",
    "            # #############################################\n",
    "\n",
    "            # if not incar_file_and_df_dont_match:\n",
    "            if not data_in_df_and_dir_dont_match:\n",
    "                if verbose:\n",
    "                    print(job_id_i, \"Grabbing from prev df_jobs_data\")\n",
    "                rows_from_prev_df.append(row_from_prev_df)\n",
    "\n",
    "\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(job_id_i, \"Failed to grab job data from elsewhere\")\n",
    "\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # Deciding to run job or grabbing it from elsewhere\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if compenv == \"wsl\":\n",
    "        path_full_i = os.path.join(\n",
    "            PROJ_irox_oer_gdrive,\n",
    "            gdrive_path_i)\n",
    "    else:\n",
    "        path_full_i = os.path.join(\n",
    "            os.environ[\"PROJ_irox_oer\"],\n",
    "            path_job_root_w_att_rev)\n",
    "\n",
    "    path_exists = False\n",
    "    my_file = Path(path_full_i)\n",
    "    if my_file.is_dir():\n",
    "        path_exists = True        \n",
    "\n",
    "    if run_job_i and path_exists:\n",
    "\n",
    "        print(path_full_i)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"running job\")\n",
    "\n",
    "\n",
    "        # print(\"isjfdsi 000000 - - - - \")\n",
    "        # #################################################\n",
    "        job_err_out_i = parse_job_err(path_full_i, compenv=compenv_i)\n",
    "        # print(\"isjfdsi 11\")\n",
    "        finished_i = parse_finished_file(path_full_i)\n",
    "        job_state_i = parse_job_state(path_full_i)\n",
    "        job_submitted_i = is_job_submitted(path_full_i)\n",
    "        job_started_i = is_job_started(path_full_i)\n",
    "        # print(\"isjfdsi 222\")\n",
    "        isif_i = get_isif_from_incar(path_full_i)\n",
    "        num_steps = get_number_of_ionic_steps(path_full_i)\n",
    "        oszicar_anal = analyze_oszicar(path_full_i)\n",
    "        incar_params = read_incar(path_full_i, verbose=verbose)\n",
    "        irr_kpts = get_irr_kpts_from_outcar(path_full_i)\n",
    "        pickle_data = read_data_pickle(path_full_i)\n",
    "        # print(\"isjfdsi 333\")\n",
    "        init_atoms = get_init_atoms(path_full_i)\n",
    "        final_atoms = get_final_atoms(path_full_i)\n",
    "        magmoms_i = get_magmoms_from_job(path_full_i)\n",
    "        data_dict_out_i = read_dir_data_dict(path_full_i)\n",
    "\n",
    "        forces_dict_out_i = get_forces_info(path_full_i)\n",
    "        # #################################################\n",
    "\n",
    "\n",
    "        # #################################################\n",
    "        data_dict_i.update(job_err_out_i)\n",
    "        data_dict_i.update(finished_i)\n",
    "        data_dict_i.update(job_state_i)\n",
    "        data_dict_i.update(job_submitted_i)\n",
    "        data_dict_i.update(job_started_i)\n",
    "        data_dict_i.update(isif_i)\n",
    "        data_dict_i.update(num_steps)\n",
    "        data_dict_i.update(oszicar_anal)\n",
    "        data_dict_i.update(pickle_data)\n",
    "        data_dict_i.update(data_dict_out_i)\n",
    "        data_dict_i.update(forces_dict_out_i)\n",
    "        # #################################################\n",
    "        data_dict_i[\"facet\"] = facet\n",
    "        data_dict_i[\"bulk_id\"] = bulk_id\n",
    "        data_dict_i[\"slab_id\"] = slab_id\n",
    "        data_dict_i[\"ads\"] = ads\n",
    "        data_dict_i[\"job_id\"] = job_id\n",
    "        data_dict_i[\"compenv\"] = compenv_i\n",
    "        data_dict_i[\"active_site\"] = active_site_i\n",
    "        data_dict_i[\"att_num\"] = att_num\n",
    "        data_dict_i[\"rev_num\"] = rev_num\n",
    "        data_dict_i[\"incar_params\"] = incar_params\n",
    "        data_dict_i[\"irr_kpts\"] = irr_kpts\n",
    "        data_dict_i[\"init_atoms\"] = init_atoms\n",
    "        data_dict_i[\"final_atoms\"] = final_atoms\n",
    "        data_dict_i[\"magmoms\"] = magmoms_i\n",
    "        # #################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #################################################\n",
    "\n",
    "    elif run_job_i and not path_exists and compenv == \"wsl\":\n",
    "        print(\"A job needed to be processed but couldn't be found locally, or wasn't processed on the cluster\")\n",
    "        print(job_id_i, \"|\", gdrive_path_i)\n",
    "    # else:\n",
    "    #     print(\"Uhhh something didn't go through properly, check out\")\n",
    "        \n",
    "# #########################################################\n",
    "df_jobs_data = pd.DataFrame(data_dict_list)\n",
    "df_jobs_data_clusters_tmp = pd.DataFrame(rows_from_clusters)\n",
    "df_jobs_data_from_prev = pd.DataFrame(rows_from_prev_df)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    print(\"df_jobs_data.shape:\", df_jobs_data.shape[0])\n",
    "    print(\"df_jobs_data_clusters_tmp.shape:\", df_jobs_data_clusters_tmp.shape[0])\n",
    "    print(\"df_jobs_data_from_prev.shape:\", df_jobs_data_from_prev.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_jobs_data.shape[0] > 0:\n",
    "    df_jobs_data = reorder_df_columns([\"bulk_id\", \"slab_id\", \"job_id\", \"facet\", ], df_jobs_data)\n",
    "\n",
    "    # Set index to job_id\n",
    "    df_jobs_data = df_jobs_data.set_index(\"job_id\", drop=False)\n",
    "\n",
    "\n",
    "df_jobs_data_0 = df_jobs_data\n",
    "\n",
    "# Combine rows processed here with those already processed in the cluster\n",
    "df_jobs_data = pd.concat([\n",
    "    df_jobs_data_clusters_tmp,\n",
    "    df_jobs_data_0,\n",
    "    df_jobs_data_from_prev,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing `job_state` column from `df_jobs_data_clusters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_i = df_jobs_data\n",
    "df_i[\"unique_key\"] = list(zip(df_i[\"compenv\"], df_i[\"job_id\"], df_i[\"att_num\"], ))\n",
    "df_i = df_i.set_index(\"unique_key\", drop=False)\n",
    "df_jobs_data = df_i\n",
    "\n",
    "# #########################################################\n",
    "df_i = df_jobs_data_clusters\n",
    "df_i[\"unique_key\"] = list(zip(df_i[\"compenv\"], df_i[\"job_id\"], df_i[\"att_num\"], ))\n",
    "df_i = df_i.set_index(\"unique_key\", drop=False)\n",
    "df_jobs_data_clusters = df_i\n",
    "\n",
    "# #########################################################\n",
    "df_i = df_jobs_state\n",
    "df_i[\"unique_key\"] = list(zip(df_i[\"compenv\"], df_i[\"job_id\"], df_i[\"att_num\"], ))\n",
    "df_i = df_i.set_index(\"unique_key\", drop=False)\n",
    "df_jobs_state = df_i\n",
    "\n",
    "df_jobs_state_i = df_jobs_state.drop(columns=[\"compenv\", \"job_id\", \"att_num\"])\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "if compenv != \"wsl\":\n",
    "    df1 = df_jobs_data.drop(columns=[\"job_state\"])\n",
    "    df2 = df_jobs_state_i.job_state\n",
    "\n",
    "    df_jobs_data = pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "\n",
    "    df_jobs_data = df_jobs_data.set_index(\"job_id\", drop=False)\n",
    "\n",
    "# #########################################################\n",
    "if compenv == \"wsl\":\n",
    "\n",
    "    tmp = df_jobs_data.index.difference(df_jobs_data_clusters.index)\n",
    "    mess_i = \"Must be no differencec between df_jobs_data and df_jobs_data_clusters\"\n",
    "    mess_i += \"\\n\" + \"Usually this means you must rerun scripts on cluster\"\n",
    "    # assert len(tmp) == 0, mess_i\n",
    "\n",
    "    mess_i = \"Must be equal\"\n",
    "    # assert df_jobs_data.shape[0] == df_jobs_data_clusters.shape[0], mess_i\n",
    "\n",
    "    # #########################################################\n",
    "    df1 = df_jobs_data.drop(columns=[\"job_state\"])\n",
    "    df2 = df_jobs_data_clusters.job_state\n",
    "\n",
    "    df_jobs_data = pd.concat([\n",
    "        df1,\n",
    "        df2.loc[df2.index.intersection(df1.index)]\n",
    "        ], axis=1, )\n",
    "\n",
    "    df_jobs_data = df_jobs_data.set_index(\"job_id\", drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    print(\"df1.shape:\", df1.shape[0])\n",
    "    print(\"df2.shape:\", df2.shape[0])\n",
    "    print(\"\")\n",
    "\n",
    "keys_missing_from_df2 = []\n",
    "for key_i in df1.index:\n",
    "    key_in_df = key_i in df2.index\n",
    "    if not key_in_df:\n",
    "        keys_missing_from_df2.append(key_i)\n",
    "if verbose:\n",
    "    print(\"len(keys_missing_from_df2)\", len(keys_missing_from_df2))\n",
    "\n",
    "\n",
    "keys_missing_from_df1 = []\n",
    "for key_i in df2.index:\n",
    "    key_in_df = key_i in df1.index\n",
    "    if not key_in_df:\n",
    "        keys_missing_from_df1.append(key_i)\n",
    "if verbose:\n",
    "    print(\"len(keys_missing_from_df1)\", len(keys_missing_from_df1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting DFT electronic energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # #####################################################\n",
    "    final_atoms_i = row_i.final_atoms\n",
    "    # #####################################################\n",
    "\n",
    "    if final_atoms_i is not None:\n",
    "        pot_e_i = final_atoms_i.get_potential_energy()\n",
    "    else:\n",
    "        pot_e_i = None\n",
    "\n",
    "    return(pot_e_i)\n",
    "\n",
    "df_jobs_data[\"pot_e\"] = df_jobs_data.apply(method, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write `df_jobs_data` to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "my_file = Path(os.environ[\"PROJ_irox_oer_gdrive\"])\n",
    "if my_file.is_dir() or compenv != \"wsl\":\n",
    "    # Pickling data ###########################################\n",
    "    directory = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"dft_workflow/job_processing\",\n",
    "        \"out_data\")\n",
    "\n",
    "    pre_path = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer\"],\n",
    "        \"dft_workflow/job_processing\")\n",
    "\n",
    "    if compenv == \"wsl\":\n",
    "        file_name_i = \"df_jobs_data.pickle\"\n",
    "        path_i = os.path.join(pre_path, directory, file_name_i)\n",
    "    else:\n",
    "        file_name_i = \"df_jobs_data_\" + compenv + \".pickle\"\n",
    "        path_i = os.path.join(pre_path, directory, file_name_i)\n",
    "\n",
    "    if not os.path.exists(directory): os.makedirs(directory)\n",
    "    with open(path_i, \"wb\") as fle:\n",
    "        pickle.dump(df_jobs_data, fle)\n",
    "    # #########################################################\n",
    "\n",
    "    file_path_i = path_i\n",
    "\n",
    "    db_path = os.path.join(\n",
    "        \"01_norskov/00_git_repos/PROJ_IrOx_OER\",\n",
    "        \"dft_workflow/job_processing/out_data\" ,\n",
    "        file_name_i)\n",
    "\n",
    "    rclone_remote = os.environ.get(\"rclone_dropbox\", \"raul_dropbox\")\n",
    "    bash_comm = \"rclone copyto \" + file_path_i + \" \" + rclone_remote + \":\" + db_path\n",
    "\n",
    "    if compenv != \"wsl\":\n",
    "        if verbose:\n",
    "            print(\"Running rclone command\")\n",
    "            print(\"bash_comm:\", bash_comm)\n",
    "        os.system(bash_comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Printing dataframe, rereading from method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_jobs_data\n",
    "\n",
    "df_jobs_data_new = get_df_jobs_data(exclude_wsl_paths=True)\n",
    "df_jobs_data_new.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"parse_job_data.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
