{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run rapidos and bader scripts in job dirs of DOS calculations\n",
    "---"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from methods_dos import PDOS_Plotting, process_PDOS, calc_band_center\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_data,\n",
    "    get_df_jobs_anal,\n",
    "    get_df_jobs_paths,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = get_df_jobs()\n",
    "df_jobs_i = df_jobs\n",
    "\n",
    "df_jobs_anal = get_df_jobs_anal()\n",
    "df_jobs_anal_i = df_jobs_anal\n",
    "\n",
    "df_jobs_paths = get_df_jobs_paths()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data objects\n",
    "\n",
    "Only include `dos_bader` job types"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_i = df_jobs_i[df_jobs_i.job_type == \"dos_bader\"]\n",
    "\n",
    "df_ind = df_jobs_anal_i.index.to_frame()\n",
    "df_jobs_anal_i = df_jobs_anal_i.loc[\n",
    "    df_ind[df_ind.job_type == \"dos_bader\"].index\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering `df_jobs` by `job_completely_done` being True"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids_completely_done = df_jobs_anal_i[\n",
    "    df_jobs_anal_i.job_completely_done == True].job_id_max.tolist()\n",
    "\n",
    "df_jobs_i = df_jobs_i.loc[\n",
    "    job_ids_completely_done\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/dos_analysis\",\n",
    "    \"out_data/pdos_data\")\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_file(job_id_i, df_pdos, df_band_centers, directory=None):\n",
    "\n",
    "    # #########################################################\n",
    "    file_path_i = os.path.join(\n",
    "        directory,\n",
    "        job_id_i + \"__df_pdos\" + \".pickle\")\n",
    "    with open(file_path_i, \"wb\") as fle:\n",
    "        pickle.dump(df_pdos, fle)\n",
    "\n",
    "    # #########################################################\n",
    "    file_path_i = os.path.join(\n",
    "        directory,\n",
    "        job_id_i + \"__df_band_centers\" + \".pickle\")\n",
    "    with open(file_path_i, \"wb\") as fle:\n",
    "        pickle.dump(df_band_centers, fle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect into groups and make sure that only 1 revision per system"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "job_id_list = []\n",
    "# #########################################################\n",
    "group_cols = [\n",
    "    \"job_type\", \"compenv\", \"slab_id\", \"ads\", \"active_site\", \"att_num\", \n",
    "    ]\n",
    "grouped = df_jobs_i.groupby(group_cols)\n",
    "# #########################################################\n",
    "for name_i, group_i in grouped:\n",
    "    # print(name_i)\n",
    "\n",
    "    # DOS SLURM jobs only need 1 rev to finish, I think, this will check\n",
    "    assert group_i.shape[0] == 1, \"NOT TRUE ANYMORE RIGHT? | I think that there should only be one revision per system here\"\n",
    "\n",
    "    # #####################################################\n",
    "    row_i = group_i.iloc[0]\n",
    "    # #####################################################\n",
    "    job_id_i = row_i.name\n",
    "    # #####################################################\n",
    "\n",
    "    job_id_list.append(job_id_i)\n",
    "\n",
    "# #########################################################\n",
    "df_jobs_i = df_jobs_i.loc[job_id_list]\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids_all_done = df_jobs_anal_i[df_jobs_anal_i.job_completely_done == True].job_id_max.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "job_ids_to_process = []\n",
    "# for job_id_i, row_i in df_jobs_tmp.iterrows():\n",
    "for job_id_i, row_i in df_jobs_i.iterrows():\n",
    "\n",
    "    if verbose:\n",
    "        print(20 * \"-\")\n",
    "        print(job_id_i)\n",
    "\n",
    "    # #####################################################\n",
    "    row_paths_i = df_jobs_paths.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    path_i = row_paths_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    path_full_i = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        path_i)\n",
    "    \n",
    "    finished_file_path_i = os.path.join(\n",
    "        path_full_i,\n",
    "        \".FINISHED.new\")\n",
    "\n",
    "    file_path_0 = os.path.join(directory,\n",
    "        job_id_i + \"__df_pdos\" + \".pickle\")\n",
    "\n",
    "    file_path_1 = os.path.join(directory,\n",
    "        job_id_i + \"__df_band_centers\" + \".pickle\")\n",
    "\n",
    "\n",
    "    my_file_0 = Path(file_path_0)\n",
    "    my_file_1 = Path(file_path_1)\n",
    "    my_file = Path(finished_file_path_i)\n",
    "    if my_file.is_file() and not my_file_0.is_file() and not my_file_1.is_file():\n",
    "        print(job_id_i, \"processing...\")\n",
    "        job_ids_to_process.append(job_id_i)\n",
    "    else:\n",
    "        if not my_file.is_file():\n",
    "            if verbose:\n",
    "                print(\"Not finished\")\n",
    "        elif my_file_0.is_file() and my_file_1.is_file():\n",
    "            if verbose:\n",
    "                print(\"System already processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs_i_2 = df_jobs_i.loc[job_ids_to_process]\n",
    "for job_id_i, row_i in df_jobs_i_2.iterrows():\n",
    "\n",
    "    # #####################################################\n",
    "    row_paths_i = df_jobs_paths.loc[job_id_i]\n",
    "    # #####################################################\n",
    "    path_i = row_paths_i.gdrive_path\n",
    "    # #####################################################\n",
    "\n",
    "    path_full_i = os.path.join(\n",
    "        os.environ[\"PROJ_irox_oer_gdrive\"],\n",
    "        path_i)\n",
    "\n",
    "    PDOS_i = PDOS_Plotting(data_file_dir=path_full_i)\n",
    "\n",
    "    # #####################################################\n",
    "    pdos_data_dict = process_PDOS(\n",
    "        PDOS_i=PDOS_i,\n",
    "        )\n",
    "    # #####################################################\n",
    "    df_pdos_i = pdos_data_dict[\"df_xy\"]\n",
    "    df_band_centers_i = pdos_data_dict[\"df_band_centers\"]\n",
    "    was_processed = pdos_data_dict[\"was_processed\"]\n",
    "    # #####################################################\n",
    "\n",
    "    if was_processed:\n",
    "        df_band_centers_i.insert(0, \"system\", job_id_i)\n",
    "\n",
    "        write_data_to_file(\n",
    "            job_id_i,\n",
    "            df_pdos_i,\n",
    "            df_band_centers_i,\n",
    "            directory=directory,\n",
    "            )\n",
    "    else:\n",
    "        print(\"Was not processed, can't write data to file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import read_pdos_data\n",
    "\n",
    "df_pdos_i, df_band_centers_i = read_pdos_data(job_id_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdos_i.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_band_centers_i.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"collect_dos_data.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
