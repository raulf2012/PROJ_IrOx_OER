{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating slabs from IrOx polymorph dataset\n",
    "---\n",
    "\n",
    "This notebook is time consuming. Additional processing of the slab (correct vacuum applied, and bulk constraints, etc.) are done in `process_slabs.ipynb`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 565 total polymorphs from first project\n",
    "\n",
    "# 122 polymorphs are octahedral and unique\n",
    "# >>> Removing 12 systems manually because they are not good\n",
    "# -----\n",
    "# 110 polymorphs now\n",
    "\n",
    "\n",
    "# # ###############################################\n",
    "# 49 are layered materials\n",
    "# 61 are non-layered materials\n",
    "# -----\n",
    "# 61 polymorphs now\n",
    "\n",
    "\n",
    "# # ###############################################\n",
    "# 15 polymorphs are above the 0.3 eV/atom above hull cutoff\n",
    "# -----\n",
    "# 46 polymorphs now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/f/Dropbox/01_norskov/00_git_repos/PROJ_IrOx_OER/workflow/creating_slabs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "\n",
    "import time\n",
    "import signal\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import ase\n",
    "from ase import io\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# #########################################################\n",
    "from misc_modules.pandas_methods import drop_columns\n",
    "from misc_modules.misc_methods import GetFriendlyID\n",
    "from ase_modules.ase_methods import view_in_vesta\n",
    "\n",
    "# #########################################################\n",
    "from proj_data import metal_atom_symbol\n",
    "\n",
    "from methods import (\n",
    "    get_df_dft,\n",
    "    symmetrize_atoms,\n",
    "    get_structure_coord_df,\n",
    "    remove_atoms,\n",
    "    compare_facets_for_being_the_same,\n",
    "    TimeoutException,\n",
    "    sigalrm_handler,\n",
    "    )\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import (\n",
    "    analyse_local_coord_env,\n",
    "    check_if_sys_processed,\n",
    "    remove_nonsaturated_surface_metal_atoms,\n",
    "    remove_noncoord_oxygens,\n",
    "    create_slab_from_bulk,\n",
    "    create_final_slab_master,\n",
    "    create_save_dataframe,\n",
    "    constrain_slab,\n",
    "    read_data_json,\n",
    "    calc_surface_area,\n",
    "    create_slab,\n",
    "    update_sys_took_too_long,\n",
    "    create_save_struct_coord_df,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timelimit_seconds = 0.4 * 60\n",
    "# timelimit_seconds = 10 * 60\n",
    "# timelimit_seconds = 40 * 60\n",
    "timelimit_seconds = 100 * 60\n",
    "\n",
    "facets_manual = [\n",
    "    (1, 0, 0),\n",
    "    (0, 1, 0),\n",
    "    (0, 0, 1),\n",
    "\n",
    "    (1, 1, 1),\n",
    "\n",
    "    # (0, 1, 1),\n",
    "    # (1, 0, 1),\n",
    "    # (1, 1, 0),\n",
    "\n",
    "    ]\n",
    "facets_manual = [t for t in (set(tuple(i) for i in facets_manual))]\n",
    "\n",
    "frac_of_layered_to_include = 0.0\n",
    "\n",
    "phase_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_surf_a = 200\n",
    "# Distance from top z-coord of slab that we'll remove atoms from\n",
    "# dz = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "df_dft = get_df_dft()\n",
    "\n",
    "# #########################################################\n",
    "# Bulks not to run, manually checked to be erroneous/bad\n",
    "data_path = os.path.join(\n",
    "    \"in_data/bulks_to_not_run.json\")\n",
    "with open(data_path, \"r\") as fle:\n",
    "    bulks_to_not_run = json.load(fle)\n",
    "\n",
    "# #########################################################\n",
    "from methods import get_df_xrd\n",
    "df_xrd = get_df_xrd()\n",
    "\n",
    "# #########################################################\n",
    "from methods import get_df_bulk_manual_class\n",
    "df_bulk_manual_class = get_df_bulk_manual_class()\n",
    "\n",
    "# #########################################################\n",
    "from methods import get_bulk_selection_data\n",
    "bulk_selection_data = get_bulk_selection_data()\n",
    "bulk_ids__octa_unique = bulk_selection_data[\"bulk_ids__octa_unique\"]\n",
    "\n",
    "# #########################################################\n",
    "from methods import get_df_slab_ids, get_slab_id\n",
    "df_slab_ids = get_df_slab_ids()\n",
    "\n",
    "# #########################################################\n",
    "from methods import get_df_slab\n",
    "df_slab_old = get_df_slab(mode=\"almost-final\")\n",
    "\n",
    "# #########################################################\n",
    "from local_methods import df_dft_for_slab_creation\n",
    "df_dft_i = df_dft_for_slab_creation(\n",
    "    df_dft=df_dft,\n",
    "    bulk_ids__octa_unique=bulk_ids__octa_unique,\n",
    "    bulks_to_not_run=bulks_to_not_run,\n",
    "    df_bulk_manual_class=df_bulk_manual_class,\n",
    "    frac_of_layered_to_include=frac_of_layered_to_include,\n",
    "    verbose=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bulk_ids__octa_unique'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_bulk_selection_data().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP\n",
    "\n",
    "# mj7wbfb5nt\t011\t(0, 1, 1)\t\n",
    "\n",
    "df = df_slab_old\n",
    "df = df[\n",
    "    (df[\"bulk_id\"] == \"mj7wbfb5nt\") &\n",
    "    (df[\"facet\"] == \"011\") &\n",
    "    # (df[\"\"] == \"\") &\n",
    "    [True for i in range(len(df))]\n",
    "    ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create needed folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/creating_slabs\",\n",
    "    )\n",
    "\n",
    "directory = \"out_data/final_slabs\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"out_data/slab_progression\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"out_data/df_coord_files\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "directory = \"out_data/temp_out\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Number of bulk structures that are octahedral and unique:\",\n",
    "    \"\\n\",\n",
    "    len(bulk_ids__octa_unique))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking that df_slab_ids are unique, no repeat entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_slab_ids.index.is_unique:\n",
    "    print(\"df_slab_ids isn't unique\")\n",
    "    print(\"df_slab_ids isn't unique\")\n",
    "    print(\"df_slab_ids isn't unique\")\n",
    "    print(\"df_slab_ids isn't unique\")\n",
    "    print(\"df_slab_ids isn't unique\")\n",
    "\n",
    "print(\"Duplicate rows here (NOT GOOD!!!)\")\n",
    "display(\n",
    "    df_slab_ids[df_slab_ids.index.duplicated(keep=False)]\n",
    "    )\n",
    "\n",
    "df = df_slab_old\n",
    "df = df[\n",
    "    (df[\"bulk_id\"] == \"v1xpx482ba\") &\n",
    "    (df[\"facet\"] == \"20-21\") &\n",
    "    # (df[\"facet\"] == \"20-23\") &\n",
    "    [True for i in range(len(df))]\n",
    "    ]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "slab_ids_to_drop = []\n",
    "# #########################################################\n",
    "group_cols = [\"bulk_id\", \"facet\", ]\n",
    "grouped = df_slab_old.groupby(group_cols)\n",
    "for name_i, group_i in grouped:\n",
    "    if group_i.shape[0] > 1:\n",
    "\n",
    "        # print(name_i)\n",
    "        # display(group_i)\n",
    "\n",
    "        # name_i = ('xw9y6rbkxr', '10-12')\n",
    "        # group_i = grouped.get_group(name_i)\n",
    "\n",
    "        grp_0 = group_i[group_i.status == \"Took too long\"]\n",
    "        grp_1 = group_i[~group_i.slab_final.isna()]\n",
    "\n",
    "        if grp_1.shape[0] > 0:\n",
    "            if grp_0.shape[0] > 0:\n",
    "                slab_ids_to_drop_i = grp_0.index.tolist()\n",
    "                slab_ids_to_drop.extend(slab_ids_to_drop_i)\n",
    "\n",
    "# df_slab_old.loc[slab_ids_to_drop]\n",
    "df_slab_old = df_slab_old.drop(slab_ids_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating slabs from bulks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which systems previously took too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data_json()\n",
    "\n",
    "systems_that_took_too_long = data.get(\"systems_that_took_too_long\", []) \n",
    "\n",
    "systems_that_took_too_long_2 = []\n",
    "for i in systems_that_took_too_long:\n",
    "    systems_that_took_too_long_2.append(i[0] + \"_\" + i[1])\n",
    "\n",
    "print(\n",
    "    len(systems_that_took_too_long),\n",
    "    \" systems took too long to process and will be ignored\",\n",
    "    sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slab_old_tmp = df_slab_old.reset_index(level=0, inplace=False)\n",
    "df_slab_old_tmp = df_slab_old_tmp.set_index([\"bulk_id\", \"facet\", ], drop=False, )\n",
    "\n",
    "# df_slab_old.set_index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"This was True before, look into it if it's not\",\n",
    "    \"\\n\",\n",
    "\n",
    "    \"\\n\",\n",
    "    \"df_slab_old_tmp.index.is_unique:\",\n",
    "\n",
    "    \"\\n\",\n",
    "    df_slab_old_tmp.index.is_unique,\n",
    "\n",
    "    sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "systems_that_took_too_long__new = []\n",
    "for sys_i in systems_that_took_too_long:\n",
    "    # print(sys_i)\n",
    "\n",
    "    atoms_found = False\n",
    "    name_i = (sys_i[0], sys_i[1])\n",
    "    if name_i in df_slab_old_tmp.index:\n",
    "        # #####################################################\n",
    "        row_i = df_slab_old_tmp.loc[sys_i[0], sys_i[1]]\n",
    "        # #####################################################\n",
    "        slab_final_i = row_i.slab_final\n",
    "        # #####################################################\n",
    "\n",
    "        if isinstance(slab_final_i, ase.atoms.Atoms):\n",
    "            atoms_found = True\n",
    "    else:\n",
    "        tmp = 42\n",
    "\n",
    "    keep_sys_in_list = True\n",
    "    if atoms_found:\n",
    "        keep_sys_in_list = False\n",
    "\n",
    "    if keep_sys_in_list:\n",
    "        systems_that_took_too_long__new.append(sys_i)\n",
    "\n",
    "\n",
    "\n",
    "# ##########################################################\n",
    "# ##########################################################\n",
    "data = read_data_json()\n",
    "data[\"systems_that_took_too_long\"] = systems_that_took_too_long__new\n",
    "\n",
    "data_path = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/creating_slabs\",\n",
    "    \"out_data/data.json\")\n",
    "with open(data_path, \"w\") as fle:\n",
    "    json.dump(data, fle, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(systems_that_took_too_long__new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(systems_that_took_too_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_slab_old[df_slab_old.bulk_id == \"n36axdbw65\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out which systems haven't been run yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "systems_not_processed = []\n",
    "# #########################################################\n",
    "for i_cnt, bulk_id in enumerate(df_dft_i.index.tolist()):\n",
    "\n",
    "    # #####################################################\n",
    "    row_i = df_dft.loc[bulk_id]\n",
    "    # #####################################################\n",
    "    bulk_id_i = row_i.name\n",
    "    atoms = row_i.atoms\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_xrd_i = df_xrd.loc[bulk_id]\n",
    "    # #####################################################\n",
    "    top_facets_i = row_xrd_i.top_facets\n",
    "    all_xrd_facets_i = row_xrd_i.all_xrd_facets\n",
    "    facet_rank_i = row_xrd_i.facet_rank\n",
    "    # #####################################################\n",
    "\n",
    "    num_of_facets = 5\n",
    "    # num_of_facets = 8\n",
    "    top_facets_i = top_facets_i[0:num_of_facets]\n",
    "    facet_rank_i = facet_rank_i[0:num_of_facets]\n",
    "\n",
    "    # #####################################################\n",
    "    # Facet manipulation ##################################\n",
    "    facets_manual_2 = []\n",
    "    for i in facets_manual:\n",
    "        if i not in all_xrd_facets_i:\n",
    "            facets_manual_2.append(i)\n",
    "\n",
    "    df_facets_0 = pd.DataFrame()\n",
    "    df_facets_0[\"facet\"] = top_facets_i\n",
    "    df_facets_0[\"facet_rank\"] = facet_rank_i\n",
    "    df_facets_0[\"source\"] = \"xrd\"\n",
    "\n",
    "    df_facets_1 = pd.DataFrame()\n",
    "    df_facets_1[\"facet\"] = facets_manual_2\n",
    "    df_facets_1[\"source\"] = \"manual\"\n",
    "\n",
    "    df_facets = pd.concat([df_facets_0, df_facets_1])\n",
    "    df_facets = df_facets.reset_index()\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    # Making sure that there are no duplicates in the facets from the manual ones and xrd ones\n",
    "    # #####################################################\n",
    "    df_facets_i = df_facets[df_facets.source == \"manual\"]\n",
    "    df_facets_j = df_facets[df_facets.source == \"xrd\"]\n",
    "    # #####################################################\n",
    "    indices_to_drop = []\n",
    "    # #####################################################\n",
    "    for ind_i, row_i in df_facets_i.iterrows():\n",
    "        facet_i = row_i.facet\n",
    "        for ind_j, row_j in df_facets_j.iterrows():\n",
    "            facet_j = row_j.facet\n",
    "            facets_same = compare_facets_for_being_the_same(facet_i, facet_j)\n",
    "            if facets_same:\n",
    "                indices_to_drop.append(ind_i)\n",
    "    df_facets = df_facets.drop(index=indices_to_drop)\n",
    "\n",
    "\n",
    "    for ind_i, row_facet_i in df_facets.iterrows():\n",
    "        # #################################################\n",
    "        data_dict_i = dict()\n",
    "        # #################################################\n",
    "\n",
    "        # #################################################\n",
    "        facet = row_facet_i.facet\n",
    "        source_i = row_facet_i.source\n",
    "        facet_rank_i = row_facet_i.facet_rank\n",
    "        # #################################################\n",
    "\n",
    "        facet_i = \"\".join([str(i) for i in list(facet)])\n",
    "\n",
    "        facet_abs_sum_i = np.sum(\n",
    "            [np.abs(i) for i in facet]\n",
    "            )\n",
    "\n",
    "        sys_processed = check_if_sys_processed(\n",
    "            bulk_id_i=bulk_id_i,\n",
    "            facet_str=facet_i,\n",
    "            df_slab_old=df_slab_old)\n",
    "\n",
    "        id_comb = bulk_id + \"_\" + facet_i\n",
    "\n",
    "        took_too_long_prev = False\n",
    "        if id_comb in systems_that_took_too_long_2:\n",
    "            took_too_long_prev = True\n",
    "\n",
    "        # #################################################\n",
    "        data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "        data_dict_i[\"facet_str\"] = facet_i\n",
    "        data_dict_i[\"facet\"] = facet\n",
    "        data_dict_i[\"facet_rank\"] = facet_rank_i\n",
    "        data_dict_i[\"facet_abs_sum\"] = facet_abs_sum_i\n",
    "        data_dict_i[\"source\"] = source_i\n",
    "        data_dict_i[\"sys_processed\"] = sys_processed\n",
    "        data_dict_i[\"took_too_long_prev\"] = took_too_long_prev\n",
    "        # #################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #################################################\n",
    "\n",
    "# #########################################################\n",
    "df_to_run = pd.DataFrame(data_dict_list)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dft_i.loc[\n",
    "    \"v1xpx482ba\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_to_run\n",
    "df = df[\n",
    "    (df[\"bulk_id\"] == \"v1xpx482ba\") &\n",
    "    (df[\"facet_str\"] == \"20-23\") &\n",
    "    # (df[\"\"] == \"\") &\n",
    "    [True for i in range(len(df))]\n",
    "    ]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling data ###########################################\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/creating_slabs\",\n",
    "    \"out_data\")\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(os.path.join(directory, \"df_slabs_to_run.pickle\"), \"wb\") as fle:\n",
    "    df_slabs_to_run = df_to_run\n",
    "    pickle.dump(df_slabs_to_run, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_run_2 = df_to_run[\n",
    "    (df_to_run.sys_processed == False) & \\\n",
    "    (df_to_run.took_too_long_prev == False) & \\\n",
    "    (df_to_run.facet_abs_sum <= 7)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mj7wbfb5nt\t011\t(0, 1, 1)\t\n",
    "\n",
    "df_to_run_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_run_2 = df_to_run_2.loc[[164]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_run_2 = df_to_run[\n",
    "#     # (df_to_run.sys_processed == False) & \\\n",
    "#     (df_to_run.took_too_long_prev == True)\n",
    "#     # (df_to_run.facet_abs_sum < 7)\n",
    "#     ]\n",
    "\n",
    "# df_to_run_2 = df_to_run_2.iloc[[0]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_slabs_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_run[df_to_run.bulk_id == \"b583vr8hvw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b583vr8hvw 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_to_run_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tbulk_id\tfacet_str\tfacet\tfacet_rank\tfacet_abs_sum\tsource\tsys_processed\ttook_too_long_prev\n",
    "# 211\t9i6ixublcr\t31-3\t(3, 1, -3)\t1.0\t7\txrd\tTrue\tTrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in df_to_run_2.bulk_id.unique().tolist():\n",
    "#     print(\n",
    "#         i in df_dft_i.index\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop | Creating slabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_cnt, (ind_i, row_i) in enumerate(df_to_run_2.iterrows()):\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    bulk_id_i = row_i.bulk_id\n",
    "    facet = row_i.facet\n",
    "    facet_rank_i = row_i.facet_rank\n",
    "    source_i = row_i.source\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_dft_i = df_dft.loc[bulk_id_i]\n",
    "    # #####################################################\n",
    "    atoms_stan_prim_i = row_dft_i.atoms_stan_prim\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    # Set up signal handler for SIGALRM, saving previous value\n",
    "    t0 = time.time()\n",
    "    old_handler = signal.signal(signal.SIGALRM, sigalrm_handler)\n",
    "    signal.alarm(int(timelimit_seconds))\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    facet_i = \"\".join([str(i) for i in list(facet)])\n",
    "\n",
    "    # #####################################################\n",
    "    # Getting or generating id for slab (slab_id)\n",
    "    slab_id_i = get_slab_id(bulk_id_i, facet_i, df_slab_ids)\n",
    "    if slab_id_i is None:\n",
    "        slab_id_i = GetFriendlyID(append_random_num=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"bulk_id\"] = bulk_id_i\n",
    "    data_dict_i[\"facet\"] = facet_i\n",
    "    data_dict_i[\"facet_rank\"] = facet_rank_i\n",
    "    data_dict_i[\"source\"] = source_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"phase\"] = phase_num\n",
    "    # #####################################################\n",
    "\n",
    "    print(\n",
    "        \"bulk_id_i:\", bulk_id_i,\n",
    "        \"slab_id_i:\", slab_id_i,\n",
    "        \"facet:\", facet_i,\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        slab_final = create_slab(\n",
    "            atoms=atoms_stan_prim_i,\n",
    "            facet=facet,\n",
    "            # slab_thickness=15,\n",
    "            slab_thickness=12,\n",
    "            i_cnt=i_cnt)\n",
    "\n",
    "        create_save_struct_coord_df(\n",
    "            slab_final=slab_final,\n",
    "            slab_id=slab_id_i)\n",
    "\n",
    "        data_dict_i[\"slab_final\"] = slab_final\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\"Took to long skipping\")\n",
    "        data_dict_i[\"status\"] = \"Took too long\"\n",
    "\n",
    "        # Updating systems_that_took_too_long if bulk_id+facet combo doesn't finish in time\n",
    "        update_sys_took_too_long(bulk_id_i, facet_i)\n",
    "\n",
    "    finally:\n",
    "        # #################################################\n",
    "        signal.alarm(0)\n",
    "        signal.signal(signal.SIGALRM, old_handler)\n",
    "\n",
    "    # #####################################################\n",
    "    iter_time_i = time.time() - t0\n",
    "    data_dict_i[\"iter_time_i\"] = iter_time_i\n",
    "\n",
    "    df_slab_old = create_save_dataframe(\n",
    "        data_dict_list=[data_dict_i],\n",
    "        df_slab_old=df_slab_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slab_id_i = get_slab_id(bulk_id_i, facet_i, df_slab_ids)\n",
    "# if slab_id_i is None:\n",
    "#     slab_id_i = GetFriendlyID(append_random_num=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slab_id_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bulk_id_i: 9i6ixublcr slab_id_i: kogituwu_25 facet: 31-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import ase\n",
    "\n",
    "# import pickle"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
