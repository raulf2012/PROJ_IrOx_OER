{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining features and adsorption energies into one dataframe\n",
    "---\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import sys\n",
    "import time; ti = time.time()\n",
    "\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# #########################################################\n",
    "from methods import (\n",
    "    get_df_dft,\n",
    "    get_df_job_ids,\n",
    "    get_df_slab,\n",
    "    get_df_jobs,\n",
    "    get_df_jobs_data,\n",
    "\n",
    "    get_df_ads,\n",
    "    get_df_features,\n",
    "    \n",
    "    get_df_octa_vol_init,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import isnotebook    \n",
    "isnotebook_i = isnotebook()\n",
    "if isnotebook_i:\n",
    "    from tqdm.notebook import tqdm\n",
    "    verbose = True\n",
    "else:\n",
    "    from tqdm import tqdm\n",
    "    verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Inputs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = [\"g_o\", \"g_oh\", \"e_o\", \"e_oh\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads = get_df_ads()\n",
    "df_ads = df_ads.set_index([\"compenv\", \"slab_id\", \"active_site\", ], drop=False)\n",
    "\n",
    "df_features = get_df_features()\n",
    "df_features.index = df_features.index.droplevel(level=5)\n",
    "\n",
    "df_slab = get_df_slab()\n",
    "\n",
    "df_jobs = get_df_jobs()\n",
    "\n",
    "df_jobs_data = get_df_jobs_data()\n",
    "df_jobs_data[\"rerun_from_oh\"] = df_jobs_data[\"rerun_from_oh\"].fillna(value=False)\n",
    "\n",
    "df_dft = get_df_dft()\n",
    "\n",
    "df_job_ids = get_df_job_ids()\n",
    "df_job_ids = df_job_ids.set_index(\"job_id\")\n",
    "df_job_ids = df_job_ids[~df_job_ids.index.duplicated(keep='first')]\n",
    "\n",
    "df_octa_vol_init = get_df_octa_vol_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_features[\"features\"].columns.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting other relevent data columns from various data objects"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for index_i, row_i in df_ads.iterrows():\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    index_dict_i = dict(zip(\n",
    "        list(df_ads.index.names), index_i, ))\n",
    "    # #####################################################\n",
    "    slab_id_i = row_i.slab_id\n",
    "    job_id_o = row_i.job_id_o\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_ids_i = df_job_ids.loc[job_id_o]\n",
    "    # #####################################################\n",
    "    bulk_id_i = row_ids_i.bulk_id\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_dft_i = df_dft.loc[bulk_id_i]\n",
    "    # #####################################################\n",
    "    stoich_i = row_dft_i.stoich\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    row_slab_i = df_slab.loc[slab_id_i]\n",
    "    # #####################################################\n",
    "    phase_i = row_slab_i.phase\n",
    "    # #####################################################\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[\"phase\"] = phase_i\n",
    "    data_dict_i[\"stoich\"] = stoich_i\n",
    "    # #####################################################\n",
    "    data_dict_i.update(index_dict_i)\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "# #########################################################\n",
    "df_extra_data = pd.DataFrame(data_dict_list)\n",
    "df_extra_data = df_extra_data.set_index(\n",
    "    [\"compenv\", \"slab_id\", \"active_site\", ], drop=True)\n",
    "\n",
    "new_columns = []\n",
    "for col_i in df_extra_data.columns:\n",
    "    new_columns.append(\n",
    "        (\"data\", col_i, \"\")\n",
    "        )\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_columns)\n",
    "df_extra_data.columns = idx\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating features data by looping over `df_ads`"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dos_bader_feature_cols = [\n",
    "    \"Ir*O_bader\",\n",
    "    \"Ir_bader\",\n",
    "    \"O_bader\",\n",
    "    \"p_band_center\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEMP\n",
    "# print(111 * \"TEMP | \")\n",
    "\n",
    "# df_ads = df_ads.loc[[(\"sherlock\", \"lufinanu_76\", 46., )]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"df_ads.shape[0]:\", df_ads.shape[0])\n",
    "\n",
    "# #########################################################\n",
    "o_rows_list = []\n",
    "o_index_list = []\n",
    "# #########################################################\n",
    "oh_rows_list = []\n",
    "oh_index_list = []\n",
    "# #########################################################\n",
    "failed_indices_oh = []\n",
    "for index_i, row_i in df_ads.iterrows():\n",
    "\n",
    "    # print(\"SIDJFIJSDIFJIDSIFJISDIFJSDIOFJIODS867tr86r7t86867876t8t76\")\n",
    "\n",
    "    # #####################################################\n",
    "    index_dict_i = dict(zip(list(df_ads.index.names), index_i))\n",
    "    # #####################################################\n",
    "    job_id_o_i = row_i.job_id_o\n",
    "    job_id_oh_i = row_i.job_id_oh\n",
    "    job_id_bare_i = row_i.job_id_bare\n",
    "    # #####################################################\n",
    "\n",
    "    \n",
    "\n",
    "    # #####################################################\n",
    "    ads_i = \"o\"\n",
    "\n",
    "    idx = pd.IndexSlice\n",
    "    df_feat_i = df_features.loc[idx[\n",
    "        index_dict_i[\"compenv\"],\n",
    "        index_dict_i[\"slab_id\"],\n",
    "        ads_i,\n",
    "        index_dict_i[\"active_site\"],\n",
    "        :], :]\n",
    "\n",
    "\n",
    "    row_feat_i = df_feat_i[df_feat_i.data.job_id_max == job_id_o_i]\n",
    "    mess_i = \"There should only be one row after the previous filtering\"\n",
    "    assert row_feat_i.shape[0] == 1, mess_i\n",
    "    row_feat_i = row_feat_i.iloc[0]\n",
    "\n",
    "\n",
    "    tmp = list(row_feat_i[\"features\"][dos_bader_feature_cols].to_dict().values())\n",
    "    num_nan = len([i for i in tmp if np.isnan(i)])\n",
    "    if num_nan > 0:\n",
    "\n",
    "        tmp_dict = dict()\n",
    "        df_tmp = df_feat_i[\"features\"][dos_bader_feature_cols]\n",
    "        for i_cnt, (name_i, row_i) in enumerate(df_tmp.iterrows()):\n",
    "            # print(name_i)\n",
    "            row_values = list(row_i.to_dict().values())\n",
    "            num_nan = len([i for i in row_values if np.isnan(i)])\n",
    "            tmp_dict[i_cnt] = num_nan\n",
    "\n",
    "        max_key = None\n",
    "        for key, val in tmp_dict.items():\n",
    "            if val == np.min(list(tmp_dict.values())):\n",
    "                max_key = key\n",
    "\n",
    "        # print(\"Replaced row_feat_i with the row that has the dos/bader info\")\n",
    "        row_feat_i = df_feat_i.iloc[max_key]\n",
    "\n",
    "    # elif num_nan == 0:\n",
    "    #     tmp = 42\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    o_rows_list.append(row_feat_i)\n",
    "    o_index_list.append(row_feat_i.name)\n",
    "\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    ads_i = \"oh\"\n",
    "\n",
    "    idx = pd.IndexSlice\n",
    "    df_feat_i = df_features.loc[idx[\n",
    "        index_dict_i[\"compenv\"],\n",
    "        index_dict_i[\"slab_id\"],\n",
    "        ads_i,\n",
    "        index_dict_i[\"active_site\"],\n",
    "        :], :]\n",
    "\n",
    "    if df_feat_i.shape[0] > 0:\n",
    "        row_feat_i = df_feat_i[df_feat_i.data.job_id_max == job_id_oh_i]\n",
    "\n",
    "        if row_feat_i.shape[0] > 0:\n",
    "            mess_i = \"There should only be one row after the previous filtering\"\n",
    "            assert row_feat_i.shape[0] == 1, mess_i\n",
    "            row_feat_i = row_feat_i.iloc[0]\n",
    "\n",
    "\n",
    "            # #############################################\n",
    "            oh_rows_list.append(row_feat_i)\n",
    "            oh_index_list.append(row_feat_i.name)\n",
    "        else:\n",
    "            # failed_indices_oh.append(index_i)\n",
    "            failed_indices_oh.append(job_id_oh_i)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "idx = pd.MultiIndex.from_tuples(o_index_list, names=df_features.index.names)\n",
    "df_o = pd.DataFrame(o_rows_list, idx)\n",
    "df_o.index = df_o.index.droplevel(level=[2, 4, ])\n",
    "# #########################################################\n",
    "idx = pd.MultiIndex.from_tuples(oh_index_list, names=df_features.index.names)\n",
    "df_oh = pd.DataFrame(oh_rows_list, idx)\n",
    "df_oh.index = df_oh.index.droplevel(level=[2, 4, ])\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_o.index.to_frame()\n",
    "# df = df[\n",
    "#     (df[\"slab_id\"] == \"lufinanu_76\") &\n",
    "#     (df[\"active_site\"] == 46.) &\n",
    "#     # (df[\"\"] == \"\") &\n",
    "#     [True for i in range(len(df))]\n",
    "#     ]\n",
    "\n",
    "# df_o.loc[\n",
    "#     df.index\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking failed_indices_oh against systems that couldn't be processed"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_atoms_sorted_ind\n",
    "\n",
    "df_atoms_sorted_ind = get_df_atoms_sorted_ind()\n",
    "\n",
    "df_atoms_sorted_ind_i = df_atoms_sorted_ind[\n",
    "    df_atoms_sorted_ind.job_id.isin(failed_indices_oh)\n",
    "    ]\n",
    "\n",
    "df_tmp_8 = df_atoms_sorted_ind_i[df_atoms_sorted_ind_i.failed_to_sort == False]\n",
    "\n",
    "if df_tmp_8.shape[0] > 0:\n",
    "    print(\"Check out df_tmp_8, there where some *OH rows that weren't processed but maybe should be\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing and combining feature data columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_methods import combine_dfs_with_same_cols\n",
    "\n",
    "df_dict_i = {\n",
    "    \"oh\": df_oh[[\"data\"]],\n",
    "    \"o\": df_o[[\"data\"]],\n",
    "    }\n",
    "\n",
    "df_data_comb = combine_dfs_with_same_cols(\n",
    "    df_dict=df_dict_i,\n",
    "    verbose=False,\n",
    "    )\n",
    "\n",
    "\n",
    "# Adding another empty level to column index\n",
    "new_cols = []\n",
    "for col_i in df_data_comb.columns:\n",
    "    # new_col_i = (\"\", col_i[0], col_i[1])\n",
    "    new_col_i = (col_i[0], col_i[1], \"\", )\n",
    "    new_cols.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_cols)\n",
    "df_data_comb.columns = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `df_features_comb` and adding another column level for ads"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "ads_i = \"o\"\n",
    "\n",
    "df_features_o = df_o[[\"features\"]]\n",
    "columns_i = df_features_o.columns\n",
    "\n",
    "new_columns_i = []\n",
    "for col_i in columns_i:\n",
    "    new_col_i = (col_i[0], ads_i, col_i[1])\n",
    "    new_columns_i.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_columns_i)\n",
    "df_features_o.columns = idx\n",
    "\n",
    "# #########################################################\n",
    "ads_i = \"oh\"\n",
    "\n",
    "df_features_oh = df_oh[[\"features\"]]\n",
    "columns_i = df_features_oh.columns\n",
    "\n",
    "new_columns_i = []\n",
    "for col_i in columns_i:\n",
    "    new_col_i = (col_i[0], ads_i, col_i[1])\n",
    "    new_columns_i.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_columns_i)\n",
    "df_features_oh.columns = idx\n",
    "\n",
    "# #########################################################\n",
    "df_features_comb = pd.concat([\n",
    "    df_features_o,\n",
    "    df_features_oh,\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_ox_state_list = []\n",
    "for name_i, row_i in df_features_comb.iterrows():\n",
    "    eff_ox_state_o_i = row_i[(\"features\", \"o\", \"effective_ox_state\", )]\n",
    "    eff_ox_state_oh_i = row_i[(\"features\", \"oh\", \"effective_ox_state\", )]\n",
    "\n",
    "    eff_ox_state_i = eff_ox_state_oh_i\n",
    "    if eff_ox_state_oh_i != eff_ox_state_oh_i:\n",
    "\n",
    "        # print(name_i)\n",
    "        # print(20 * \"-\")\n",
    "        # print(eff_ox_state_oh_i)\n",
    "        # print(eff_ox_state_o_i)\n",
    "        # print(\"\")\n",
    "\n",
    "\n",
    "        \n",
    "        if np.isnan(eff_ox_state_oh_i):\n",
    "            # print(eff_ox_state_o_i)\n",
    "            if not np.isnan(eff_ox_state_o_i):\n",
    "                eff_ox_state_i = eff_ox_state_o_i\n",
    "\n",
    "\n",
    "        elif np.isnan(eff_ox_state_o_i):\n",
    "            # print(eff_ox_state_o_i)\n",
    "            if not np.isnan(eff_ox_state_oh_i):\n",
    "                eff_ox_state_i = eff_ox_state_oh_i\n",
    "\n",
    "    # if np.isnan(eff_ox_state_i):\n",
    "    #     print(eff_ox_state_i)\n",
    "\n",
    "    eff_ox_state_list.append(\n",
    "        np.round(eff_ox_state_i, 6),\n",
    "        # eff_ox_state_i\n",
    "        )\n",
    "\n",
    "\n",
    "df_features_comb[(\"features\", \"effective_ox_state\", \"\")] = eff_ox_state_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_comb = df_features_comb.drop(columns=[\n",
    "        (\"features\", \"o\", \"effective_ox_state\", ),\n",
    "        (\"features\", \"oh\", \"effective_ox_state\", ),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_ads_features = [\n",
    "    # \"effective_ox_state\",\n",
    "    \"dH_bulk\",\n",
    "    \"volume_pa\",\n",
    "    \"bulk_oxid_state\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = []\n",
    "\n",
    "new_cols = []\n",
    "\n",
    "# for col_i in df_features_comb[\"features\"].columns:\n",
    "for col_i in df_features_comb.columns:\n",
    "    # print(col_i)\n",
    "\n",
    "    if col_i[0] == \"features\":\n",
    "        tmp = 42\n",
    "        # print(col_i)\n",
    "\n",
    "        if col_i[2] in non_ads_features:\n",
    "            print(col_i)\n",
    "            if col_i[1] == \"oh\":\n",
    "                cols_to_drop.append(col_i)\n",
    "                new_cols.append(col_i)\n",
    "            elif col_i[1] == \"o\":\n",
    "                col_new_i = (col_i[0], col_i[2], \"\", )\n",
    "                new_cols.append(col_new_i)\n",
    "        else:\n",
    "            new_cols.append(col_i)\n",
    "\n",
    "    else:\n",
    "        new_cols.append(col_i)\n",
    "                \n",
    "# non_ads_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.MultiIndex.from_tuples(new_cols)\n",
    "\n",
    "df_features_comb.columns = idx\n",
    "\n",
    "df_features_comb = df_features_comb.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_features = []\n",
    "o_features = []\n",
    "other_features = []\n",
    "for col_i in df_features_comb.columns:\n",
    "    if col_i[1] == \"oh\":\n",
    "        oh_features.append(col_i)\n",
    "    elif col_i[1] == \"o\":\n",
    "        o_features.append(col_i)\n",
    "    else:\n",
    "        other_features.append(col_i)\n",
    "\n",
    "df_features_comb = df_features_comb[\n",
    "    oh_features + o_features + other_features\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding more levels to df_ads to combine\n",
    "\n",
    "new_cols = []\n",
    "for col_i in df_ads.columns:\n",
    "    # new_col_i = (\"\", \"\", col_i)\n",
    "    new_col_i = (col_i, \"\", \"\", )\n",
    "    new_cols.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_cols)\n",
    "df_ads.columns = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining all dataframes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comb = pd.concat([\n",
    "    df_features_comb,\n",
    "    df_data_comb,\n",
    "    df_ads,\n",
    "    df_extra_data,\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the p-band center feature for *OH (there are none)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comb = df_all_comb.drop(columns=[\n",
    "    ('features', 'oh', 'p_band_center'),\n",
    "    ])\n",
    "\n",
    "df_all_comb = df_all_comb.drop(columns=[\n",
    "    ('features', 'oh', 'Ir_bader'),\n",
    "    ])\n",
    "\n",
    "df_all_comb = df_all_comb.drop(columns=[\n",
    "    ('features', 'oh', 'O_bader'),\n",
    "    ])\n",
    "\n",
    "df_all_comb = df_all_comb.drop(columns=[\n",
    "    ('features', 'oh', 'Ir*O_bader'),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `name_str` column"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method(row_i):\n",
    "    # #########################################################\n",
    "    name_i = row_i.name\n",
    "    # #########################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #########################################################\n",
    "    \n",
    "    name_i = compenv_i + \"__\" + slab_id_i + \"__\" + str(int(active_site_i)).zfill(3)\n",
    "\n",
    "    return(name_i)\n",
    "\n",
    "df_all_comb[\"data\", \"name_str\", \"\"] = df_all_comb.apply(\n",
    "    method,\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads_columns = [i[0] for i in df_ads.columns.tolist()]\n",
    "\n",
    "for i in target_cols:\n",
    "    df_ads_columns.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns_all = [i[0] for i in df_all_comb[\"data\"].columns]\n",
    "\n",
    "df_ads_columns_to_add = []\n",
    "df_ads_columns_to_drop = []\n",
    "for col_i in df_ads_columns:\n",
    "    if col_i not in data_columns_all:\n",
    "        df_ads_columns_to_add.append(col_i)\n",
    "    else:\n",
    "        df_ads_columns_to_drop.append(col_i)\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "for col_i in df_ads_columns_to_drop:\n",
    "    df_all_comb.drop(columns=(col_i, \"\", \"\"), inplace=True)\n",
    "\n",
    "# #########################################################\n",
    "new_columns = []\n",
    "for col_i in df_all_comb.columns:\n",
    "    if col_i[0] in df_ads_columns_to_add:\n",
    "        new_columns.append(\n",
    "            (\"data\", col_i[0], \"\", )\n",
    "            )\n",
    "    elif col_i[0] in target_cols:\n",
    "        new_columns.append(\n",
    "            (\"targets\", col_i[0], \"\", )\n",
    "            )\n",
    "    else:\n",
    "        new_columns.append(col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_columns)\n",
    "df_all_comb.columns = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding magmom comparison data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df_magmoms_comp_i(df_magmoms_comp_i):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def method(row_i):\n",
    "        new_column_values_dict = dict(\n",
    "            job_id_0=None,\n",
    "            job_id_1=None,\n",
    "            job_id_2=None,\n",
    "            )\n",
    "\n",
    "        job_ids_tri = row_i.job_ids_tri\n",
    "\n",
    "        ids_sorted = list(np.sort(list(job_ids_tri)))\n",
    "\n",
    "        job_id_0 = ids_sorted[0]\n",
    "        job_id_1 = ids_sorted[1]\n",
    "        job_id_2 = ids_sorted[2]\n",
    "\n",
    "        new_column_values_dict[\"job_id_0\"] = job_id_0\n",
    "        new_column_values_dict[\"job_id_1\"] = job_id_1\n",
    "        new_column_values_dict[\"job_id_2\"] = job_id_2\n",
    "\n",
    "        for key, value in new_column_values_dict.items():\n",
    "            row_i[key] = value\n",
    "        return(row_i)\n",
    "\n",
    "    df_magmoms_comp_i = df_magmoms_comp_i.apply(method, axis=1)\n",
    "    df_magmoms_comp_i = df_magmoms_comp_i.set_index([\"job_id_0\", \"job_id_1\", \"job_id_2\", ])\n",
    "\n",
    "    return(df_magmoms_comp_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_magmoms, read_magmom_comp_data\n",
    "\n",
    "df_magmoms = get_df_magmoms()\n",
    "\n",
    "\n",
    "data_dict_list = []\n",
    "for name_i, row_i in df_all_comb.iterrows():\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    index_dict_i = dict(zip(df_all_comb.index.names, name_i))\n",
    "    # #####################################################\n",
    "\n",
    "    magmom_data_i = read_magmom_comp_data(name=name_i)\n",
    "    if magmom_data_i is not None:\n",
    "        df_magmoms_comp_i = magmom_data_i[\"df_magmoms_comp\"]\n",
    "        df_magmoms_comp_i = process_df_magmoms_comp_i(df_magmoms_comp_i)\n",
    "\n",
    "        # tmp = df_magmoms_comp_i.sum_norm_abs_magmom_diff.min()\n",
    "        # tmp_list.append(tmp)\n",
    "\n",
    "        job_ids = []\n",
    "        for ads_j in [\"o\", \"oh\", \"bare\", ]:\n",
    "            job_id_j = row_i[\"data\"][\"job_id_\" + ads_j][\"\"]\n",
    "            if job_id_j is not None:\n",
    "                job_ids.append(job_id_j)\n",
    "\n",
    "\n",
    "        sum_norm_abs_magmom_diff_i = None\n",
    "        if len(job_ids) == 3:\n",
    "            job_ids = list(np.sort(job_ids))\n",
    "            job_id_0 = job_ids[0]\n",
    "            job_id_1 = job_ids[1]\n",
    "            job_id_2 = job_ids[2]\n",
    "\n",
    "            row_mags_i = df_magmoms_comp_i.loc[\n",
    "                (job_id_0, job_id_1, job_id_2, )\n",
    "                ]\n",
    "            sum_norm_abs_magmom_diff_i = row_mags_i.sum_norm_abs_magmom_diff\n",
    "            norm_sum_norm_abs_magmom_diff_i = sum_norm_abs_magmom_diff_i / 3\n",
    "            \n",
    "        # #################################################\n",
    "        data_dict_i.update(index_dict_i)\n",
    "        # #################################################\n",
    "        data_dict_i[\"sum_norm_abs_magmom_diff\"] = sum_norm_abs_magmom_diff_i\n",
    "        data_dict_i[\"norm_sum_norm_abs_magmom_diff\"] = norm_sum_norm_abs_magmom_diff_i\n",
    "        # #################################################\n",
    "        data_dict_list.append(data_dict_i)\n",
    "        # #################################################\n",
    "\n",
    "# #########################################################\n",
    "df_tmp = pd.DataFrame(data_dict_list)\n",
    "df_tmp = df_tmp.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "\n",
    "# #########################################################\n",
    "new_cols = []\n",
    "for col_i in df_tmp.columns:\n",
    "    new_col_i = (\"data\", col_i, \"\")\n",
    "    new_cols.append(new_col_i)\n",
    "idx = pd.MultiIndex.from_tuples(new_cols)\n",
    "df_tmp.columns = idx\n",
    "\n",
    "df_all_comb = pd.concat([df_all_comb, df_tmp], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add OER overpotential data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "import pickle; import os\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/oer_analysis\",\n",
    "    \"out_data\")\n",
    "path_i = os.path.join(\n",
    "    directory,\n",
    "    \"df_overpot.pickle\")\n",
    "with open(path_i, \"rb\") as fle:\n",
    "    df_overpot = pickle.load(fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_overpot = df_overpot.drop(columns=\"name\")\n",
    "\n",
    "new_cols = []\n",
    "for col_i in df_overpot.columns:\n",
    "    new_col_i = (\"data\", col_i, \"\", )\n",
    "    new_cols.append(new_col_i)\n",
    "df_overpot.columns = pd.MultiIndex.from_tuples(new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comb = pd.concat([\n",
    "    df_all_comb,\n",
    "    df_overpot,\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding surface energy data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_SE\n",
    "df_SE = get_df_SE()\n",
    "\n",
    "\n",
    "new_cols = []\n",
    "for col_i in df_SE.columns:\n",
    "    new_col_i = (\"data\", col_i, \"\", )\n",
    "    new_cols.append(new_col_i)\n",
    "df_SE.columns = pd.MultiIndex.from_tuples(new_cols)\n",
    "\n",
    "\n",
    "cols_to_remove = []\n",
    "for col_i in df_SE.columns.tolist():\n",
    "    if col_i in df_all_comb.columns.tolist():\n",
    "        cols_to_remove.append(col_i)\n",
    "\n",
    "\n",
    "df_all_comb = pd.concat([\n",
    "    df_all_comb,\n",
    "    # df_SE,\n",
    "    df_SE.drop(columns=cols_to_remove),\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding plot format properties"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj_data import stoich_color_dict\n",
    "\n",
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "# for index_i, row_i in df_features_targets.iterrows():\n",
    "for index_i, row_i in df_all_comb.iterrows():\n",
    "    # #####################################################\n",
    "    data_dict_i = dict()\n",
    "    # #####################################################\n",
    "    index_dict_i = dict(zip(list(df_all_comb.index.names), index_i))\n",
    "    # #####################################################\n",
    "    row_data_i = row_i[\"data\"]\n",
    "    # #####################################################\n",
    "    stoich_i = row_data_i[\"stoich\"][\"\"]\n",
    "    norm_sum_norm_abs_magmom_diff_i = \\\n",
    "        row_data_i[\"norm_sum_norm_abs_magmom_diff\"][\"\"]\n",
    "    # #####################################################\n",
    "\n",
    "    if stoich_i == \"AB2\":\n",
    "        color__stoich_i = stoich_color_dict[\"AB2\"]\n",
    "    elif stoich_i == \"AB3\":\n",
    "        color__stoich_i = stoich_color_dict[\"AB3\"]\n",
    "    else:\n",
    "        color__stoich_i = stoich_color_dict[\"None\"]\n",
    "\n",
    "\n",
    "    # #####################################################\n",
    "    data_dict_i[(\"format\", \"color\", \"stoich\")] = color__stoich_i\n",
    "    data_dict_i[(\"format\", \"color\", \"norm_sum_norm_abs_magmom_diff\")] = \\\n",
    "        norm_sum_norm_abs_magmom_diff_i\n",
    "    # #####################################################\n",
    "    data_dict_i.update(index_dict_i)\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "# #########################################################\n",
    "df_format = pd.DataFrame(data_dict_list)\n",
    "df_format = df_format.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "df_format.columns = pd.MultiIndex.from_tuples(df_format.columns)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comb = pd.concat(\n",
    "    [\n",
    "        df_all_comb,\n",
    "        df_format,\n",
    "        ],\n",
    "    axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixing Bader charges with bond lengths"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all_comb[\"features\"][\"\"]\n",
    "# df_all_comb[(\"features\", \"o\", \"Ir*O_bader\", )]\n",
    "\n",
    "df_all_comb[(\"features\", \"o\", \"Ir*O_bader/ir_o_mean\", )] = \\\n",
    "    df_all_comb[(\"features\", \"o\", \"Ir*O_bader\", )] / df_all_comb[(\"features\", \"o\", \"ir_o_mean\", )]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating \u0394G_OmOH target column"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing \u0394G_O-OH\n",
    "g_o = df_all_comb[(\"targets\", \"g_o\", \"\")]\n",
    "g_oh = df_all_comb[(\"targets\", \"g_oh\", \"\")]\n",
    "\n",
    "df_all_comb[(\"targets\", \"g_o_m_oh\", \"\")] = g_o - g_oh\n",
    "\n",
    "# Computing \u0394E_O-OH\n",
    "e_o = df_all_comb[(\"targets\", \"e_o\", \"\")]\n",
    "e_oh = df_all_comb[(\"targets\", \"e_oh\", \"\")]\n",
    "\n",
    "df_all_comb[(\"targets\", \"e_o_m_oh\", \"\")] = e_o - e_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding in pre-DFT features"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "data_dict_list = []\n",
    "# #########################################################\n",
    "for name_i, row_i in df_all_comb.iterrows():\n",
    "    # #####################################################\n",
    "    compenv_i = name_i[0]\n",
    "    slab_id_i = name_i[1]\n",
    "    active_site_i = name_i[2]\n",
    "    # #####################################################\n",
    "\n",
    "    job_id_o_i = row_i[(\"data\", \"job_id_o\", \"\")]\n",
    "\n",
    "    name_octa_i = (compenv_i, slab_id_i,\n",
    "        \"o\", active_site_i, 1, )\n",
    "    row_octa_i = df_octa_vol_init.loc[\n",
    "        name_octa_i\n",
    "        ]\n",
    "\n",
    "    row_octa_dict_i = row_octa_i[\"features\"].to_dict()\n",
    "    \n",
    "    # #####################################################\n",
    "    data_dict_i = {}\n",
    "    # #####################################################\n",
    "    data_dict_i[\"compenv\"] = compenv_i\n",
    "    data_dict_i[\"slab_id\"] = slab_id_i\n",
    "    data_dict_i[\"active_site\"] = active_site_i\n",
    "    # #####################################################\n",
    "    data_dict_i.update(row_octa_dict_i)\n",
    "    # #####################################################\n",
    "    data_dict_list.append(data_dict_i)\n",
    "    # #####################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_feat_pre = pd.DataFrame(data_dict_list)\n",
    "df_feat_pre = df_feat_pre.set_index([\"compenv\", \"slab_id\", \"active_site\", ])\n",
    "\n",
    "new_cols = []\n",
    "for col_i in df_feat_pre.columns:\n",
    "    new_col_i = (\"features_pre_dft\", col_i + \"__pre\", \"\")\n",
    "    new_cols.append(new_col_i)\n",
    "\n",
    "idx = pd.MultiIndex.from_tuples(new_cols)\n",
    "df_feat_pre.columns = idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_all_comb = pd.concat([\n",
    "    df_all_comb,\n",
    "    df_feat_pre,\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindexing multiindex to get order columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_comb = df_all_comb.reindex(columns=[\n",
    "    'targets',\n",
    "    'data',\n",
    "    'format',\n",
    "    'features',\n",
    "    'features_pre_dft',\n",
    "    'features_stan',\n",
    "    ], level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing rows that aren't supposed to be processed (bad slabs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_slabs_to_run\n",
    "df_slabs_to_run = get_df_slabs_to_run()\n",
    "df_slabs_to_not_run = df_slabs_to_run[df_slabs_to_run.status == \"bad\"]\n",
    "\n",
    "slab_ids_to_not_include = df_slabs_to_not_run.slab_id.tolist()\n",
    "\n",
    "df_index = df_all_comb.index.to_frame()\n",
    "df_all_comb = df_all_comb.loc[\n",
    "    ~df_index.slab_id.isin(slab_ids_to_not_include)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD DEPRECATED | Getting rid of NERSC jobs"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Getting rid of NERSC jobs and phase 1 systems\")\n",
    "\n",
    "# Getting rid of NERSC jobs\n",
    "\n",
    "# indices_to_keep = []\n",
    "# for i in df_all_comb.index:\n",
    "#     if i[0] != \"nersc\":\n",
    "#         indices_to_keep.append(i)\n",
    "\n",
    "# df_all_comb = df_all_comb.loc[\n",
    "#     indices_to_keep\n",
    "#     ]\n",
    "\n",
    "df_all_comb = df_all_comb[df_all_comb[\"data\"][\"phase\"] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing how many `NaN` rows there are for each feature"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_i in df_all_comb.features.columns:\n",
    "    if verbose:\n",
    "        df_tmp_i = df_all_comb[df_all_comb[\"features\"][col_i].isna()]\n",
    "        print(col_i, \":\", df_tmp_i.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write data to pickle"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_targets = df_all_comb\n",
    "# Pickling data ###########################################\n",
    "directory = os.path.join(\n",
    "    os.environ[\"PROJ_irox_oer\"],\n",
    "    \"workflow/feature_engineering\",\n",
    "    \"out_data\")\n",
    "file_name_i = \"df_features_targets.pickle\"\n",
    "path_i = os.path.join(directory, file_name_i)\n",
    "if not os.path.exists(directory): os.makedirs(directory)\n",
    "with open(path_i, \"wb\") as fle:\n",
    "    pickle.dump(df_features_targets, fle)\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods import get_df_features_targets\n",
    "\n",
    "df_features_targets_tmp = get_df_features_targets()\n",
    "df_features_targets_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #########################################################\n",
    "print(20 * \"# # \")\n",
    "print(\"All done!\")\n",
    "print(\"Run time:\", np.round((time.time() - ti) / 60, 3), \"min\")\n",
    "print(\"combine_features_targets.ipynb\")\n",
    "print(20 * \"# # \")\n",
    "# #########################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PROJ_irox_oer] *",
   "language": "python",
   "name": "conda-env-PROJ_irox_oer-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
